{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDataset import MyDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lama_embedding_path = '../llama_embedding/CL/tumor_CL_E1_finetuned_5layers/dataset_tensor/'\n",
    "bert_embedding_path = '../bert_embedding/tumor_finetuned/dataset_tensor/'\n",
    "roberta_embedding_path = '../roberta_embedding/tumor_finetuned/dataset_tensor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset('train', lama_embedding_path, bert_embedding_path, roberta_embedding_path)\n",
    "test_data =MyDataset('test', lama_embedding_path, bert_embedding_path, roberta_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "test_data = DataLoader(test_data, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[ 1.3342, -0.1144, -0.2511,  ..., -2.1754,  1.2194,  0.1429],\n",
      "         [ 1.1941, -0.8088, -0.2573,  ..., -1.8149,  0.6076, -1.3753],\n",
      "         [-0.7613,  0.2525, -0.4715,  ..., -0.4663, -0.3716, -0.0810],\n",
      "         [-1.2385,  0.2488, -0.2435,  ..., -0.2386, -0.0275, -0.2264],\n",
      "         [-0.6734,  0.1882, -0.2414,  ..., -0.0393, -0.4785,  0.0698]]]), tensor([[-0.8550,  0.6309, -0.6627,  ..., -0.5210,  0.2564, -0.3477]]), tensor([[ 1.7732,  0.9521, -0.7721,  ..., -0.5331, -0.3238, -0.3959]]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_data):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rep, b_rep, r_rep, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_rep.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4096]), torch.Size([1024]), torch.Size([1024]), 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rep.squeeze().shape, b_rep.squeeze().shape, r_rep.squeeze().shape, label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps 별 코사인 유사도 평균 구해보기\n",
    "def get_l1_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).sum()\n",
    "def get_l2_distance(x1, x2):\n",
    "    return ((x1 - x2)**2).sum()**.5\n",
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_cosine_similarity(reps_dict[0]['lama'][0].flatten() , reps_dict[0]['lama'][1].flatten()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 끼리의 코사인 유사도\n",
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.5043090581893921, std : 0.33062195777893066, min : -0.020971229299902916, max : 0.9975094199180603\n",
      "Bert mean : 0.9680062532424927, std : 0.06482237577438354, min : 0.5762061476707458, max : 0.9999645352363586\n",
      "Roberta mean : 0.7679231762886047, std : 0.3273807466030121, min : -0.14232468605041504, max : 0.9999403357505798\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.8716726303100586, std : 0.24739646911621094, min : 0.01143911387771368, max : 0.9993343949317932\n",
      "Bert mean : 0.967339038848877, std : 0.11414065212011337, min : 0.08446459472179413, max : 0.9999260902404785\n",
      "Roberta mean : 0.9764479398727417, std : 0.1192246600985527, min : 0.14114268124103546, max : 0.9999465346336365\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9026783108711243, std : 0.13419905304908752, min : 0.030011961236596107, max : 0.9970760345458984\n",
      "Bert mean : 0.9849059581756592, std : 0.014416240155696869, min : 0.9077816605567932, max : 1.0\n",
      "Roberta mean : 0.9430029988288879, std : 0.08131575584411621, min : 0.4610406756401062, max : 0.9998674392700195\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.8527932167053223, std : 0.2806805968284607, min : -0.014721560291945934, max : 0.9994171857833862\n",
      "Bert mean : 0.8764873147010803, std : 0.19998440146446228, min : -0.4412735402584076, max : 0.9999936819076538\n",
      "Roberta mean : 0.9174246191978455, std : 0.14946486055850983, min : 0.05138710141181946, max : 0.9999562501907349\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.7593218684196472, std : 0.3278258144855499, min : -0.015980970114469528, max : 0.9992056488990784\n",
      "Bert mean : 0.9256547689437866, std : 0.1994471251964569, min : -0.47379860281944275, max : 0.9999668598175049\n",
      "Roberta mean : 0.8538801670074463, std : 0.23610827326774597, min : -0.15768025815486908, max : 0.9999735355377197\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.8460637927055359, std : 0.29353067278862, min : 0.0017694468842819333, max : 0.9985698461532593\n",
      "Bert mean : 0.9804783463478088, std : 0.02478441223502159, min : 0.7493387460708618, max : 0.9998576641082764\n",
      "Roberta mean : 0.7132827639579773, std : 0.3693503737449646, min : -0.2799872159957886, max : 0.9978198409080505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.4659726023674011, std : 0.3475796580314636, min : 0.019955679774284363, max : 0.9957528710365295\n",
      "Bert mean : 0.8336034417152405, std : 0.3186386525630951, min : 0.08265118300914764, max : 0.9998219013214111\n",
      "Roberta mean : 0.6699555516242981, std : 0.3867779076099396, min : -0.21332025527954102, max : 0.9997442364692688\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.7504607439041138, std : 0.31600087881088257, min : 0.029431937262415886, max : 0.9986459016799927\n",
      "Bert mean : 0.7791711091995239, std : 0.4013894498348236, min : -0.16923663020133972, max : 0.9998753070831299\n",
      "Roberta mean : 0.9898001551628113, std : 0.011732799932360649, min : 0.9500171542167664, max : 0.9999515414237976\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9173040390014648, std : 0.04208584874868393, min : 0.7690723538398743, max : 0.9659547805786133\n",
      "Bert mean : 0.9522998332977295, std : 0.059857457876205444, min : 0.6458935737609863, max : 0.9971312284469604\n",
      "Roberta mean : 0.6465266346931458, std : 0.39903706312179565, min : -0.11720796674489975, max : 0.9968264102935791\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.8700740337371826, std : 0.2552449405193329, min : 0.012738249264657497, max : 0.9986140727996826\n",
      "Bert mean : 0.8970180153846741, std : 0.0863928273320198, min : 0.6242274045944214, max : 0.999889612197876\n",
      "Roberta mean : 0.8786481618881226, std : 0.22249220311641693, min : 0.08519376814365387, max : 0.999923825263977\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.7283310294151306, std : 0.33055272698402405, min : 0.012261606752872467, max : 0.970221996307373\n",
      "Bert mean : 0.6472193002700806, std : 0.46094951033592224, min : -0.22326429188251495, max : 0.9991183280944824\n",
      "Roberta mean : 0.7454431056976318, std : 0.34253740310668945, min : -0.050079647451639175, max : 0.9964597225189209\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.70322185754776, std : 0.34190648794174194, min : 0.013399969786405563, max : 0.995223343372345\n",
      "Bert mean : 0.5748860836029053, std : 0.49861255288124084, min : -0.22613559663295746, max : 0.9996704459190369\n",
      "Roberta mean : 0.5188899636268616, std : 0.45197492837905884, min : -0.2699298560619354, max : 0.9902639985084534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Layer 별 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)\n",
    "# # llama layer별\n",
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict[idx]['lama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 라벨들과의 코사인 유사도 비교\n",
    "## TrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.06343119591474533, std : 0.1442990005016327, min : -0.03741266950964928, max : 0.9445816874504089\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390362352132797, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.027880383655428886, std : 0.026576420292258263, min : -0.03434417024254799, max : 0.5867004990577698\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.09269070625305176, std : 0.22619467973709106, min : -0.03012235276401043, max : 0.983535885810852\n",
      "Bert mean : 0.01212023664265871, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777684669941664, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.031098397448658943, std : 0.11827380210161209, min : -0.07677605003118515, max : 0.9647419452667236\n",
      "Bert mean : -0.24299663305282593, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745851278305, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.3116267919540405, std : 0.3375121057033539, min : -0.023554904386401176, max : 0.9835547804832458\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175329387188, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.06343119591474533, std : 0.1442990005016327, min : -0.03741266950964928, max : 0.9445816874504089\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390361607074738, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.10666052252054214, std : 0.13529439270496368, min : -0.013460694812238216, max : 0.9564144015312195\n",
      "Bert mean : -0.22500866651535034, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239936351776123, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.020712945610284805, std : 0.03922416269779205, min : -0.034677017480134964, max : 0.8929867744445801\n",
      "Bert mean : -0.08799591660499573, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.057366516441106796, std : 0.03411151468753815, min : -0.03779689222574234, max : 0.9018683433532715\n",
      "Bert mean : -0.2719807028770447, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572444558143616, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.0809682160615921, std : 0.13258694112300873, min : -0.008651929907500744, max : 0.954234778881073\n",
      "Bert mean : -0.23431454598903656, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608237445354462, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.027880383655428886, std : 0.026576420292258263, min : -0.03434417024254799, max : 0.5867004990577698\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.10666052997112274, std : 0.13529439270496368, min : -0.013460694812238216, max : 0.9564144015312195\n",
      "Bert mean : -0.22500869631767273, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239934489130974, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.037491895258426666, std : 0.10353273153305054, min : -0.015958385542035103, max : 0.9267233610153198\n",
      "Bert mean : -0.3086966276168823, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.06470011919736862, std : 0.1261521726846695, min : -0.02235877886414528, max : 0.9349383115768433\n",
      "Bert mean : 0.14542877674102783, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856135308742523, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : 0.04531552270054817, std : 0.1437550038099289, min : -0.013672673143446445, max : 0.9355769157409668\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.09269070625305176, std : 0.22619467973709106, min : -0.03012235276401043, max : 0.983535885810852\n",
      "Bert mean : 0.012120235711336136, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777686066925526, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.020712943747639656, std : 0.03922416269779205, min : -0.034677017480134964, max : 0.8929867744445801\n",
      "Bert mean : -0.08799590915441513, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.03749189153313637, std : 0.10353273153305054, min : -0.015958385542035103, max : 0.9267233610153198\n",
      "Bert mean : -0.30869656801223755, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.05263237655162811, std : 0.1466101109981537, min : -0.029163256287574768, max : 0.9740216732025146\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754584550857544, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.05823441222310066, std : 0.07649566233158112, min : -0.023895151913166046, max : 0.9073539972305298\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.0388345941901207, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.031098399311304092, std : 0.11827380210161209, min : -0.07677605003118515, max : 0.9647419452667236\n",
      "Bert mean : -0.24299664795398712, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745702266693, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.057366516441106796, std : 0.03411151468753815, min : -0.03779689222574234, max : 0.9018683433532715\n",
      "Bert mean : -0.27198073267936707, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572443068027496, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.06470011174678802, std : 0.1261521726846695, min : -0.02235877886414528, max : 0.9349383115768433\n",
      "Bert mean : 0.14542879164218903, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856133818626404, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.05263237655162811, std : 0.1466101109981537, min : -0.029163256287574768, max : 0.9740216732025146\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754583954811096, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.065766341984272, std : 0.1565222144126892, min : -0.035796865820884705, max : 0.9729412198066711\n",
      "Bert mean : 0.020568503066897392, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.3116267919540405, std : 0.3375121057033539, min : -0.023554904386401176, max : 0.9835547804832458\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175143122673, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.0809682086110115, std : 0.13258694112300873, min : -0.008651929907500744, max : 0.954234778881073\n",
      "Bert mean : -0.23431456089019775, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608236700296402, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : 0.04531552270054817, std : 0.1437550038099289, min : -0.013672673143446445, max : 0.9355769157409668\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.058234404772520065, std : 0.07649566233158112, min : -0.023895151913166046, max : 0.9073539972305298\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.038834597915410995, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.065766341984272, std : 0.1565222144126892, min : -0.035796865820884705, max : 0.9729412198066711\n",
      "Bert mean : 0.02056850492954254, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lama layer 별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.09503074735403061, std : 0.18214407563209534, min : -0.014006875455379486, max : 0.9583102464675903\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963636338710785, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.03006155602633953, std : 0.03521095961332321, min : -0.012020361609756947, max : 0.22559259831905365\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491055458784103, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.035117872059345245, std : 0.022450504824519157, min : -0.007773036602884531, max : 0.21566255390644073\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.037685707211494446, std : 0.1269196718931198, min : -0.024288766086101532, max : 0.9739083647727966\n",
      "Bert mean : -0.0997287854552269, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851363122463226, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.394714891910553, std : 0.3867027461528778, min : -0.0020224626641720533, max : 0.9755245447158813\n",
      "Bert mean : 0.22141341865062714, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352897554636, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.09503073990345001, std : 0.18214407563209534, min : -0.014006875455379486, max : 0.9583102464675903\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963634848594666, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.13072174787521362, std : 0.1841888427734375, min : -0.012220894917845726, max : 0.8170368671417236\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014228403568268, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.022426489740610123, std : 0.04339566454291344, min : -0.005316819529980421, max : 0.7857775092124939\n",
      "Bert mean : -0.09645825624465942, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.04982351139187813, std : 0.05156664177775383, min : -0.017191611230373383, max : 0.9348642230033875\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.14665621519088745, std : 0.23533707857131958, min : -0.00944268237799406, max : 0.9761648774147034\n",
      "Bert mean : 0.008446179330348969, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238015964627266, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.03006155602633953, std : 0.03521095961332321, min : -0.012020361609756947, max : 0.22559259831905365\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491056203842163, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.13072174787521362, std : 0.1841888427734375, min : -0.012220894917845726, max : 0.8170368671417236\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014227658510208, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.05901847034692764, std : 0.17563436925411224, min : -0.004496431443840265, max : 0.930661141872406\n",
      "Bert mean : -0.3473791182041168, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312171295285225, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.037656281143426895, std : 0.02522962912917137, min : -0.003399668959900737, max : 0.14419007301330566\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : 0.0434943325817585, std : 0.10050291568040848, min : -0.015533610247075558, max : 0.5233324766159058\n",
      "Bert mean : -0.04856457933783531, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.03511786460876465, std : 0.022450504824519157, min : -0.007773036602884531, max : 0.21566255390644073\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.022426487877964973, std : 0.04339566454291344, min : -0.005316819529980421, max : 0.7857775092124939\n",
      "Bert mean : -0.09645824879407883, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.059018466621637344, std : 0.17563436925411224, min : -0.004496431443840265, max : 0.930661141872406\n",
      "Bert mean : -0.3473791778087616, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312172133475542, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.08114819973707199, std : 0.19384008646011353, min : -0.004455572459846735, max : 0.9782978892326355\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.11733051389455795, std : 0.2200620323419571, min : -0.001419587410055101, max : 0.899938702583313\n",
      "Bert mean : -0.08600692451000214, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.037685707211494446, std : 0.1269196718931198, min : -0.024288766086101532, max : 0.9739083647727966\n",
      "Bert mean : -0.0997287929058075, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851364612579346, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.04982350766658783, std : 0.05156664177775383, min : -0.017191611230373383, max : 0.9348642230033875\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.037656281143426895, std : 0.02522962912917137, min : -0.003399668959900737, max : 0.14419007301330566\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.08114820718765259, std : 0.19384008646011353, min : -0.004455572459846735, max : 0.9782978892326355\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.0785989761352539, std : 0.18187497556209564, min : -0.003552345559000969, max : 0.9747207760810852\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.3947148621082306, std : 0.3867027461528778, min : -0.0020224626641720533, max : 0.9755245447158813\n",
      "Bert mean : 0.22141343355178833, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352890104055405, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.14665623009204865, std : 0.23533707857131958, min : -0.00944268237799406, max : 0.9761648774147034\n",
      "Bert mean : 0.008446180261671543, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238017827272415, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : 0.0434943288564682, std : 0.10050291568040848, min : -0.015533610247075558, max : 0.5233324766159058\n",
      "Bert mean : -0.04856457561254501, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.11733051389455795, std : 0.2200620323419571, min : -0.001419587410055101, max : 0.899938702583313\n",
      "Bert mean : -0.08600693196058273, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.0785989761352539, std : 0.18187497556209564, min : -0.003552345559000969, max : 0.9747207760810852\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama layer별로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(6)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'].append(l_rep)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
