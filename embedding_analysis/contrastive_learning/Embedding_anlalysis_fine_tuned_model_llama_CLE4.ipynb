{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDataset import MyDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lama_embedding_path = '../llama_embedding/CL/tumor_CL_E4_finetuned_5layers/dataset_tensor/'\n",
    "bert_embedding_path = '../bert_embedding/tumor_finetuned/dataset_tensor/'\n",
    "roberta_embedding_path = '../roberta_embedding/tumor_finetuned/dataset_tensor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset('train', lama_embedding_path, bert_embedding_path, roberta_embedding_path)\n",
    "test_data =MyDataset('test', lama_embedding_path, bert_embedding_path, roberta_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "test_data = DataLoader(test_data, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[-2.5400, -0.5635,  2.1839,  ..., -0.3251, -1.7481, -1.7482],\n",
      "         [-1.8591, -0.0377, -0.2380,  ..., -0.1417, -0.6042, -0.1818],\n",
      "         [-1.6179,  0.0962,  0.3442,  ..., -0.1503, -0.7252, -0.0583],\n",
      "         [-1.1659,  0.0261,  0.2783,  ..., -0.2496, -0.4004, -0.1842],\n",
      "         [-0.2559,  0.0079,  0.5561,  ...,  0.4443, -0.3619, -0.0875]]]), tensor([[-0.8550,  0.6309, -0.6627,  ..., -0.5210,  0.2564, -0.3477]]), tensor([[ 1.7732,  0.9521, -0.7721,  ..., -0.5331, -0.3238, -0.3959]]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_data):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rep, b_rep, r_rep, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_rep.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4096]), torch.Size([1024]), torch.Size([1024]), 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rep.squeeze().shape, b_rep.squeeze().shape, r_rep.squeeze().shape, label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps 별 코사인 유사도 평균 구해보기\n",
    "def get_l1_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).sum()\n",
    "def get_l2_distance(x1, x2):\n",
    "    return ((x1 - x2)**2).sum()**.5\n",
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_cosine_similarity(reps_dict[0]['lama'][0].flatten() , reps_dict[0]['lama'][1].flatten()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 끼리의 코사인 유사도\n",
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.43977591395378113, std : 0.7914059162139893, min : -0.7604721188545227, max : 0.9991500377655029\n",
      "Bert mean : 0.9680062532424927, std : 0.06482237577438354, min : 0.5762061476707458, max : 0.9999645352363586\n",
      "Roberta mean : 0.7679231762886047, std : 0.3273807466030121, min : -0.14232468605041504, max : 0.9999403357505798\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.8011625409126282, std : 0.5161224603652954, min : -0.7593939900398254, max : 0.9996709227561951\n",
      "Bert mean : 0.967339038848877, std : 0.11414065212011337, min : 0.08446459472179413, max : 0.9999260902404785\n",
      "Roberta mean : 0.9764479398727417, std : 0.1192246600985527, min : 0.14114268124103546, max : 0.9999465346336365\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.968824565410614, std : 0.045308277010917664, min : 0.7218787670135498, max : 0.999514639377594\n",
      "Bert mean : 0.9849059581756592, std : 0.014416240155696869, min : 0.9077816605567932, max : 1.0\n",
      "Roberta mean : 0.9430029988288879, std : 0.08131575584411621, min : 0.4610406756401062, max : 0.9998674392700195\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.9455048441886902, std : 0.24026699364185333, min : -0.775479257106781, max : 0.9995784759521484\n",
      "Bert mean : 0.8764873147010803, std : 0.19998440146446228, min : -0.4412735402584076, max : 0.9999936819076538\n",
      "Roberta mean : 0.9174246191978455, std : 0.14946486055850983, min : 0.05138710141181946, max : 0.9999562501907349\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.9393671751022339, std : 0.23258209228515625, min : -0.5970863699913025, max : 0.9998573660850525\n",
      "Bert mean : 0.9256547689437866, std : 0.1994471251964569, min : -0.47379860281944275, max : 0.9999668598175049\n",
      "Roberta mean : 0.8538801670074463, std : 0.23610827326774597, min : -0.15768025815486908, max : 0.9999735355377197\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.6053222417831421, std : 0.6663744449615479, min : -0.7641728520393372, max : 0.9995914101600647\n",
      "Bert mean : 0.9804783463478088, std : 0.02478441223502159, min : 0.7493387460708618, max : 0.9998576641082764\n",
      "Roberta mean : 0.7132827639579773, std : 0.3693503737449646, min : -0.2799872159957886, max : 0.9978198409080505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.46736282110214233, std : 0.7935091853141785, min : -0.7632442116737366, max : 0.9985213875770569\n",
      "Bert mean : 0.8336034417152405, std : 0.3186386525630951, min : 0.08265118300914764, max : 0.9998219013214111\n",
      "Roberta mean : 0.6699555516242981, std : 0.3867779076099396, min : -0.21332025527954102, max : 0.9997442364692688\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.9882259368896484, std : 0.009410900995135307, min : 0.9517926573753357, max : 0.9993868470191956\n",
      "Bert mean : 0.7791711091995239, std : 0.4013894498348236, min : -0.16923663020133972, max : 0.9998753070831299\n",
      "Roberta mean : 0.9898001551628113, std : 0.011732799932360649, min : 0.9500171542167664, max : 0.9999515414237976\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9773823022842407, std : 0.016380999237298965, min : 0.9340450167655945, max : 0.9956802129745483\n",
      "Bert mean : 0.9522998332977295, std : 0.059857457876205444, min : 0.6458935737609863, max : 0.9971312284469604\n",
      "Roberta mean : 0.6465266346931458, std : 0.39903706312179565, min : -0.11720796674489975, max : 0.9968264102935791\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.9846405982971191, std : 0.006163137964904308, min : 0.95938640832901, max : 0.9985459446907043\n",
      "Bert mean : 0.8970180153846741, std : 0.0863928273320198, min : 0.6242274045944214, max : 0.999889612197876\n",
      "Roberta mean : 0.8786481618881226, std : 0.22249220311641693, min : 0.08519376814365387, max : 0.999923825263977\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.9819396734237671, std : 0.01145982090383768, min : 0.9280980229377747, max : 0.9978293776512146\n",
      "Bert mean : 0.6472193002700806, std : 0.46094951033592224, min : -0.22326429188251495, max : 0.9991183280944824\n",
      "Roberta mean : 0.7454431056976318, std : 0.34253740310668945, min : -0.050079647451639175, max : 0.9964597225189209\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.3569570779800415, std : 0.7017876505851746, min : -0.7617502808570862, max : 0.9969576001167297\n",
      "Bert mean : 0.5748860836029053, std : 0.49861255288124084, min : -0.22613559663295746, max : 0.9996704459190369\n",
      "Roberta mean : 0.5188899636268616, std : 0.45197492837905884, min : -0.2699298560619354, max : 0.9902639985084534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Layer 별 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)\n",
    "# # llama layer별\n",
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict[idx]['lama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 라벨들과의 코사인 유사도 비교\n",
    "## TrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.5887658596038818, std : 0.7115778923034668, min : -0.7687420845031738, max : 0.9971274137496948\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390362352132797, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : -0.3802506923675537, std : 0.6574790477752686, min : -0.7780069708824158, max : 0.9958550333976746\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : -0.37041348218917847, std : 0.6687567830085754, min : -0.7687517404556274, max : 0.9967846870422363\n",
      "Bert mean : 0.01212023664265871, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777684669941664, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : -0.36838197708129883, std : 0.6647058725357056, min : -0.7614229917526245, max : 0.9945594668388367\n",
      "Bert mean : -0.24299663305282593, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745851278305, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.5182442665100098, std : 0.7374114990234375, min : -0.7697904706001282, max : 0.9977679252624512\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175329387188, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.5887659192085266, std : 0.7115778923034668, min : -0.7687420845031738, max : 0.9971274137496948\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390361607074738, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : -0.6147043108940125, std : 0.3719882667064667, min : -0.7832543253898621, max : 0.9942055940628052\n",
      "Bert mean : -0.22500866651535034, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239936351776123, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : -0.6016719341278076, std : 0.40192753076553345, min : -0.7737839221954346, max : 0.9936873912811279\n",
      "Bert mean : -0.08799591660499573, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : -0.5979776382446289, std : 0.40125545859336853, min : -0.7679585218429565, max : 0.9942761063575745\n",
      "Bert mean : -0.2719807028770447, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572444558143616, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.6955443620681763, std : 0.6088668704032898, min : -0.7627211809158325, max : 0.9973911643028259\n",
      "Bert mean : -0.23431454598903656, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608237445354462, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : -0.3802506625652313, std : 0.6574790477752686, min : -0.7780069708824158, max : 0.9958550333976746\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : -0.6147043704986572, std : 0.3719882667064667, min : -0.7832543253898621, max : 0.9942055940628052\n",
      "Bert mean : -0.22500869631767273, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239934489130974, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.9555784463882446, std : 0.1724865585565567, min : -0.7827993631362915, max : 0.9980766773223877\n",
      "Bert mean : -0.3086966276168823, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.9523011445999146, std : 0.16828826069831848, min : -0.6187198758125305, max : 0.9975693225860596\n",
      "Bert mean : 0.14542877674102783, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856135308742523, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : -0.4913441836833954, std : 0.5355568528175354, min : -0.7860621809959412, max : 0.9960463047027588\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : -0.37041348218917847, std : 0.6687567830085754, min : -0.7687517404556274, max : 0.9967846870422363\n",
      "Bert mean : 0.012120235711336136, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777686066925526, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : -0.6016718149185181, std : 0.40192753076553345, min : -0.7737839221954346, max : 0.9936873912811279\n",
      "Bert mean : -0.08799590915441513, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.9555785655975342, std : 0.1724865585565567, min : -0.7827993631362915, max : 0.9980766773223877\n",
      "Bert mean : -0.30869656801223755, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.9405109286308289, std : 0.23480045795440674, min : -0.7665165662765503, max : 0.998794674873352\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754584550857544, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : -0.47977015376091003, std : 0.5520321130752563, min : -0.7757136225700378, max : 0.9960373640060425\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.0388345941901207, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : -0.36838191747665405, std : 0.6647058725357056, min : -0.7614229917526245, max : 0.9945594668388367\n",
      "Bert mean : -0.24299664795398712, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745702266693, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : -0.5979776382446289, std : 0.40125545859336853, min : -0.7679585218429565, max : 0.9942761063575745\n",
      "Bert mean : -0.27198073267936707, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572443068027496, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.9523011445999146, std : 0.16828826069831848, min : -0.6187198758125305, max : 0.9975693225860596\n",
      "Bert mean : 0.14542879164218903, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856133818626404, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.9405109286308289, std : 0.23480045795440674, min : -0.7665165662765503, max : 0.998794674873352\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754583954811096, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : -0.4767317771911621, std : 0.5499328970909119, min : -0.7696444392204285, max : 0.9943721294403076\n",
      "Bert mean : 0.020568503066897392, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.5182442665100098, std : 0.7374114990234375, min : -0.7697904706001282, max : 0.9977679252624512\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175143122673, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.6955443620681763, std : 0.6088668704032898, min : -0.7627211809158325, max : 0.9973911643028259\n",
      "Bert mean : -0.23431456089019775, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608236700296402, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : -0.4913441836833954, std : 0.5355568528175354, min : -0.7860621809959412, max : 0.9960463047027588\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : -0.47977015376091003, std : 0.5520321130752563, min : -0.7757136225700378, max : 0.9960373640060425\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.038834597915410995, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : -0.4767318069934845, std : 0.5499328970909119, min : -0.7696444392204285, max : 0.9943721294403076\n",
      "Bert mean : 0.02056850492954254, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lama layer 별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.6844321489334106, std : 0.6550635099411011, min : -0.7639706134796143, max : 0.9969674944877625\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963636338710785, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : -0.4191645383834839, std : 0.6363650560379028, min : -0.7745843529701233, max : 0.9934712648391724\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491055458784103, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : -0.4204787313938141, std : 0.6385104656219482, min : -0.7524959444999695, max : 0.9924899935722351\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : -0.418876588344574, std : 0.6350128650665283, min : -0.7435424327850342, max : 0.9946752786636353\n",
      "Bert mean : -0.0997287854552269, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851363122463226, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.41779977083206177, std : 0.7392277121543884, min : -0.7646307349205017, max : 0.9971415400505066\n",
      "Bert mean : 0.22141341865062714, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352897554636, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.6844321489334106, std : 0.6550635099411011, min : -0.7639706134796143, max : 0.9969674944877625\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963634848594666, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : -0.7115831971168518, std : 0.028145818039774895, min : -0.7738584876060486, max : -0.5745166540145874\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014228403568268, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : -0.714145302772522, std : 0.022014927119016647, min : -0.7547801733016968, max : -0.6022094488143921\n",
      "Bert mean : -0.09645825624465942, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : -0.7110345959663391, std : 0.023977413773536682, min : -0.7432199716567993, max : -0.5841096043586731\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.5700749754905701, std : 0.6589452028274536, min : -0.761959969997406, max : 0.9971933960914612\n",
      "Bert mean : 0.008446179330348969, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238015964627266, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : -0.4191645681858063, std : 0.6363650560379028, min : -0.7745843529701233, max : 0.9934712648391724\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491056203842163, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : -0.7115831971168518, std : 0.028145818039774895, min : -0.7738584876060486, max : -0.5745166540145874\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014227658510208, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.9796581864356995, std : 0.011556213721632957, min : 0.9372343420982361, max : 0.9955874085426331\n",
      "Bert mean : -0.3473791182041168, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312171295285225, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.9778827428817749, std : 0.01445804163813591, min : 0.9174195528030396, max : 0.9951232075691223\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : -0.2868359386920929, std : 0.6504762768745422, min : -0.775212287902832, max : 0.988048255443573\n",
      "Bert mean : -0.04856457933783531, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : -0.4204787015914917, std : 0.6385104656219482, min : -0.7524959444999695, max : 0.9924899935722351\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : -0.714145302772522, std : 0.022014927119016647, min : -0.7547801733016968, max : -0.6022094488143921\n",
      "Bert mean : -0.09645824879407883, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.9796581268310547, std : 0.011556213721632957, min : 0.9372343420982361, max : 0.9955874085426331\n",
      "Bert mean : -0.3473791778087616, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312172133475542, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.981576681137085, std : 0.008730250410735607, min : 0.9380807280540466, max : 0.9954671859741211\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : -0.28778398036956787, std : 0.6527166366577148, min : -0.7540924549102783, max : 0.9885466694831848\n",
      "Bert mean : -0.08600692451000214, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : -0.41887661814689636, std : 0.6350128650665283, min : -0.7435424327850342, max : 0.9946752786636353\n",
      "Bert mean : -0.0997287929058075, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851364612579346, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : -0.7110345363616943, std : 0.023977413773536682, min : -0.7432199716567993, max : -0.5841096043586731\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.9778827428817749, std : 0.01445804163813591, min : 0.9174195528030396, max : 0.9951232075691223\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.9815766215324402, std : 0.008730250410735607, min : 0.9380807280540466, max : 0.9954671859741211\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : -0.2865201234817505, std : 0.6501327157020569, min : -0.7446986436843872, max : 0.9877040386199951\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.4177997410297394, std : 0.7392277121543884, min : -0.7646307349205017, max : 0.9971415400505066\n",
      "Bert mean : 0.22141343355178833, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352890104055405, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.5700749754905701, std : 0.6589452028274536, min : -0.761959969997406, max : 0.9971933960914612\n",
      "Bert mean : 0.008446180261671543, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238017827272415, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : -0.2868359386920929, std : 0.6504762768745422, min : -0.775212287902832, max : 0.988048255443573\n",
      "Bert mean : -0.04856457561254501, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : -0.28778398036956787, std : 0.6527166366577148, min : -0.7540924549102783, max : 0.9885466694831848\n",
      "Bert mean : -0.08600693196058273, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : -0.2865201234817505, std : 0.6501327157020569, min : -0.7446986436843872, max : 0.9877040386199951\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama layer별로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m r_rep \u001b[38;5;241m=\u001b[39m r_rep\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mreps_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlama\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(l_rep)\n\u001b[1;32m     13\u001b[0m reps_dict[label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(b_rep)\n\u001b[1;32m     14\u001b[0m reps_dict[label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(r_rep)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : {j : [] for j in range(6)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
