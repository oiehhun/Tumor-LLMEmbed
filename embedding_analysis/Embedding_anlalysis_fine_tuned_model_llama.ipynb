{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDataset import MyDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lama_embedding_path = '../llama_embedding/tumor_3.1_CL_finetuned/dataset_tensor/'\n",
    "bert_embedding_path = '../bert_embedding/tumor_finetuned/dataset_tensor/'\n",
    "roberta_embedding_path = '../roberta_embedding/tumor_finetuned/dataset_tensor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset('train', lama_embedding_path, bert_embedding_path, roberta_embedding_path)\n",
    "test_data =MyDataset('test', lama_embedding_path, bert_embedding_path, roberta_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "test_data = DataLoader(test_data, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[-1.5044,  0.6670,  2.3295,  ...,  0.9674, -2.3429,  1.8316],\n",
      "         [-0.4019,  0.1248,  0.0830,  ...,  0.0686, -0.6041,  0.1625],\n",
      "         [-0.4163,  0.2404,  0.1163,  ..., -0.1729, -0.4311,  0.2859],\n",
      "         [-0.4265,  0.3218,  0.1691,  ..., -0.0411, -0.4049,  0.1611],\n",
      "         [-0.2563,  0.5020,  0.2488,  ...,  0.0147, -0.3530,  0.1012]]]), tensor([[-0.0811, -0.7083,  0.8326,  ..., -0.6350,  0.9180,  0.5636]]), tensor([[-0.5352,  0.6397, -0.1189,  ..., -1.5615, -1.9301, -1.0474]]), tensor([3])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_data):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rep, b_rep, r_rep, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_rep.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4096]), torch.Size([1024]), torch.Size([1024]), 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rep.squeeze().shape, b_rep.squeeze().shape, r_rep.squeeze().shape, label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps 별 코사인 유사도 평균 구해보기\n",
    "def get_l1_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).sum()\n",
    "def get_l2_distance(x1, x2):\n",
    "    return ((x1 - x2)**2).sum()**.5\n",
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_cosine_similarity(reps_dict[0]['lama'][0].flatten() , reps_dict[0]['lama'][1].flatten()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 끼리의 코사인 유사도\n",
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.48030292987823486, std : 0.3244469463825226, min : -0.02666034922003746, max : 0.9885945320129395\n",
      "Bert mean : 0.9927564263343811, std : 0.00888011883944273, min : 0.9261558651924133, max : 0.9999865889549255\n",
      "Roberta mean : 0.9656540751457214, std : 0.08705567568540573, min : 0.37397775053977966, max : 0.9999938011169434\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.7042180299758911, std : 0.16205398738384247, min : -0.020111149176955223, max : 0.9940289855003357\n",
      "Bert mean : 0.9966709017753601, std : 0.0028710165061056614, min : 0.9811343550682068, max : 0.9999918937683105\n",
      "Roberta mean : 0.9827726483345032, std : 0.014596483670175076, min : 0.8483975529670715, max : 0.9997824430465698\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.5735873579978943, std : 0.2338789403438568, min : -0.04098382592201233, max : 0.9748389720916748\n",
      "Bert mean : 0.9953146576881409, std : 0.008820051327347755, min : 0.9241839647293091, max : 1.0000001192092896\n",
      "Roberta mean : 0.9897527098655701, std : 0.0062699466943740845, min : 0.9594651460647583, max : 0.9998767971992493\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.7312836647033691, std : 0.18545478582382202, min : 0.04870292544364929, max : 0.9952909350395203\n",
      "Bert mean : 0.9902612566947937, std : 0.013682028278708458, min : 0.8573510050773621, max : 0.9999902844429016\n",
      "Roberta mean : 0.9233803153038025, std : 0.09234721958637238, min : 0.40024492144584656, max : 0.9999734163284302\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.6932483911514282, std : 0.202296182513237, min : 0.0004357116704341024, max : 0.9967086315155029\n",
      "Bert mean : 0.997686505317688, std : 0.003138964297249913, min : 0.9765340089797974, max : 0.9999982714653015\n",
      "Roberta mean : 0.9907273650169373, std : 0.008725536987185478, min : 0.9164097905158997, max : 0.9999611973762512\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.5324814915657043, std : 0.28050127625465393, min : -0.051555782556533813, max : 0.9788541793823242\n",
      "Bert mean : 0.9866918325424194, std : 0.021993299946188927, min : 0.8248568773269653, max : 0.9998867511749268\n",
      "Roberta mean : 0.945020318031311, std : 0.08781798928976059, min : 0.3872833251953125, max : 0.9998385906219482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.4840444326400757, std : 0.3246239125728607, min : -0.015935935080051422, max : 0.9873759150505066\n",
      "Bert mean : 0.8825313448905945, std : 0.3595038950443268, min : -0.2971953749656677, max : 0.9997621774673462\n",
      "Roberta mean : 0.9556592106819153, std : 0.05133352428674698, min : 0.7699362635612488, max : 0.999946117401123\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.598544716835022, std : 0.2573408782482147, min : -0.023980149999260902, max : 0.9900539517402649\n",
      "Bert mean : 0.6866618394851685, std : 0.4628823399543762, min : -0.3245903551578522, max : 0.999981701374054\n",
      "Roberta mean : 0.6968095302581787, std : 0.39791131019592285, min : -0.018198715522885323, max : 0.9996711015701294\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.6018698215484619, std : 0.2203962653875351, min : 0.006546007469296455, max : 0.980207622051239\n",
      "Bert mean : 0.9619156122207642, std : 0.06619256734848022, min : 0.7581995129585266, max : 0.9999444484710693\n",
      "Roberta mean : 0.984171986579895, std : 0.021882247179746628, min : 0.8971543312072754, max : 0.9998447895050049\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.7617293000221252, std : 0.21137744188308716, min : 0.06798982620239258, max : 0.9997273087501526\n",
      "Bert mean : 0.8803348541259766, std : 0.3041750192642212, min : -0.1790059208869934, max : 0.9999881386756897\n",
      "Roberta mean : 0.7136609554290771, std : 0.37764349579811096, min : -0.04104319587349892, max : 0.9999953508377075\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.7093899846076965, std : 0.19510860741138458, min : 0.1327100694179535, max : 0.9226217865943909\n",
      "Bert mean : 0.8238371014595032, std : 0.382494181394577, min : -0.11306235939264297, max : 0.9998080134391785\n",
      "Roberta mean : 0.8235276937484741, std : 0.3699018657207489, min : -0.02122327871620655, max : 0.9977568984031677\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.42194095253944397, std : 0.319303423166275, min : -0.016377314925193787, max : 0.9659274816513062\n",
      "Bert mean : 0.42792582511901855, std : 0.5251755118370056, min : -0.3752881586551666, max : 0.9997733235359192\n",
      "Roberta mean : 0.5350468754768372, std : 0.46039333939552307, min : -0.08681812882423401, max : 0.9991058707237244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Layer 별 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    l_rep_0 = l_rep[0]\n",
    "    l_rep_1 = l_rep[1]\n",
    "    l_rep_2 = l_rep[2]\n",
    "    l_rep_3 = l_rep[3]\n",
    "    l_rep_4 = l_rep[4]\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "    reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "    reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "    reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "    reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['bert'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)\n",
    "# llama layer별\n",
    "for idx in range(6):\n",
    "    for i in range(5): # lama layer별\n",
    "        lama_cosine = []\n",
    "        for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "            for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "                lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "        all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'lama': {0: tensor([ 0.6344,  0.1352,  0.8553,  ...,  0.6920,  0.0605, -0.0425]),\n",
       "   1: tensor([0.6758, 0.5113, 0.8539,  ..., 0.7890, 0.3661, 0.2882]),\n",
       "   2: tensor([0.7249, 0.5905, 0.8682,  ..., 0.8361, 0.3782, 0.3104]),\n",
       "   3: tensor([0.7350, 0.6311, 0.8764,  ..., 0.8465, 0.3890, 0.3157]),\n",
       "   4: tensor([0.7694, 0.6668, 0.8960,  ..., 0.8641, 0.3984, 0.3262])},\n",
       "  'bert': tensor([0.9965, 0.9817, 0.9993,  ..., 0.9832, 0.9956, 0.9825]),\n",
       "  'roberta': tensor([0.9552, 0.9433, 0.9975,  ..., 0.9304, 0.9637, 0.9434])},\n",
       " 1: {'lama': {0: tensor([0.6794, 0.7393, 0.7902,  ..., 0.3354, 0.5700, 0.1227]),\n",
       "   1: tensor([0.7381, 0.7872, 0.8148,  ..., 0.6435, 0.6945, 0.4575]),\n",
       "   2: tensor([0.7814, 0.8233, 0.8422,  ..., 0.6785, 0.7457, 0.5053]),\n",
       "   3: tensor([0.7903, 0.8447, 0.8545,  ..., 0.7105, 0.7683, 0.5502]),\n",
       "   4: tensor([0.8157, 0.8703, 0.8731,  ..., 0.7335, 0.7970, 0.5872])},\n",
       "  'bert': tensor([0.9982, 0.9992, 0.9981,  ..., 0.9944, 0.9976, 0.9978]),\n",
       "  'roberta': tensor([0.9901, 0.9853, 0.9857,  ..., 0.9725, 0.9812, 0.9893])},\n",
       " 2: {'lama': {0: tensor([0.4869, 0.5547, 0.3585,  ..., 0.4505, 0.4686, 0.3470]),\n",
       "   1: tensor([0.5907, 0.6589, 0.4804,  ..., 0.5583, 0.6050, 0.5290]),\n",
       "   2: tensor([0.6112, 0.6913, 0.5053,  ..., 0.5704, 0.6430, 0.5118]),\n",
       "   3: tensor([0.6266, 0.7084, 0.5365,  ..., 0.6024, 0.6473, 0.5293]),\n",
       "   4: tensor([0.6474, 0.7307, 0.5653,  ..., 0.6254, 0.6779, 0.5334])},\n",
       "  'bert': tensor([0.9980, 0.9971, 0.9970,  ..., 0.9979, 0.9347, 0.9385]),\n",
       "  'roberta': tensor([0.9858, 0.9661, 0.9872,  ..., 0.9875, 0.9700, 0.9795])},\n",
       " 3: {'lama': {0: tensor([0.6586, 0.6324, 0.6949,  ..., 0.8539, 0.8389, 0.9040]),\n",
       "   1: tensor([0.7373, 0.7468, 0.8259,  ..., 0.8809, 0.8673, 0.9375]),\n",
       "   2: tensor([0.7539, 0.7557, 0.8381,  ..., 0.8885, 0.8722, 0.9405]),\n",
       "   3: tensor([0.7813, 0.7869, 0.8507,  ..., 0.9006, 0.8849, 0.9496]),\n",
       "   4: tensor([0.7875, 0.7965, 0.8643,  ..., 0.9059, 0.8910, 0.9561])},\n",
       "  'bert': tensor([0.9942, 0.9883, 0.9853,  ..., 0.9836, 0.9498, 0.8874]),\n",
       "  'roberta': tensor([0.9777, 0.9704, 0.9749,  ..., 0.9085, 0.6173, 0.5407])},\n",
       " 4: {'lama': {0: tensor([0.6564, 0.7634, 0.7922,  ..., 0.1420, 0.6931, 0.1387]),\n",
       "   1: tensor([0.7106, 0.7953, 0.8774,  ..., 0.3412, 0.7228, 0.4724]),\n",
       "   2: tensor([0.7232, 0.7987, 0.8849,  ..., 0.3468, 0.7313, 0.5087]),\n",
       "   3: tensor([0.7443, 0.8147, 0.9017,  ..., 0.3797, 0.7517, 0.5411]),\n",
       "   4: tensor([0.7579, 0.8237, 0.9097,  ..., 0.4090, 0.7698, 0.5830])},\n",
       "  'bert': tensor([0.9995, 0.9993, 0.9989,  ..., 0.9978, 0.9990, 0.9985]),\n",
       "  'roberta': tensor([0.9975, 0.9948, 0.9950,  ..., 0.9869, 0.9929, 0.9934])},\n",
       " 5: {'lama': {0: tensor([0.7949, 0.8341, 0.1640,  ..., 0.6137, 0.6889, 0.7152]),\n",
       "   1: tensor([0.8552, 0.8647, 0.5082,  ..., 0.7475, 0.8038, 0.7968]),\n",
       "   2: tensor([0.8793, 0.8891, 0.5851,  ..., 0.7843, 0.8373, 0.8319]),\n",
       "   3: tensor([0.8915, 0.8940, 0.6300,  ..., 0.8020, 0.8503, 0.8426]),\n",
       "   4: tensor([0.9097, 0.9108, 0.6745,  ..., 0.8297, 0.8737, 0.8672])},\n",
       "  'bert': tensor([0.9972, 0.9955, 0.9887,  ..., 0.9791, 0.9985, 0.9848]),\n",
       "  'roberta': tensor([0.9243, 0.9956, 0.9495,  ..., 0.9164, 0.9910, 0.9235])}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [tensor([ 0.9748,  1.0260,  3.0652,  ..., -0.2705,  0.3263,  1.3657]),\n",
       "  tensor([ 0.0175,  2.6631,  0.5847,  ...,  0.8788, -0.1573,  1.0420]),\n",
       "  tensor([ 1.2309,  1.1161,  3.4340,  ..., -0.6855,  1.3896,  1.5609]),\n",
       "  tensor([-1.2121,  0.1060,  1.4803,  ..., -0.7845,  0.6384,  1.6443]),\n",
       "  tensor([ 1.0451,  2.5450,  2.2916,  ..., -0.0995, -0.9983,  1.1426]),\n",
       "  tensor([0.9411, 0.8431, 2.2509,  ..., 0.1797, 0.2007, 1.6376]),\n",
       "  tensor([ 1.2576,  0.4515,  2.2577,  ..., -0.1496,  0.0136,  1.8808]),\n",
       "  tensor([ 0.6587, -0.6355,  3.5464,  ...,  0.6583,  0.5341,  1.4443]),\n",
       "  tensor([ 0.6783,  0.7595,  3.3668,  ..., -1.6063,  0.3123,  1.1846]),\n",
       "  tensor([ 2.0099, -0.3052,  3.0834,  ...,  0.4188,  0.9428,  2.9046]),\n",
       "  tensor([2.5103, 0.2459, 1.9508,  ..., 0.8245, 0.1666, 1.2920]),\n",
       "  tensor([2.4977, 0.7005, 0.7694,  ..., 0.7291, 1.5053, 2.4429]),\n",
       "  tensor([-0.8598,  0.3640,  1.9714,  ...,  1.4377, -0.6585,  0.7879]),\n",
       "  tensor([-1.1287,  0.6013,  2.5646,  ...,  2.4176, -2.2848,  0.3262]),\n",
       "  tensor([-0.0811,  1.8095,  1.9007,  ...,  0.7148, -2.3227,  1.5717]),\n",
       "  tensor([0.9104, 1.3741, 1.1598,  ..., 0.1507, 0.8687, 1.0064]),\n",
       "  tensor([-0.3087, -0.1856,  0.8535,  ...,  0.6464, -1.8451,  1.1046]),\n",
       "  tensor([ 1.7260,  0.6618,  2.0156,  ..., -0.5167,  1.0236,  0.1474]),\n",
       "  tensor([-0.4683, -0.2477,  1.2672,  ...,  0.3360, -1.9111,  1.2188]),\n",
       "  tensor([-0.0779,  0.9404,  1.3626,  ...,  0.6328, -0.3822,  2.6069]),\n",
       "  tensor([-1.6490, -0.4169,  2.0130,  ...,  1.1410, -1.0777,  0.7011]),\n",
       "  tensor([-0.8356,  0.0523,  1.3787,  ...,  0.5464, -1.6607,  1.0637]),\n",
       "  tensor([-0.1563,  0.6333,  0.6600,  ...,  0.9564, -1.1574,  1.2542]),\n",
       "  tensor([ 0.3297,  1.4094,  1.9527,  ..., -1.5609, -0.2466,  1.5684]),\n",
       "  tensor([ 0.2554,  1.8571,  3.5662,  ..., -0.6301,  0.1732,  0.7060]),\n",
       "  tensor([-2.5922,  1.1739,  1.1909,  ...,  0.9144, -1.6335,  0.4953]),\n",
       "  tensor([1.6835, 0.8690, 2.7105,  ..., 0.0311, 0.7484, 1.2612]),\n",
       "  tensor([-0.8791, -0.2074,  1.4101,  ...,  0.3389, -1.7340,  1.3092]),\n",
       "  tensor([ 1.3155,  0.7581,  2.6397,  ..., -0.7352,  0.2703,  1.3122]),\n",
       "  tensor([ 0.2532, -0.0125,  3.1662,  ...,  0.4574,  0.4925,  1.1996]),\n",
       "  tensor([-0.0048,  2.8003,  2.9600,  ..., -0.1085, -0.4028,  1.2473]),\n",
       "  tensor([ 2.8266, -0.0336,  3.6422,  ..., -0.0191,  1.1360,  1.3888]),\n",
       "  tensor([ 0.6436, -0.0549,  1.4815,  ..., -0.0775, -0.1534,  1.4867]),\n",
       "  tensor([ 1.3815,  0.6708,  2.3527,  ..., -1.7664,  0.8590,  1.1719]),\n",
       "  tensor([1.8115, 0.3370, 2.5277,  ..., 0.1514, 1.2122, 1.5534]),\n",
       "  tensor([ 1.9805,  0.5539,  2.5021,  ..., -0.5886,  0.4029,  1.4898]),\n",
       "  tensor([ 0.7715,  1.9100,  2.2331,  ..., -1.1767, -1.8034,  2.8701]),\n",
       "  tensor([-0.0729,  3.2422,  1.8233,  ...,  0.6649,  0.4944,  0.5434]),\n",
       "  tensor([-0.7972, -0.3141,  1.1918,  ...,  0.3330, -1.6416,  1.4258]),\n",
       "  tensor([ 0.9080, -0.3737,  2.3913,  ...,  0.2808,  0.8783,  2.2981]),\n",
       "  tensor([ 0.4215,  1.3062,  1.4439,  ..., -0.4000, -0.6522,  3.0751]),\n",
       "  tensor([ 2.0873,  0.7907,  1.9448,  ..., -1.4163,  1.4384,  1.9643]),\n",
       "  tensor([ 1.1160,  0.3394,  2.2211,  ..., -0.6548,  0.5389,  1.1893]),\n",
       "  tensor([-1.7338,  0.4585,  3.0421,  ...,  0.7461, -0.1686,  0.9701]),\n",
       "  tensor([-0.0337,  1.1272,  2.0003,  ...,  0.5666, -0.6744,  2.3924]),\n",
       "  tensor([-0.8035,  1.6150,  2.9078,  ..., -1.5724,  0.2055,  2.0420]),\n",
       "  tensor([-0.4034,  1.8958,  1.5489,  ...,  0.9249, -2.1631,  2.0318]),\n",
       "  tensor([ 0.2012,  0.7651,  1.6176,  ...,  0.8487, -0.3822,  2.1034]),\n",
       "  tensor([-0.2706,  0.4583,  1.9957,  ..., -0.8263, -1.1988,  1.8611]),\n",
       "  tensor([0.1180, 2.1938, 3.1412,  ..., 0.3760, 0.4659, 0.2686]),\n",
       "  tensor([-3.6868,  1.4807,  3.0348,  ...,  1.2373, -2.0533,  0.0346]),\n",
       "  tensor([ 0.5585,  3.1929,  2.7795,  ..., -1.3462,  0.0514,  0.9119]),\n",
       "  tensor([ 0.8074,  0.5503,  3.0182,  ..., -1.4936,  1.6017,  1.9821]),\n",
       "  tensor([1.6038, 1.3296, 2.5958,  ..., 0.5795, 0.7419, 0.8387]),\n",
       "  tensor([1.3625, 0.1139, 1.9761,  ..., 1.2768, 1.4641, 1.5631]),\n",
       "  tensor([ 0.3697, -0.4079,  4.1437,  ..., -0.4634,  1.3708,  0.6113]),\n",
       "  tensor([ 1.2781, -0.5328,  2.6096,  ..., -0.5159,  0.3732,  0.8300]),\n",
       "  tensor([-0.6555, -0.0097,  0.6051,  ...,  0.0291, -0.0967,  1.4633]),\n",
       "  tensor([ 0.8241,  1.4248,  2.2143,  ..., -1.4442,  1.3307,  2.2000]),\n",
       "  tensor([ 0.6639,  1.4144,  3.6986,  ..., -0.6223,  0.2060,  1.8110]),\n",
       "  tensor([ 1.0814,  0.9046,  3.0724,  ..., -0.2712,  1.4202,  1.9512]),\n",
       "  tensor([ 0.8224,  0.5358,  3.4050,  ..., -1.4252, -0.1272,  2.0877]),\n",
       "  tensor([ 0.4554,  0.9286,  2.1035,  ..., -1.4826,  0.2835,  2.0766]),\n",
       "  tensor([-2.1553,  0.8998,  1.2333,  ...,  0.9341,  0.8676,  0.6065]),\n",
       "  tensor([-1.2961, -0.1790,  1.7169,  ...,  0.7571, -1.3880,  0.5117]),\n",
       "  tensor([ 0.7726,  2.1095,  2.6313,  ..., -0.6433,  0.6043,  0.4743]),\n",
       "  tensor([ 0.5978,  2.8286,  3.3933,  ..., -0.9322,  0.8160,  0.2773]),\n",
       "  tensor([ 0.3480,  0.8469,  2.5431,  ..., -0.1193,  1.3557,  1.3057]),\n",
       "  tensor([ 1.1160,  0.2605,  1.7015,  ...,  0.7162, -0.3387,  1.5186]),\n",
       "  tensor([ 0.8795,  1.0728,  1.9583,  ...,  0.5471, -0.5243,  1.0879]),\n",
       "  tensor([ 0.3330,  0.6505,  2.0247,  ..., -0.9769,  0.6621,  0.3665]),\n",
       "  tensor([-1.3369,  2.5559,  1.4875,  ..., -0.3031, -1.2618,  1.3404]),\n",
       "  tensor([ 0.2398,  0.5620,  0.4306,  ..., -0.1533, -0.0425,  1.6069]),\n",
       "  tensor([ 1.6237,  0.8774,  2.4006,  ..., -1.2065,  0.6489,  0.3482]),\n",
       "  tensor([ 1.3137, -0.2711,  2.2069,  ...,  0.0981,  1.5625,  0.7613]),\n",
       "  tensor([-1.9824,  0.9954,  3.1944,  ...,  1.7514, -3.0042,  0.8948]),\n",
       "  tensor([ 1.3821,  0.8869,  1.9139,  ..., -1.6819,  2.3472,  1.7821]),\n",
       "  tensor([ 0.5969,  1.3479,  2.8116,  ..., -0.5561,  0.6234,  1.8319]),\n",
       "  tensor([ 1.6889,  0.1075,  3.3524,  ..., -1.1906,  0.1518,  1.3975]),\n",
       "  tensor([-1.6131,  1.6543,  3.2357,  ...,  1.2335, -2.1634,  0.6824]),\n",
       "  tensor([ 1.3101,  1.6451,  3.0023,  ..., -0.6844,  0.0979,  1.5762]),\n",
       "  tensor([ 1.9438,  0.1112,  2.7453,  ..., -0.5069,  0.5247,  1.6527]),\n",
       "  tensor([-0.8458,  1.1130,  2.4554,  ...,  1.5105, -0.3620,  1.3936]),\n",
       "  tensor([ 1.2904,  0.5305,  1.2119,  ..., -1.2894,  0.6339,  1.5515]),\n",
       "  tensor([ 1.2523, -0.2588,  2.1327,  ..., -0.4489, -0.1655,  1.3363]),\n",
       "  tensor([-0.0668,  0.3270,  1.5739,  ...,  0.4939,  0.2673,  1.3338]),\n",
       "  tensor([ 2.1845,  0.1012,  4.0167,  ...,  0.1219,  0.6005, -0.1294]),\n",
       "  tensor([0.8945, 0.2351, 1.6077,  ..., 0.1091, 0.4396, 1.8778]),\n",
       "  tensor([ 1.2943,  0.6203,  2.6045,  ..., -0.3674,  0.0926,  1.7323])],\n",
       " 1: [tensor([ 0.0094, -0.0187, -0.0330,  ..., -0.2285,  0.5398,  0.2104]),\n",
       "  tensor([-0.0977,  0.1431, -0.2325,  ..., -0.2574,  0.4276,  0.1136]),\n",
       "  tensor([-0.0497, -0.0312,  0.0575,  ..., -0.1329,  0.5110,  0.2903]),\n",
       "  tensor([-0.3651,  0.1266, -0.0918,  ..., -0.3202, -0.0405,  0.1944]),\n",
       "  tensor([ 0.1880, -0.0779,  0.0095,  ..., -0.2327,  0.0555,  0.1004]),\n",
       "  tensor([-0.1543, -0.0710, -0.0074,  ..., -0.3112,  0.1929,  0.2814]),\n",
       "  tensor([-0.0962,  0.1108,  0.0397,  ..., -0.3673,  0.2482,  0.3060]),\n",
       "  tensor([-0.0625, -0.0893,  0.2732,  ..., -0.0307,  0.2543,  0.2941]),\n",
       "  tensor([ 0.0078,  0.1407,  0.0976,  ..., -0.2780,  0.1973,  0.1497]),\n",
       "  tensor([ 0.0815, -0.1446, -0.0173,  ..., -0.0917,  0.5983,  0.4421]),\n",
       "  tensor([ 0.0780,  0.0242, -0.1176,  ..., -0.0415,  0.4021,  0.2845]),\n",
       "  tensor([ 0.2075,  0.0717, -0.1780,  ..., -0.0199,  0.6268,  0.5039]),\n",
       "  tensor([-0.3099,  0.2377, -0.1483,  ..., -0.0960, -0.0313,  0.2398]),\n",
       "  tensor([-0.3500,  0.1890,  0.2885,  ...,  0.4151, -0.8305, -0.0760]),\n",
       "  tensor([-0.1220,  0.4306,  0.1901,  ...,  0.0576, -0.6929,  0.1871]),\n",
       "  tensor([ 0.0326,  0.2623, -0.0977,  ..., -0.0177,  0.4672,  0.2021]),\n",
       "  tensor([-0.1466, -0.0897, -0.1672,  ...,  0.1073, -0.3977,  0.0161]),\n",
       "  tensor([ 0.1428,  0.2636, -0.2376,  ..., -0.2699,  0.2802,  0.0803]),\n",
       "  tensor([-0.0989, -0.0898, -0.1432,  ...,  0.0793, -0.3949,  0.0734]),\n",
       "  tensor([-0.0951,  0.0549, -0.3539,  ..., -0.0328, -0.0507,  0.5542]),\n",
       "  tensor([-0.2214, -0.0833,  0.1277,  ...,  0.0299, -0.2849,  0.0788]),\n",
       "  tensor([-0.2115, -0.0595, -0.1268,  ...,  0.0688, -0.4126,  0.0113]),\n",
       "  tensor([ 0.0239,  0.1116, -0.0894,  ...,  0.1512, -0.4686,  0.0285]),\n",
       "  tensor([ 0.1266,  0.1835, -0.0390,  ..., -0.2917,  0.1209,  0.0390]),\n",
       "  tensor([-0.0924,  0.0703,  0.1432,  ..., -0.2738,  0.2810,  0.0089]),\n",
       "  tensor([-0.3135,  0.2600, -0.0521,  ...,  0.1089, -0.0341,  0.1041]),\n",
       "  tensor([-0.0443,  0.1183,  0.0104,  ..., -0.2871,  0.2930,  0.1278]),\n",
       "  tensor([-0.1580, -0.1577, -0.1185,  ...,  0.0630, -0.3489,  0.0743]),\n",
       "  tensor([ 0.0752,  0.1688, -0.0575,  ..., -0.2003,  0.0219,  0.4529]),\n",
       "  tensor([-0.0188, -0.1120, -0.0201,  ..., -0.0459,  0.1458,  0.3088]),\n",
       "  tensor([ 0.1454,  0.1124, -0.0282,  ..., -0.3059,  0.3761,  0.1745]),\n",
       "  tensor([ 0.2876, -0.0338,  0.2955,  ..., -0.1097,  0.3245,  0.0537]),\n",
       "  tensor([-0.1578, -0.1007, -0.0104,  ..., -0.1834,  0.0464,  0.2070]),\n",
       "  tensor([ 0.0259, -0.0315, -0.0248,  ..., -0.3998,  0.5105,  0.2612]),\n",
       "  tensor([ 0.1678, -0.0591, -0.0088,  ..., -0.0223,  0.2557,  0.1005]),\n",
       "  tensor([ 0.1108,  0.1092, -0.0220,  ..., -0.2105,  0.1084,  0.4835]),\n",
       "  tensor([-0.0177,  0.2118, -0.0642,  ..., -0.3879,  0.0367,  0.2013]),\n",
       "  tensor([-0.0544,  0.1348, -0.0325,  ..., -0.0891,  0.3925,  0.1600]),\n",
       "  tensor([-0.1495, -0.1137, -0.1097,  ...,  0.0569, -0.3724,  0.0703]),\n",
       "  tensor([ 0.0041, -0.1242,  0.0027,  ..., -0.1873,  0.3569,  0.3924]),\n",
       "  tensor([ 0.0383,  0.1945, -0.0391,  ..., -0.1773,  0.1819,  0.4957]),\n",
       "  tensor([ 0.2597, -0.0186,  0.0225,  ..., -0.1994,  0.4545,  0.4256]),\n",
       "  tensor([ 0.0560, -0.0111,  0.0179,  ..., -0.2701,  0.3263, -0.0356]),\n",
       "  tensor([-0.0753,  0.0060, -0.1664,  ...,  0.0271, -0.0252,  0.1642]),\n",
       "  tensor([-0.0265,  0.2025, -0.1240,  ..., -0.1413,  0.2004,  0.4006]),\n",
       "  tensor([ 0.1054,  0.1539,  0.1030,  ..., -0.4424,  0.1310,  0.1839]),\n",
       "  tensor([-0.1647,  0.2974,  0.0660,  ..., -0.0239, -0.6356,  0.2602]),\n",
       "  tensor([-0.1483,  0.0878, -0.1748,  ..., -0.0804,  0.2167,  0.3650]),\n",
       "  tensor([-0.0867,  0.0044, -0.1612,  ..., -0.0812,  0.0044,  0.3410]),\n",
       "  tensor([-0.1319,  0.0874,  0.0482,  ..., -0.1797,  0.2951,  0.0247]),\n",
       "  tensor([-0.6183,  0.2506,  0.3129,  ...,  0.1519, -0.2314,  0.1342]),\n",
       "  tensor([ 0.0801,  0.3535,  0.3540,  ..., -0.3862,  0.2119,  0.1945]),\n",
       "  tensor([ 0.3047,  0.1007,  0.0653,  ..., -0.3479,  0.7128,  0.3930]),\n",
       "  tensor([-0.0251,  0.1574,  0.1181,  ..., -0.2165,  0.2270,  0.0528]),\n",
       "  tensor([0.0027, 0.1556, 0.1014,  ..., 0.0577, 0.4869, 0.2585]),\n",
       "  tensor([ 0.0022, -0.2769,  0.0235,  ..., -0.1428,  0.5529,  0.3155]),\n",
       "  tensor([ 0.0758,  0.1395, -0.0354,  ..., -0.2753,  0.2626,  0.1637]),\n",
       "  tensor([-0.1889, -0.1610, -0.2533,  ..., -0.3292,  0.0437,  0.2012]),\n",
       "  tensor([ 0.3874, -0.1661, -0.0499,  ..., -0.2638,  0.3948,  0.4642]),\n",
       "  tensor([-0.1045,  0.1940,  0.1323,  ..., -0.2316,  0.1289,  0.1307]),\n",
       "  tensor([ 0.0898,  0.1958,  0.1909,  ..., -0.0085,  0.4034,  0.3359]),\n",
       "  tensor([-0.0451,  0.2263,  0.1744,  ..., -0.3948,  0.2962,  0.1547]),\n",
       "  tensor([-0.0681,  0.0038,  0.0965,  ..., -0.2935,  0.3587,  0.2597]),\n",
       "  tensor([-0.3036,  0.2367, -0.0288,  ...,  0.0032,  0.2022,  0.2150]),\n",
       "  tensor([-0.1959, -0.0583,  0.0313,  ...,  0.0384, -0.3962, -0.0501]),\n",
       "  tensor([-0.1595, -0.2044, -0.0600,  ...,  0.0490,  0.6512,  0.1669]),\n",
       "  tensor([-0.0827, -0.0711, -0.0048,  ..., -0.1990,  0.4840,  0.0565]),\n",
       "  tensor([ 0.0614, -0.1218, -0.1021,  ..., -0.3011,  0.1896,  0.1386]),\n",
       "  tensor([-0.0525,  0.1023,  0.0808,  ..., -0.2237, -0.0003,  0.1580]),\n",
       "  tensor([-0.1250,  0.0345, -0.0070,  ..., -0.2189,  0.2497,  0.2974]),\n",
       "  tensor([-0.0367, -0.0420, -0.0766,  ..., -0.1644,  0.1100,  0.1918]),\n",
       "  tensor([-0.2548,  0.2597,  0.0686,  ..., -0.2783, -0.1857,  0.2804]),\n",
       "  tensor([ 0.0381,  0.1248, -0.3153,  ..., -0.1488,  0.2674,  0.2988]),\n",
       "  tensor([ 0.1573,  0.1277,  0.0445,  ..., -0.3444,  0.2275,  0.0869]),\n",
       "  tensor([ 0.0229,  0.0046, -0.0764,  ..., -0.1346,  0.3919,  0.2160]),\n",
       "  tensor([-0.4226,  0.3555,  0.3398,  ...,  0.4715, -0.8905, -0.0283]),\n",
       "  tensor([ 0.3727, -0.0078, -0.1040,  ..., -0.3562,  0.6261,  0.4706]),\n",
       "  tensor([ 0.0258,  0.2036,  0.0607,  ..., -0.2108,  0.2714,  0.4577]),\n",
       "  tensor([ 0.0996, -0.0255, -0.0394,  ..., -0.1542,  0.3780,  0.4188]),\n",
       "  tensor([-0.3098,  0.3993,  0.4015,  ...,  0.3203, -0.5645,  0.0586]),\n",
       "  tensor([ 0.0551,  0.1229,  0.0651,  ..., -0.2056,  0.3179,  0.2311]),\n",
       "  tensor([ 0.1838, -0.1717,  0.0809,  ..., -0.0613,  0.4741,  0.3404]),\n",
       "  tensor([-0.2530,  0.2301, -0.0325,  ...,  0.0706,  0.1900,  0.1860]),\n",
       "  tensor([ 0.0992, -0.0413,  0.0315,  ..., -0.3810,  0.0259,  0.2192]),\n",
       "  tensor([ 0.0888,  0.0852, -0.0036,  ..., -0.1977,  0.2009,  0.1680]),\n",
       "  tensor([-0.2589, -0.1780, -0.0606,  ..., -0.1910, -0.0032,  0.1913]),\n",
       "  tensor([ 0.1380,  0.0691,  0.1794,  ..., -0.2631,  0.1327, -0.1133]),\n",
       "  tensor([-0.0659, -0.0564, -0.0415,  ..., -0.1245,  0.1133,  0.2389]),\n",
       "  tensor([ 0.0071,  0.1237,  0.0015,  ..., -0.2481,  0.0776,  0.4570])],\n",
       " 2: [tensor([ 0.0893,  0.1992,  0.0626,  ..., -0.2585,  0.4247,  0.1640]),\n",
       "  tensor([-0.0903,  0.3468, -0.1615,  ..., -0.1517,  0.3867,  0.1363]),\n",
       "  tensor([ 0.0118,  0.1902,  0.0849,  ..., -0.1517,  0.3988,  0.3183]),\n",
       "  tensor([-0.2922,  0.2537, -0.2508,  ..., -0.0359,  0.0359,  0.3117]),\n",
       "  tensor([ 0.0319,  0.0704, -0.0191,  ..., -0.1645,  0.2382,  0.1478]),\n",
       "  tensor([-0.1181,  0.0787,  0.0257,  ..., -0.2265,  0.1612,  0.2132]),\n",
       "  tensor([-0.0278,  0.2675,  0.1231,  ..., -0.3584,  0.2571,  0.3826]),\n",
       "  tensor([-0.1309,  0.0780,  0.1610,  ..., -0.0462,  0.1993,  0.3861]),\n",
       "  tensor([ 0.0441,  0.2747,  0.1456,  ..., -0.2038,  0.2139,  0.2132]),\n",
       "  tensor([ 0.0328,  0.1702,  0.0945,  ..., -0.1728,  0.4781,  0.4498]),\n",
       "  tensor([ 0.0349,  0.2750, -0.0505,  ..., -0.1097,  0.3297,  0.2080]),\n",
       "  tensor([ 0.0350,  0.4324, -0.1758,  ..., -0.0613,  0.5327,  0.3681]),\n",
       "  tensor([-0.3015,  0.4221, -0.2092,  ..., -0.0080, -0.1018,  0.2191]),\n",
       "  tensor([-0.1542,  0.3299,  0.2378,  ..., -0.0482, -0.5366,  0.1429]),\n",
       "  tensor([-0.2493,  0.3395,  0.2081,  ..., -0.1543, -0.5278,  0.3953]),\n",
       "  tensor([-0.0422,  0.5891, -0.1208,  ..., -0.0701,  0.2944,  0.1715]),\n",
       "  tensor([-0.0175,  0.1995, -0.0481,  ...,  0.0758, -0.2174,  0.1803]),\n",
       "  tensor([ 0.0162,  0.3556, -0.0380,  ..., -0.1968,  0.0883,  0.1437]),\n",
       "  tensor([-1.0212e-04,  2.0951e-01, -3.1247e-02,  ...,  6.6184e-02,\n",
       "          -2.0223e-01,  2.2403e-01]),\n",
       "  tensor([-0.0928,  0.2198, -0.3147,  ...,  0.1242, -0.0064,  0.3882]),\n",
       "  tensor([-0.2126,  0.1565,  0.2281,  ..., -0.0685, -0.1363,  0.2619]),\n",
       "  tensor([-0.1026,  0.1596, -0.0180,  ...,  0.0796, -0.2334,  0.1892]),\n",
       "  tensor([ 0.0784,  0.2599,  0.0219,  ...,  0.0037, -0.3530,  0.2078]),\n",
       "  tensor([-0.0164,  0.3283, -0.0122,  ..., -0.1310,  0.1309,  0.1469]),\n",
       "  tensor([ 0.0150,  0.2335,  0.1000,  ..., -0.3117,  0.2844,  0.0308]),\n",
       "  tensor([-0.3332,  0.3436, -0.2074,  ...,  0.1369,  0.0869,  0.3328]),\n",
       "  tensor([-0.0397,  0.2492, -0.0261,  ..., -0.3170,  0.1719,  0.1383]),\n",
       "  tensor([-0.0117,  0.1764, -0.0393,  ...,  0.0626, -0.1933,  0.1869]),\n",
       "  tensor([ 0.0941,  0.2691,  0.0249,  ..., -0.0273,  0.1659,  0.3021]),\n",
       "  tensor([0.0023, 0.0554, 0.0130,  ..., 0.0103, 0.1358, 0.2642]),\n",
       "  tensor([ 0.1356,  0.3277,  0.0032,  ..., -0.3222,  0.3990,  0.1967]),\n",
       "  tensor([ 0.1882,  0.0986,  0.2643,  ..., -0.2328,  0.2992,  0.2407]),\n",
       "  tensor([-0.0715,  0.0954, -0.0160,  ..., -0.2193,  0.0571,  0.1443]),\n",
       "  tensor([ 0.1094,  0.2675, -0.0075,  ..., -0.3926,  0.3388,  0.2856]),\n",
       "  tensor([ 0.1048,  0.1133,  0.1272,  ..., -0.1062,  0.2032,  0.1707]),\n",
       "  tensor([ 0.1581,  0.2504,  0.0568,  ..., -0.0335,  0.1961,  0.3349]),\n",
       "  tensor([-0.0106,  0.5168, -0.0900,  ..., -0.1840,  0.0405,  0.3914]),\n",
       "  tensor([-0.0219,  0.3027, -0.0924,  ..., -0.1288,  0.3673,  0.2384]),\n",
       "  tensor([-0.0378,  0.2163,  0.0181,  ...,  0.0350, -0.1939,  0.2087]),\n",
       "  tensor([ 0.0860,  0.1721,  0.1344,  ..., -0.1833,  0.2871,  0.2589]),\n",
       "  tensor([-0.0273,  0.3722, -0.1787,  ..., -0.0642,  0.0770,  0.4197]),\n",
       "  tensor([ 0.2253,  0.1915,  0.0253,  ..., -0.2375,  0.4334,  0.2711]),\n",
       "  tensor([-0.0269,  0.2758,  0.0645,  ..., -0.2357,  0.2587,  0.1527]),\n",
       "  tensor([-0.1756,  0.2728, -0.2147,  ...,  0.0480, -0.0203,  0.1779]),\n",
       "  tensor([-0.0819,  0.3186, -0.1166,  ..., -0.1095,  0.1312,  0.4247]),\n",
       "  tensor([ 0.1174,  0.2943, -0.0090,  ..., -0.4168,  0.1132,  0.2568]),\n",
       "  tensor([-0.2704,  0.3308,  0.0137,  ..., -0.3549, -0.4928,  0.4080]),\n",
       "  tensor([-0.1531,  0.2609, -0.1429,  ..., -0.0625,  0.2223,  0.4317]),\n",
       "  tensor([-0.0696,  0.2080, -0.1094,  ..., -0.0555, -0.1627,  0.4282]),\n",
       "  tensor([-0.1028,  0.2964,  0.0252,  ..., -0.1980,  0.2915,  0.0718]),\n",
       "  tensor([-0.6729,  0.3163,  0.2654,  ...,  0.0260, -0.0308,  0.3171]),\n",
       "  tensor([-0.0247,  0.3935,  0.2210,  ..., -0.4127,  0.1610,  0.1579]),\n",
       "  tensor([ 0.2900,  0.3861,  0.1618,  ..., -0.3025,  0.5399,  0.3035]),\n",
       "  tensor([ 0.0246,  0.2760,  0.1486,  ..., -0.2232,  0.2207,  0.1362]),\n",
       "  tensor([0.0233, 0.2945, 0.1636,  ..., 0.0230, 0.3522, 0.2168]),\n",
       "  tensor([-0.0013,  0.0246, -0.0086,  ..., -0.1483,  0.4235,  0.2738]),\n",
       "  tensor([ 0.0202,  0.3474, -0.0567,  ..., -0.1518,  0.1310,  0.1360]),\n",
       "  tensor([-0.0637,  0.0242, -0.1318,  ..., -0.2739,  0.0859,  0.3162]),\n",
       "  tensor([ 0.2929,  0.1746, -0.0122,  ..., -0.2171,  0.4119,  0.4361]),\n",
       "  tensor([-0.0879,  0.3356,  0.1451,  ..., -0.1941,  0.0912,  0.2882]),\n",
       "  tensor([-0.1015,  0.3765,  0.1718,  ...,  0.0435,  0.2789,  0.3742]),\n",
       "  tensor([ 0.0969,  0.4499,  0.1496,  ..., -0.3828,  0.3587,  0.1680]),\n",
       "  tensor([-0.0582,  0.2028,  0.1615,  ..., -0.1954,  0.2679,  0.2774]),\n",
       "  tensor([-0.3042,  0.3703, -0.0553,  ..., -0.0193,  0.0610,  0.3120]),\n",
       "  tensor([-0.1836,  0.1376,  0.1008,  ...,  0.0251, -0.1897,  0.0917]),\n",
       "  tensor([-2.1424e-01,  1.1490e-01, -8.1776e-02,  ...,  1.5726e-04,\n",
       "           5.6860e-01,  1.9318e-01]),\n",
       "  tensor([-0.1387,  0.3247,  0.0437,  ..., -0.2274,  0.4073,  0.1536]),\n",
       "  tensor([ 0.0249,  0.0891, -0.0938,  ..., -0.2111,  0.2181,  0.2823]),\n",
       "  tensor([ 0.0502,  0.2519,  0.0961,  ..., -0.1965,  0.0072,  0.1710]),\n",
       "  tensor([-0.1274,  0.2360,  0.0875,  ..., -0.1885,  0.3446,  0.3192]),\n",
       "  tensor([-0.0034,  0.1326, -0.0353,  ..., -0.1210,  0.1365,  0.1534]),\n",
       "  tensor([-0.2307,  0.4478, -0.0911,  ..., -0.2143, -0.1492,  0.3859]),\n",
       "  tensor([-0.1254,  0.2647, -0.1591,  ..., -0.1536,  0.1873,  0.3790]),\n",
       "  tensor([ 0.1339,  0.2549, -0.0401,  ..., -0.3462,  0.1171,  0.1642]),\n",
       "  tensor([ 0.1502,  0.1773, -0.0714,  ..., -0.1139,  0.2988,  0.2024]),\n",
       "  tensor([-0.3927,  0.5547,  0.3742,  ...,  0.0245, -0.7160,  0.1833]),\n",
       "  tensor([ 0.2465,  0.2668, -0.0220,  ..., -0.1406,  0.4073,  0.3621]),\n",
       "  tensor([ 0.0176,  0.3447, -0.0260,  ..., -0.1700,  0.2929,  0.3480]),\n",
       "  tensor([ 0.1436,  0.1446, -0.0121,  ..., -0.2434,  0.2415,  0.3279]),\n",
       "  tensor([-0.3886,  0.4671,  0.4110,  ..., -0.0344, -0.2700,  0.3134]),\n",
       "  tensor([ 0.0782,  0.3073, -0.0079,  ..., -0.1504,  0.2269,  0.1163]),\n",
       "  tensor([ 0.1771,  0.1276, -0.0070,  ..., -0.0962,  0.2467,  0.2968]),\n",
       "  tensor([-0.2351,  0.3456, -0.0761,  ...,  0.1280,  0.1309,  0.2729]),\n",
       "  tensor([ 0.0869,  0.1207,  0.0578,  ..., -0.3303,  0.1254,  0.2966]),\n",
       "  tensor([ 0.2681,  0.2710,  0.0755,  ..., -0.2756,  0.2138,  0.2263]),\n",
       "  tensor([-0.2090,  0.0509,  0.0865,  ..., -0.1656,  0.0707,  0.1684]),\n",
       "  tensor([ 0.1308,  0.2506,  0.1274,  ..., -0.2682,  0.1485, -0.0063]),\n",
       "  tensor([-0.0284,  0.1215,  0.0129,  ..., -0.1487,  0.1401,  0.1777]),\n",
       "  tensor([ 0.0794,  0.2775,  0.0644,  ..., -0.0367,  0.1369,  0.3238])],\n",
       " 3: [tensor([ 0.0238,  0.0039,  0.1940,  ..., -0.3283,  0.3322,  0.0917]),\n",
       "  tensor([-0.0376,  0.2099,  0.1093,  ..., -0.2213,  0.2688,  0.1835]),\n",
       "  tensor([-0.0097,  0.0616,  0.2436,  ..., -0.1785,  0.3270,  0.2191]),\n",
       "  tensor([-0.1014,  0.2233, -0.0789,  ..., -0.0286,  0.0787,  0.2500]),\n",
       "  tensor([-0.1045,  0.0103,  0.1286,  ..., -0.3178,  0.1363,  0.1417]),\n",
       "  tensor([-0.1727,  0.0730,  0.1654,  ..., -0.2144,  0.1946,  0.1688]),\n",
       "  tensor([-0.0609,  0.1795,  0.2195,  ..., -0.3233,  0.2267,  0.2905]),\n",
       "  tensor([-0.2376,  0.0789,  0.3846,  ..., -0.1894,  0.1751,  0.2493]),\n",
       "  tensor([ 0.0052,  0.2148,  0.2092,  ..., -0.2314,  0.0999,  0.2235]),\n",
       "  tensor([ 0.0431,  0.1384,  0.1502,  ..., -0.2294,  0.4327,  0.3035]),\n",
       "  tensor([ 0.0270,  0.1469,  0.0992,  ..., -0.1031,  0.3126,  0.2053]),\n",
       "  tensor([ 0.1076,  0.3213, -0.0123,  ..., -0.0935,  0.4067,  0.5627]),\n",
       "  tensor([-0.1473,  0.2551,  0.0170,  ..., -0.0646, -0.0084,  0.1752]),\n",
       "  tensor([-0.2069,  0.3466,  0.2595,  ..., -0.0821, -0.5416, -0.0107]),\n",
       "  tensor([-0.2234,  0.3877,  0.3114,  ..., -0.0935, -0.5483,  0.2214]),\n",
       "  tensor([-0.1363,  0.3266,  0.1073,  ..., -0.1203,  0.2583,  0.1908]),\n",
       "  tensor([-0.0269,  0.3311,  0.0386,  ...,  0.0749, -0.0881, -0.0411]),\n",
       "  tensor([-0.0739,  0.1781,  0.2090,  ..., -0.2970,  0.0944,  0.1267]),\n",
       "  tensor([ 0.0093,  0.3386,  0.0852,  ...,  0.0717, -0.0780, -0.0051]),\n",
       "  tensor([-0.0521,  0.1725, -0.1860,  ...,  0.0421,  0.0572,  0.2591]),\n",
       "  tensor([-0.1371,  0.2792,  0.2840,  ..., -0.1024,  0.0044,  0.1769]),\n",
       "  tensor([-0.0952,  0.3077,  0.0952,  ...,  0.0735, -0.0969, -0.0330]),\n",
       "  tensor([-0.0758,  0.3164,  0.1386,  ..., -0.0183, -0.2522,  0.0395]),\n",
       "  tensor([-0.0798,  0.1505,  0.1965,  ..., -0.2133,  0.1252,  0.2072]),\n",
       "  tensor([ 0.0317,  0.1314,  0.2356,  ..., -0.3741,  0.2770, -0.0211]),\n",
       "  tensor([-0.1364,  0.3064,  0.0562,  ...,  0.0016,  0.1420,  0.1589]),\n",
       "  tensor([-0.0503,  0.1409,  0.0435,  ..., -0.3252,  0.1327,  0.1343]),\n",
       "  tensor([-0.0139,  0.2986,  0.0847,  ...,  0.0512, -0.0637, -0.0192]),\n",
       "  tensor([ 0.0518,  0.2329,  0.0570,  ..., -0.0832,  0.1504,  0.2170]),\n",
       "  tensor([-0.0802, -0.0121,  0.0670,  ..., -0.0984,  0.1322,  0.1122]),\n",
       "  tensor([ 0.1283,  0.1431,  0.1543,  ..., -0.4194,  0.3211,  0.1370]),\n",
       "  tensor([ 0.0028, -0.0093,  0.2813,  ..., -0.2604,  0.1584,  0.2525]),\n",
       "  tensor([-0.1723,  0.1289,  0.0859,  ..., -0.2097,  0.1265,  0.0680]),\n",
       "  tensor([-0.0098,  0.2156,  0.1722,  ..., -0.4429,  0.3085,  0.2039]),\n",
       "  tensor([ 0.0330,  0.0501,  0.1822,  ..., -0.1413,  0.2031,  0.1506]),\n",
       "  tensor([ 0.0695,  0.2009,  0.0883,  ..., -0.0790,  0.1825,  0.2486]),\n",
       "  tensor([ 0.0452,  0.3737,  0.0799,  ..., -0.2066, -0.0657,  0.3834]),\n",
       "  tensor([-0.0539,  0.1665,  0.1740,  ..., -0.3654,  0.2279,  0.1687]),\n",
       "  tensor([-0.0354,  0.3700,  0.1138,  ...,  0.0492, -0.0693, -0.0195]),\n",
       "  tensor([ 0.0317,  0.1515,  0.2030,  ..., -0.1544,  0.2941,  0.1091]),\n",
       "  tensor([ 0.0079,  0.2272,  0.1160,  ..., -0.1894,  0.0247,  0.3967]),\n",
       "  tensor([ 0.0578,  0.0992,  0.1696,  ..., -0.2903,  0.3858,  0.2287]),\n",
       "  tensor([ 0.0108,  0.1876,  0.2122,  ..., -0.3005,  0.1447,  0.1927]),\n",
       "  tensor([-0.1348,  0.1469,  0.1225,  ..., -0.0504, -0.0500,  0.0703]),\n",
       "  tensor([-0.0954,  0.1790,  0.0276,  ..., -0.1484,  0.1451,  0.2647]),\n",
       "  tensor([ 0.0267,  0.1874,  0.0996,  ..., -0.4889,  0.0413,  0.2000]),\n",
       "  tensor([-0.2361,  0.3378,  0.1355,  ..., -0.2239, -0.5123,  0.2394]),\n",
       "  tensor([-0.1203,  0.2168, -0.0023,  ..., -0.1214,  0.2446,  0.3299]),\n",
       "  tensor([-0.0101,  0.1413,  0.1273,  ..., -0.0731, -0.2145,  0.2544]),\n",
       "  tensor([-0.1514,  0.2120,  0.2022,  ..., -0.2381,  0.2695,  0.0660]),\n",
       "  tensor([-0.5830,  0.2787,  0.2873,  ..., -0.0221, -0.1346,  0.1246]),\n",
       "  tensor([-0.0748,  0.1954,  0.3071,  ..., -0.5690,  0.1510,  0.0609]),\n",
       "  tensor([ 0.1028,  0.1440,  0.2688,  ..., -0.3195,  0.5277,  0.3018]),\n",
       "  tensor([ 0.0020,  0.1502,  0.2580,  ..., -0.2938,  0.1887,  0.0598]),\n",
       "  tensor([ 0.0938,  0.1766,  0.2308,  ..., -0.0533,  0.3092,  0.2703]),\n",
       "  tensor([-0.1051, -0.0701,  0.1104,  ..., -0.1378,  0.3199,  0.2504]),\n",
       "  tensor([ 0.0053,  0.2455, -0.0157,  ..., -0.2715,  0.1005,  0.0456]),\n",
       "  tensor([-0.0155,  0.0156,  0.0267,  ..., -0.2499,  0.1493,  0.2315]),\n",
       "  tensor([ 0.0308,  0.1279,  0.1227,  ..., -0.2658,  0.3822,  0.3280]),\n",
       "  tensor([-0.1272,  0.2046,  0.2933,  ..., -0.3058,  0.0040,  0.2729]),\n",
       "  tensor([-0.0546,  0.2578,  0.2536,  ..., -0.0684,  0.0707,  0.3077]),\n",
       "  tensor([-0.0941,  0.3457,  0.2794,  ..., -0.3966,  0.2700,  0.1204]),\n",
       "  tensor([-0.1030,  0.1204,  0.2421,  ..., -0.1859,  0.1862,  0.2866]),\n",
       "  tensor([-0.1906,  0.3520,  0.0912,  ..., -0.0536,  0.0939,  0.1040]),\n",
       "  tensor([-0.1531,  0.2323,  0.1686,  ..., -0.0220, -0.0626, -0.0144]),\n",
       "  tensor([-0.1509, -0.0253,  0.0767,  ..., -0.0577,  0.3814,  0.1817]),\n",
       "  tensor([-0.1185,  0.1336,  0.3039,  ..., -0.2486,  0.3338,  0.1440]),\n",
       "  tensor([-0.0164,  0.0866,  0.0038,  ..., -0.2419,  0.1704,  0.2530]),\n",
       "  tensor([-0.0012,  0.2222,  0.2062,  ..., -0.2316,  0.0514,  0.0488]),\n",
       "  tensor([-0.1545,  0.1000,  0.1805,  ..., -0.1913,  0.3069,  0.2746]),\n",
       "  tensor([-0.1562,  0.1011,  0.1863,  ..., -0.1910,  0.1417,  0.0413]),\n",
       "  tensor([-0.1358,  0.3338,  0.0303,  ..., -0.1724, -0.0707,  0.3078]),\n",
       "  tensor([-0.1363,  0.1280,  0.0053,  ..., -0.2253,  0.2014,  0.2641]),\n",
       "  tensor([-0.0091,  0.1270,  0.0867,  ..., -0.4557,  0.0333,  0.0930]),\n",
       "  tensor([ 0.1000,  0.1580,  0.0547,  ..., -0.1839,  0.2883,  0.1517]),\n",
       "  tensor([-0.4219,  0.6220,  0.4276,  ...,  0.0152, -0.5115,  0.1060]),\n",
       "  tensor([ 0.1332,  0.1322,  0.0887,  ..., -0.2600,  0.3885,  0.3392]),\n",
       "  tensor([-0.0588,  0.2354,  0.1493,  ..., -0.1681,  0.2755,  0.2617]),\n",
       "  tensor([ 0.0361,  0.0524,  0.2308,  ..., -0.2409,  0.2076,  0.2538]),\n",
       "  tensor([-0.3932,  0.4654,  0.4115,  ..., -0.0783, -0.2966,  0.0648]),\n",
       "  tensor([ 0.0069,  0.2279,  0.1626,  ..., -0.2096,  0.1859,  0.0753]),\n",
       "  tensor([ 0.0256,  0.0160,  0.1568,  ..., -0.1444,  0.2936,  0.2886]),\n",
       "  tensor([-0.0776,  0.1856,  0.1278,  ...,  0.0329,  0.0718,  0.1006]),\n",
       "  tensor([ 0.0639,  0.0993,  0.1704,  ..., -0.3273,  0.0857,  0.2099]),\n",
       "  tensor([ 0.1097,  0.2630,  0.1670,  ..., -0.3050,  0.2724,  0.1126]),\n",
       "  tensor([-0.1881,  0.0640,  0.1700,  ..., -0.1336,  0.1075,  0.0854]),\n",
       "  tensor([ 0.0233,  0.1825,  0.2354,  ..., -0.3406,  0.1649, -0.0598]),\n",
       "  tensor([-0.1056,  0.1414,  0.1580,  ..., -0.1455,  0.1340,  0.0918]),\n",
       "  tensor([-0.0185,  0.1934,  0.0830,  ..., -0.0747,  0.1317,  0.2206])],\n",
       " 4: [tensor([-0.0255, -0.0111,  0.1072,  ..., -0.1763,  0.1701,  0.0578]),\n",
       "  tensor([-0.0125,  0.2478,  0.0296,  ..., -0.1357,  0.1675,  0.1224]),\n",
       "  tensor([ 0.0241,  0.0955,  0.2051,  ..., -0.0929,  0.1809,  0.1893]),\n",
       "  tensor([ 0.0126,  0.2907, -0.0142,  ...,  0.0013,  0.0150,  0.1627]),\n",
       "  tensor([-0.0710,  0.0621,  0.0981,  ..., -0.0888,  0.0446,  0.0764]),\n",
       "  tensor([-0.0810,  0.0946,  0.1013,  ..., -0.1009,  0.1074,  0.1023]),\n",
       "  tensor([-0.0312,  0.1842,  0.1318,  ..., -0.2531,  0.1088,  0.2540]),\n",
       "  tensor([-0.1629,  0.1301,  0.3219,  ..., -0.0478,  0.1459,  0.0878]),\n",
       "  tensor([ 0.0452,  0.2499,  0.1399,  ..., -0.1369,  0.0185,  0.0884]),\n",
       "  tensor([ 0.0713,  0.1876,  0.1508,  ..., -0.0950,  0.3638,  0.1320]),\n",
       "  tensor([ 0.0238,  0.2181,  0.0904,  ..., -0.0224,  0.2275,  0.0821]),\n",
       "  tensor([ 0.0678,  0.3526,  0.0896,  ..., -0.0484,  0.2491,  0.4039]),\n",
       "  tensor([-0.0615,  0.3073, -0.0017,  ..., -0.0048,  0.0370,  0.1220]),\n",
       "  tensor([-0.1121,  0.4454,  0.2382,  ...,  0.0096, -0.3578,  0.0203]),\n",
       "  tensor([-0.1729,  0.4654,  0.2904,  ..., -0.1447, -0.3691,  0.1753]),\n",
       "  tensor([-0.1170,  0.2346,  0.0470,  ...,  0.1166,  0.1809,  0.0965]),\n",
       "  tensor([ 0.1819,  0.3178, -0.0051,  ...,  0.0995, -0.2469, -0.0849]),\n",
       "  tensor([-0.0201,  0.1910,  0.1354,  ..., -0.1374, -0.0220,  0.0779]),\n",
       "  tensor([ 0.2182,  0.3438,  0.0312,  ...,  0.1127, -0.2157, -0.0362]),\n",
       "  tensor([ 0.0235,  0.2288, -0.1379,  ...,  0.1390,  0.0017,  0.2290]),\n",
       "  tensor([-0.0018,  0.3670,  0.2378,  ..., -0.0599, -0.0107,  0.0690]),\n",
       "  tensor([ 0.1420,  0.3054,  0.0126,  ...,  0.1029, -0.2292, -0.0415]),\n",
       "  tensor([ 0.0174,  0.3870,  0.1428,  ..., -0.0300, -0.2104,  0.0118]),\n",
       "  tensor([ 0.0133,  0.1839,  0.1457,  ..., -0.1100,  0.1971,  0.1121]),\n",
       "  tensor([ 0.0723,  0.1290,  0.1186,  ..., -0.1980,  0.1173, -0.0490]),\n",
       "  tensor([-0.0570,  0.3858, -0.0344,  ...,  0.0479,  0.1361,  0.1085]),\n",
       "  tensor([-0.0255,  0.1432, -0.0330,  ..., -0.1657,  0.0672,  0.0489]),\n",
       "  tensor([ 0.1790,  0.3149,  0.0212,  ...,  0.0856, -0.1888, -0.0774]),\n",
       "  tensor([ 0.0584,  0.2511,  0.0505,  ..., -0.0184,  0.0224,  0.1387]),\n",
       "  tensor([ 0.0105,  0.1364,  0.0170,  ..., -0.0050,  0.0973,  0.0204]),\n",
       "  tensor([ 0.1115,  0.1405,  0.0799,  ..., -0.2610,  0.1352,  0.0867]),\n",
       "  tensor([-0.0481,  0.0684,  0.1857,  ..., -0.1950,  0.0832,  0.1545]),\n",
       "  tensor([-0.1050,  0.1533,  0.0347,  ..., -0.0706,  0.0345,  0.0029]),\n",
       "  tensor([-0.0178,  0.2078,  0.1055,  ..., -0.2473,  0.2236,  0.0700]),\n",
       "  tensor([ 0.0928,  0.1266,  0.1739,  ..., -0.0459,  0.0623,  0.0834]),\n",
       "  tensor([0.0421, 0.2116, 0.0680,  ..., 0.0239, 0.0418, 0.1462]),\n",
       "  tensor([ 0.1025,  0.3931,  0.0177,  ..., -0.1465, -0.0269,  0.2514]),\n",
       "  tensor([-0.0273,  0.1144,  0.1246,  ..., -0.1964,  0.0775,  0.0638]),\n",
       "  tensor([ 0.1860,  0.3494,  0.0534,  ...,  0.0944, -0.2115, -0.0649]),\n",
       "  tensor([ 0.0306,  0.1469,  0.2026,  ..., -0.0381,  0.1699,  0.0359]),\n",
       "  tensor([ 0.0573,  0.2356,  0.1000,  ..., -0.1204,  0.0865,  0.2657]),\n",
       "  tensor([ 0.0302,  0.0938,  0.1239,  ..., -0.1399,  0.2596,  0.1020]),\n",
       "  tensor([ 0.0877,  0.2175,  0.1750,  ..., -0.1930,  0.1192,  0.0797]),\n",
       "  tensor([0.0021, 0.2078, 0.1431,  ..., 0.0266, 0.0344, 0.0091]),\n",
       "  tensor([-0.0053,  0.2207, -0.0031,  ..., -0.0446,  0.0760,  0.1727]),\n",
       "  tensor([ 0.1128,  0.1657,  0.1113,  ..., -0.3576, -0.0206,  0.0662]),\n",
       "  tensor([-0.1820,  0.4421,  0.1998,  ..., -0.1681, -0.3795,  0.1334]),\n",
       "  tensor([-0.0685,  0.2283, -0.0114,  ..., -0.0575,  0.2187,  0.2437]),\n",
       "  tensor([ 0.0623,  0.2072,  0.0873,  ..., -0.0663, -0.0511,  0.1353]),\n",
       "  tensor([-0.1166,  0.2174,  0.1263,  ..., -0.0820,  0.1022,  0.0847]),\n",
       "  tensor([-0.4084,  0.3481,  0.2267,  ..., -0.0357, -0.0874,  0.0667]),\n",
       "  tensor([-0.0860,  0.1272,  0.1813,  ..., -0.4202,  0.0306, -0.0028]),\n",
       "  tensor([ 0.0195,  0.1258,  0.1489,  ..., -0.1305,  0.3952,  0.1813]),\n",
       "  tensor([ 0.0481,  0.1109,  0.1349,  ..., -0.1527,  0.0950,  0.0271]),\n",
       "  tensor([-0.0434,  0.2357,  0.2523,  ..., -0.0223,  0.2320,  0.0891]),\n",
       "  tensor([-0.1089,  0.0250,  0.1195,  ..., -0.0234,  0.2346,  0.1089]),\n",
       "  tensor([-0.0040,  0.2319, -0.0179,  ..., -0.1288,  0.0835, -0.0023]),\n",
       "  tensor([ 0.0452,  0.0740,  0.0177,  ..., -0.1897,  0.0842,  0.1768]),\n",
       "  tensor([ 0.0325,  0.1408, -0.0038,  ..., -0.0595,  0.2642,  0.1881]),\n",
       "  tensor([-0.0670,  0.2492,  0.1707,  ..., -0.2151, -0.0129,  0.1720]),\n",
       "  tensor([ 0.0571,  0.3035,  0.2099,  ..., -0.0471, -0.0653,  0.2005]),\n",
       "  tensor([-0.0379,  0.2659,  0.1681,  ..., -0.1532,  0.2125, -0.0072]),\n",
       "  tensor([-0.0482,  0.2136,  0.2308,  ..., -0.0908,  0.0946,  0.1698]),\n",
       "  tensor([-0.0375,  0.3507,  0.0361,  ..., -0.0895,  0.0935, -0.0471]),\n",
       "  tensor([ 0.0474,  0.2821,  0.1582,  ..., -0.0147, -0.1981,  0.0465]),\n",
       "  tensor([-0.1296,  0.0219, -0.0148,  ...,  0.0325,  0.2354,  0.1109]),\n",
       "  tensor([-0.0901,  0.1637,  0.1983,  ..., -0.1306,  0.1705,  0.0910]),\n",
       "  tensor([ 0.0361,  0.1542, -0.0060,  ..., -0.1554,  0.0920,  0.1728]),\n",
       "  tensor([ 0.0429,  0.1881,  0.1305,  ..., -0.1597, -0.0417,  0.0177]),\n",
       "  tensor([-0.0815,  0.1055,  0.1596,  ..., -0.0895,  0.1869,  0.2008]),\n",
       "  tensor([-0.1120,  0.1319,  0.1584,  ..., -0.0725,  0.0613, -0.0026]),\n",
       "  tensor([-0.0197,  0.4279,  0.0013,  ..., -0.1242,  0.0120,  0.1453]),\n",
       "  tensor([-0.1068,  0.1054,  0.0031,  ..., -0.1080,  0.1941,  0.1814]),\n",
       "  tensor([ 0.0793,  0.1311, -0.0352,  ..., -0.2763, -0.0488, -0.0011]),\n",
       "  tensor([ 0.0960,  0.1973,  0.0676,  ..., -0.1104,  0.1655,  0.0515]),\n",
       "  tensor([-0.1860,  0.5393,  0.4098,  ..., -0.0145, -0.3964, -0.0096]),\n",
       "  tensor([ 0.1115,  0.1551,  0.0689,  ..., -0.1066,  0.2912,  0.1791]),\n",
       "  tensor([-0.0075,  0.2429,  0.0832,  ..., -0.0171,  0.1985,  0.1319]),\n",
       "  tensor([ 0.0621,  0.1025,  0.1548,  ..., -0.1302,  0.1195,  0.1788]),\n",
       "  tensor([-0.2072,  0.4823,  0.3633,  ..., -0.0854, -0.2235,  0.0163]),\n",
       "  tensor([-0.0114,  0.2061,  0.0949,  ..., -0.0954,  0.1328, -0.0155]),\n",
       "  tensor([0.0554, 0.0355, 0.1103,  ..., 0.0292, 0.1863, 0.1798]),\n",
       "  tensor([-0.0011,  0.2614, -0.0013,  ...,  0.0990,  0.1164,  0.0705]),\n",
       "  tensor([ 0.1109,  0.1255,  0.1927,  ..., -0.2933, -0.0753,  0.1345]),\n",
       "  tensor([ 0.1499,  0.2046,  0.0771,  ..., -0.1924,  0.1969,  0.0016]),\n",
       "  tensor([-0.1117,  0.0922,  0.1221,  ..., -0.0695,  0.0220,  0.0669]),\n",
       "  tensor([ 0.0339,  0.2182,  0.1531,  ..., -0.2087,  0.1129, -0.0637]),\n",
       "  tensor([-0.0614,  0.1917,  0.1280,  ..., -0.0734,  0.0494,  0.0747]),\n",
       "  tensor([-0.0236,  0.2372,  0.0492,  ...,  0.0440,  0.0586,  0.1267])]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict[idx]['lama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "lama layer 0  mean : 0.4582816958427429 std : 0.3399696946144104 min : -0.07265432178974152 max : 0.9879365563392639\n",
      "lama layer 1  mean : 0.6002259850502014 std : 0.22470062971115112 min : 0.16175426542758942 max : 0.9912566542625427\n",
      "lama layer 2  mean : 0.6332541704177856 std : 0.23044390976428986 min : 0.1818157434463501 max : 0.9924421310424805\n",
      "lama layer 3  mean : 0.6474221348762512 std : 0.23154298961162567 min : 0.20032057166099548 max : 0.9934854507446289\n",
      "lama layer 4  mean : 0.671459972858429 std : 0.23618336021900177 min : 0.21222372353076935 max : 0.9942880868911743\n",
      "Label 1 :\n",
      "lama layer 0  mean : 0.6929686665534973 std : 0.17042873799800873 min : -0.06605807691812515 max : 0.993717610836029\n",
      "lama layer 1  mean : 0.7530477643013 std : 0.11401186883449554 min : 0.21396896243095398 max : 0.995693564414978\n",
      "lama layer 2  mean : 0.7917009592056274 std : 0.10813746601343155 min : 0.25296857953071594 max : 0.9969007968902588\n",
      "lama layer 3  mean : 0.8060942888259888 std : 0.10498916357755661 min : 0.27038484811782837 max : 0.9971517324447632\n",
      "lama layer 4  mean : 0.8291993141174316 std : 0.102793850004673 min : 0.2713756263256073 max : 0.9974713325500488\n",
      "Label 2 :\n",
      "lama layer 0  mean : 0.5505133271217346 std : 0.24836666882038116 min : -0.15644140541553497 max : 0.9736753702163696\n",
      "lama layer 1  mean : 0.6806016564369202 std : 0.16051383316516876 min : 0.25212931632995605 max : 0.9797840714454651\n",
      "lama layer 2  mean : 0.6948263645172119 std : 0.1594812273979187 min : 0.25279155373573303 max : 0.981630265712738\n",
      "lama layer 3  mean : 0.7156544923782349 std : 0.15798889100551605 min : 0.25969329476356506 max : 0.9831368327140808\n",
      "lama layer 4  mean : 0.7304850816726685 std : 0.15554969012737274 min : 0.25977277755737305 max : 0.9849550724029541\n",
      "Label 3 :\n",
      "lama layer 0  mean : 0.7179704308509827 std : 0.19458472728729248 min : 0.0014278395101428032 max : 0.9949774742126465\n",
      "lama layer 1  mean : 0.7901353240013123 std : 0.13556459546089172 min : 0.25126761198043823 max : 0.9962399005889893\n",
      "lama layer 2  mean : 0.8003661632537842 std : 0.1383315473794937 min : 0.23861324787139893 max : 0.9969807863235474\n",
      "lama layer 3  mean : 0.8194299340248108 std : 0.13237108290195465 min : 0.2476656436920166 max : 0.9975205659866333\n",
      "lama layer 4  mean : 0.829199492931366 std : 0.13075003027915955 min : 0.2415580451488495 max : 0.9978510737419128\n",
      "Label 4 :\n",
      "lama layer 0  mean : 0.6762889623641968 std : 0.2153283804655075 min : -0.09006667137145996 max : 0.9963897466659546\n",
      "lama layer 1  mean : 0.7681695222854614 std : 0.141077920794487 min : 0.21215155720710754 max : 0.9978001117706299\n",
      "lama layer 2  mean : 0.7751220464706421 std : 0.14535847306251526 min : 0.16456879675388336 max : 0.9981617331504822\n",
      "lama layer 3  mean : 0.7937005758285522 std : 0.1420792043209076 min : 0.16688749194145203 max : 0.9984734654426575\n",
      "lama layer 4  mean : 0.8047174215316772 std : 0.14040720462799072 min : 0.15925678610801697 max : 0.9986340999603271\n",
      "Label 5 :\n",
      "lama layer 0  mean : 0.5044660568237305 std : 0.3030520975589752 min : -0.12587641179561615 max : 0.9761119484901428\n",
      "lama layer 1  mean : 0.6689443588256836 std : 0.16692520678043365 min : 0.22001372277736664 max : 0.9886308908462524\n",
      "lama layer 2  mean : 0.7078704237937927 std : 0.16184532642364502 min : 0.25574010610580444 max : 0.9908947348594666\n",
      "lama layer 3  mean : 0.7278034687042236 std : 0.15767312049865723 min : 0.27756932377815247 max : 0.99222731590271\n",
      "lama layer 4  mean : 0.7536899447441101 std : 0.15788565576076508 min : 0.3037589192390442 max : 0.9925923347473145\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    # lama layer 별\n",
    "    print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "    print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "    print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "    print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "    print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    l_rep_0 = l_rep[0]\n",
    "    l_rep_1 = l_rep[1]\n",
    "    l_rep_2 = l_rep[2]\n",
    "    l_rep_3 = l_rep[3]\n",
    "    l_rep_4 = l_rep[4]\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "    reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "    reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "    reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "    reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['bert'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(6):\n",
    "    for i in range(5): # lama layer별\n",
    "        lama_cosine = []\n",
    "        for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "            for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "                lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "        all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "lama layer 0  mean : 0.46358874440193176 std : 0.3384522795677185 min : -0.05568278580904007 max : 0.986672580242157\n",
      "lama layer 1  mean : 0.5983796119689941 std : 0.22708453238010406 min : 0.19598516821861267 max : 0.9901775121688843\n",
      "lama layer 2  mean : 0.6308391094207764 std : 0.23406483232975006 min : 0.21928724646568298 max : 0.9917163252830505\n",
      "lama layer 3  mean : 0.6414186954498291 std : 0.23933541774749756 min : 0.21745069324970245 max : 0.9927277565002441\n",
      "lama layer 4  mean : 0.661725640296936 std : 0.2469824254512787 min : 0.24644210934638977 max : 0.9938218593597412\n",
      "Label 1 :\n",
      "lama layer 0  mean : 0.5795263648033142 std : 0.2719472348690033 min : -0.06521344184875488 max : 0.9896935224533081\n",
      "lama layer 1  mean : 0.6882783770561218 std : 0.17085333168506622 min : 0.20340560376644135 max : 0.9917173981666565\n",
      "lama layer 2  mean : 0.7320974469184875 std : 0.1680101603269577 min : 0.250400185585022 max : 0.9932606816291809\n",
      "lama layer 3  mean : 0.7470207214355469 std : 0.16776609420776367 min : 0.26357364654541016 max : 0.9936550855636597\n",
      "lama layer 4  mean : 0.7686368227005005 std : 0.16737999022006989 min : 0.2793988585472107 max : 0.9944127798080444\n",
      "Label 2 :\n",
      "lama layer 0  mean : 0.5808221697807312 std : 0.23453593254089355 min : -0.037712618708610535 max : 0.9792981147766113\n",
      "lama layer 1  mean : 0.6913323402404785 std : 0.15981607139110565 min : 0.21190103888511658 max : 0.9841523766517639\n",
      "lama layer 2  mean : 0.7054492831230164 std : 0.16350045800209045 min : 0.19937755167484283 max : 0.9853357076644897\n",
      "lama layer 3  mean : 0.7240762710571289 std : 0.16460008919239044 min : 0.18545934557914734 max : 0.9865483045578003\n",
      "lama layer 4  mean : 0.7398493885993958 std : 0.1627771258354187 min : 0.18652421236038208 max : 0.9880910515785217\n",
      "Label 3 :\n",
      "lama layer 0  mean : 0.7499058842658997 std : 0.22102399170398712 min : 0.01649346947669983 max : 0.9997084140777588\n",
      "lama layer 1  mean : 0.8139287233352661 std : 0.15308499336242676 min : 0.2767539918422699 max : 0.999790370464325\n",
      "lama layer 2  mean : 0.8233410716056824 std : 0.15624529123306274 min : 0.25060558319091797 max : 0.9998208284378052\n",
      "lama layer 3  mean : 0.838866114616394 std : 0.15129756927490234 min : 0.2741076946258545 max : 0.9998519420623779\n",
      "lama layer 4  mean : 0.84786057472229 std : 0.1499967873096466 min : 0.2693746089935303 max : 0.9998724460601807\n",
      "Label 4 :\n",
      "lama layer 0  mean : 0.6948240399360657 std : 0.20648565888404846 min : 0.10191647708415985 max : 0.9191955924034119\n",
      "lama layer 1  mean : 0.775602400302887 std : 0.13477718830108643 min : 0.31470850110054016 max : 0.9382208585739136\n",
      "lama layer 2  mean : 0.7828717231750488 std : 0.13905513286590576 min : 0.28687623143196106 max : 0.9430835247039795\n",
      "lama layer 3  mean : 0.8006311655044556 std : 0.13378062844276428 min : 0.3130203187465668 max : 0.9505762457847595\n",
      "lama layer 4  mean : 0.8102929592132568 std : 0.13194657862186432 min : 0.33207929134368896 max : 0.9536572694778442\n",
      "Label 5 :\n",
      "lama layer 0  mean : 0.38317859172821045 std : 0.34462592005729675 min : -0.07409030199050903 max : 0.9609623551368713\n",
      "lama layer 1  mean : 0.6066029667854309 std : 0.1965636909008026 min : 0.2201845794916153 max : 0.9823691248893738\n",
      "lama layer 2  mean : 0.6433244347572327 std : 0.19100628793239594 min : 0.24429374933242798 max : 0.9857977032661438\n",
      "lama layer 3  mean : 0.6647171378135681 std : 0.18627053499221802 min : 0.2688550353050232 max : 0.9881300330162048\n",
      "lama layer 4  mean : 0.6919374465942383 std : 0.1848042756319046 min : 0.2945047914981842 max : 0.989264965057373\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    # lama layer 별\n",
    "    print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "    print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "    print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "    print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "    print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 라벨들과의 코사인 유사도 비교\n",
    "## TrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.5413272976875305, std : 0.2847304940223694, min : -0.037676941603422165, max : 0.9191119074821472\n",
      "Bert mean : -0.1666981279850006, std : 0.010526364669203758, min : -0.208282470703125, max : -0.12093579024076462\n",
      "Roberta mean : -0.03852460905909538, std : 0.03408439829945564, min : -0.16758498549461365, max : 0.1900651603937149\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.20587420463562012, std : 0.26441818475723267, min : -0.09556057304143906, max : 0.9188210368156433\n",
      "Bert mean : -0.2327321171760559, std : 0.0077667078003287315, min : -0.2909048795700073, max : -0.19608166813850403\n",
      "Roberta mean : -0.018670227378606796, std : 0.0872267335653305, min : -0.09566497057676315, max : 0.8220071792602539\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.18722771108150482, std : 0.27942630648612976, min : -0.04657138139009476, max : 0.9261351823806763\n",
      "Bert mean : -0.18996699154376984, std : 0.011919560842216015, min : -0.24039670825004578, max : -0.13743461668491364\n",
      "Roberta mean : -0.03633812814950943, std : 0.05369073525071144, min : -0.21458128094673157, max : 0.3927105665206909\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.1796570122241974, std : 0.2711106538772583, min : -0.07739551365375519, max : 0.9308699369430542\n",
      "Bert mean : -0.17969273030757904, std : 0.01971498876810074, min : -0.27686750888824463, max : -0.11130425333976746\n",
      "Roberta mean : 0.06374305486679077, std : 0.02806142531335354, min : -0.046891748905181885, max : 0.2275833636522293\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.4958428144454956, std : 0.3020383417606354, min : -0.05067402124404907, max : 0.9398689270019531\n",
      "Bert mean : -0.13676847517490387, std : 0.016758203506469727, min : -0.20293112099170685, max : -0.044069770723581314\n",
      "Roberta mean : 0.27591192722320557, std : 0.10707959532737732, min : -0.028748491778969765, max : 0.9394990801811218\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.5413272976875305, std : 0.2847304940223694, min : -0.037676941603422165, max : 0.9191119074821472\n",
      "Bert mean : -0.1666981279850006, std : 0.010526364669203758, min : -0.208282470703125, max : -0.12093579024076462\n",
      "Roberta mean : -0.03852460905909538, std : 0.03408439829945564, min : -0.16758498549461365, max : 0.1900651603937149\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.10442135483026505, std : 0.180636465549469, min : -0.10434553772211075, max : 0.8673315644264221\n",
      "Bert mean : -0.0738585814833641, std : 0.016620852053165436, min : -0.15159833431243896, max : -0.03180885687470436\n",
      "Roberta mean : 0.006647137459367514, std : 0.02528870292007923, min : -0.044925838708877563, max : 0.18437284231185913\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.053525783121585846, std : 0.12993603944778442, min : -0.04536883160471916, max : 0.8219248652458191\n",
      "Bert mean : -0.12921379506587982, std : 0.018683701753616333, min : -0.16777624189853668, max : -0.02370145171880722\n",
      "Roberta mean : -0.018267137929797173, std : 0.034752532839775085, min : -0.1622258424758911, max : 0.15646211802959442\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.05295474827289581, std : 0.12914979457855225, min : -0.09127052873373032, max : 0.8425520658493042\n",
      "Bert mean : -0.3835200369358063, std : 0.011492789722979069, min : -0.4301632046699524, max : -0.33157965540885925\n",
      "Roberta mean : 0.0240237507969141, std : 0.021285761147737503, min : -0.06424067169427872, max : 0.15173859894275665\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.5811410546302795, std : 0.24965651333332062, min : -0.04677266255021095, max : 0.9421374797821045\n",
      "Bert mean : -0.09302949905395508, std : 0.04165785014629364, min : -0.16472922265529633, max : 0.19366060197353363\n",
      "Roberta mean : 0.08335035294294357, std : 0.04686318710446358, min : -0.09727919846773148, max : 0.3840734660625458\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.20587418973445892, std : 0.26441818475723267, min : -0.09556057304143906, max : 0.9188210368156433\n",
      "Bert mean : -0.2327321469783783, std : 0.0077667078003287315, min : -0.2909048795700073, max : -0.19608166813850403\n",
      "Roberta mean : -0.018670227378606796, std : 0.0872267335653305, min : -0.09566497057676315, max : 0.8220071792602539\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.10442135483026505, std : 0.180636465549469, min : -0.10434553772211075, max : 0.8673315644264221\n",
      "Bert mean : -0.0738585889339447, std : 0.016620852053165436, min : -0.15159833431243896, max : -0.03180885687470436\n",
      "Roberta mean : 0.006647137925028801, std : 0.02528870292007923, min : -0.044925838708877563, max : 0.18437284231185913\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.6231273412704468, std : 0.22253254055976868, min : 0.002282039960846305, max : 0.9342137575149536\n",
      "Bert mean : -0.11563295125961304, std : 0.018156882375478745, min : -0.15807034075260162, max : -0.02087702974677086\n",
      "Roberta mean : 0.01534575130790472, std : 0.06373961269855499, min : -0.07898331433534622, max : 0.46439796686172485\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.6126832365989685, std : 0.22441639006137848, min : -0.026303639635443687, max : 0.9402561187744141\n",
      "Bert mean : -0.12760435044765472, std : 0.020093146711587906, min : -0.18202950060367584, max : 0.008646608330309391\n",
      "Roberta mean : 0.009704973548650742, std : 0.02990102209150791, min : -0.06237594410777092, max : 0.18110965192317963\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : 0.1834707111120224, std : 0.23657996952533722, min : -0.09166352450847626, max : 0.898853063583374\n",
      "Bert mean : -0.19343872368335724, std : 0.013579152524471283, min : -0.22535483539104462, max : -0.12208045274019241\n",
      "Roberta mean : -0.09212414175271988, std : 0.0633523091673851, min : -0.16939470171928406, max : 0.5139960050582886\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.18722771108150482, std : 0.27942630648612976, min : -0.04657138139009476, max : 0.9261351823806763\n",
      "Bert mean : -0.18996699154376984, std : 0.011919560842216015, min : -0.24039670825004578, max : -0.13743461668491364\n",
      "Roberta mean : -0.03633812814950943, std : 0.05369073525071144, min : -0.21458128094673157, max : 0.3927105665206909\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.05352577939629555, std : 0.12993603944778442, min : -0.04536883160471916, max : 0.8219248652458191\n",
      "Bert mean : -0.12921379506587982, std : 0.018683701753616333, min : -0.16777624189853668, max : -0.02370145171880722\n",
      "Roberta mean : -0.018267139792442322, std : 0.034752532839775085, min : -0.1622258424758911, max : 0.15646211802959442\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.6231274008750916, std : 0.22253254055976868, min : 0.002282039960846305, max : 0.9342137575149536\n",
      "Bert mean : -0.11563295125961304, std : 0.018156882375478745, min : -0.15807034075260162, max : -0.02087702974677086\n",
      "Roberta mean : 0.01534575130790472, std : 0.06373961269855499, min : -0.07898331433534622, max : 0.46439796686172485\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.7027518153190613, std : 0.1933077573776245, min : 0.043230678886175156, max : 0.9537256956100464\n",
      "Bert mean : -0.08077384531497955, std : 0.012684598565101624, min : -0.1213790774345398, max : -0.014056956395506859\n",
      "Roberta mean : 0.07123278081417084, std : 0.11277011036872864, min : -0.11984659731388092, max : 0.8688826560974121\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.1467970758676529, std : 0.22858351469039917, min : -0.0463038831949234, max : 0.9194029569625854\n",
      "Bert mean : -0.199179545044899, std : 0.015762684866786003, min : -0.2278851866722107, max : -0.10059773176908493\n",
      "Roberta mean : 0.013862081803381443, std : 0.05543379858136177, min : -0.21620561182498932, max : 0.2467423975467682\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.1796570122241974, std : 0.2711106538772583, min : -0.07739551365375519, max : 0.9308699369430542\n",
      "Bert mean : -0.17969271540641785, std : 0.01971498876810074, min : -0.27686750888824463, max : -0.11130425333976746\n",
      "Roberta mean : 0.06374305486679077, std : 0.02806142531335354, min : -0.046891748905181885, max : 0.2275833636522293\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.05295474827289581, std : 0.12914979457855225, min : -0.09127052873373032, max : 0.8425520658493042\n",
      "Bert mean : -0.3835200071334839, std : 0.011492789722979069, min : -0.4301632046699524, max : -0.33157965540885925\n",
      "Roberta mean : 0.02402375265955925, std : 0.021285761147737503, min : -0.06424067169427872, max : 0.15173859894275665\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.6126832365989685, std : 0.22441639006137848, min : -0.026303639635443687, max : 0.9402561187744141\n",
      "Bert mean : -0.12760436534881592, std : 0.020093146711587906, min : -0.18202950060367584, max : 0.008646608330309391\n",
      "Roberta mean : 0.009704974479973316, std : 0.02990102209150791, min : -0.06237594410777092, max : 0.18110965192317963\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.7027518153190613, std : 0.1933077573776245, min : 0.043230678886175156, max : 0.9537256956100464\n",
      "Bert mean : -0.08077383786439896, std : 0.012684598565101624, min : -0.1213790774345398, max : -0.014056956395506859\n",
      "Roberta mean : 0.07123278081417084, std : 0.11277011036872864, min : -0.11984659731388092, max : 0.8688826560974121\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.15037469565868378, std : 0.22948357462882996, min : -0.08154848217964172, max : 0.9138151407241821\n",
      "Bert mean : -0.20215891301631927, std : 0.03873356431722641, min : -0.4562912583351135, max : -0.10840586572885513\n",
      "Roberta mean : -0.011392700485885143, std : 0.026473958045244217, min : -0.14931583404541016, max : 0.14334484934806824\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.495842844247818, std : 0.3020383417606354, min : -0.05067402124404907, max : 0.9398689270019531\n",
      "Bert mean : -0.13676847517490387, std : 0.016758203506469727, min : -0.20293112099170685, max : -0.044069770723581314\n",
      "Roberta mean : 0.27591195702552795, std : 0.10707959532737732, min : -0.028748491778969765, max : 0.9394990801811218\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.5811409950256348, std : 0.24965651333332062, min : -0.04677266255021095, max : 0.9421374797821045\n",
      "Bert mean : -0.09302949160337448, std : 0.04165785014629364, min : -0.16472922265529633, max : 0.19366060197353363\n",
      "Roberta mean : 0.08335036039352417, std : 0.04686318710446358, min : -0.09727919846773148, max : 0.3840734660625458\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : 0.1834707111120224, std : 0.23657996952533722, min : -0.09166352450847626, max : 0.898853063583374\n",
      "Bert mean : -0.19343872368335724, std : 0.013579152524471283, min : -0.22535483539104462, max : -0.12208045274019241\n",
      "Roberta mean : -0.09212414920330048, std : 0.0633523091673851, min : -0.16939470171928406, max : 0.5139960050582886\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.1467970758676529, std : 0.22858351469039917, min : -0.0463038831949234, max : 0.9194029569625854\n",
      "Bert mean : -0.19917957484722137, std : 0.015762684866786003, min : -0.2278851866722107, max : -0.10059773176908493\n",
      "Roberta mean : 0.013862081803381443, std : 0.05543379858136177, min : -0.21620561182498932, max : 0.2467423975467682\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.15037469565868378, std : 0.22948357462882996, min : -0.08154848217964172, max : 0.9138151407241821\n",
      "Bert mean : -0.20215892791748047, std : 0.03873356431722641, min : -0.4562912583351135, max : -0.10840586572885513\n",
      "Roberta mean : -0.011392699554562569, std : 0.026473958045244217, min : -0.14931583404541016, max : 0.14334484934806824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lama layer 별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    l_rep_0 = l_rep[0]\n",
    "    l_rep_1 = l_rep[1]\n",
    "    l_rep_2 = l_rep[2]\n",
    "    l_rep_3 = l_rep[3]\n",
    "    l_rep_4 = l_rep[4]\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "    reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "    reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "    reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "    reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "for main_label in range(6):\n",
    "    for next_label in range(6):\n",
    "        for layer_num in range(5):\n",
    "            lama_cosine = []\n",
    "            if main_label != next_label:\n",
    "                for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "                    for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "                        lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "                all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.5225430727005005, std : 0.2988731265068054, min : -0.07923213392496109, max : 0.9178649187088013\n",
      "Label 0 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.6363747119903564, std : 0.19578397274017334, min : 0.16304539144039154, max : 0.9276597499847412\n",
      "Label 0 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.6780866980552673, std : 0.19929613173007965, min : 0.18186984956264496, max : 0.9340721368789673\n",
      "Label 0 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.6917426586151123, std : 0.20077604055404663, min : 0.1970122754573822, max : 0.9393244385719299\n",
      "Label 0 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.7177930474281311, std : 0.2048761397600174, min : 0.21089527010917664, max : 0.9482427835464478\n",
      "Label 0 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.16440613567829132, std : 0.2787114083766937, min : -0.19989082217216492, max : 0.9130476117134094\n",
      "Label 0 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.4311179220676422, std : 0.1844501495361328, min : 0.17007513344287872, max : 0.9373192191123962\n",
      "Label 0 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.46085208654403687, std : 0.18433278799057007, min : 0.2097053974866867, max : 0.9459279179573059\n",
      "Label 0 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.4815312623977661, std : 0.1872490793466568, min : 0.221388041973114, max : 0.9536564946174622\n",
      "Label 0 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.5023761987686157, std : 0.18799708783626556, min : 0.22199730575084686, max : 0.9581859111785889\n",
      "Label 0 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.150477334856987, std : 0.290691614151001, min : -0.10740198940038681, max : 0.9219547510147095\n",
      "Label 0 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.39534991979599, std : 0.20856298506259918, min : 0.14367014169692993, max : 0.941573441028595\n",
      "Label 0 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.41513484716415405, std : 0.21012501418590546, min : 0.17024677991867065, max : 0.9454233646392822\n",
      "Label 0 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.43359124660491943, std : 0.21357014775276184, min : 0.1717073619365692, max : 0.9540903568267822\n",
      "Label 0 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.45226308703422546, std : 0.2135603129863739, min : 0.17556054890155792, max : 0.9583377838134766\n",
      "Label 0 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.13553300499916077, std : 0.28613483905792236, min : -0.1939380019903183, max : 0.9266573786735535\n",
      "Label 0 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.4168540835380554, std : 0.19344574213027954, min : 0.14771002531051636, max : 0.9460487961769104\n",
      "Label 0 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.4417460858821869, std : 0.19479167461395264, min : 0.1580411195755005, max : 0.9508605003356934\n",
      "Label 0 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.46453094482421875, std : 0.1973867565393448, min : 0.17003680765628815, max : 0.9584901332855225\n",
      "Label 0 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.48534730076789856, std : 0.19680845737457275, min : 0.15541820228099823, max : 0.9631954431533813\n",
      "Label 0 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.4709872901439667, std : 0.3205362558364868, min : -0.11561296880245209, max : 0.9378495216369629\n",
      "Label 0 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.6238322257995605, std : 0.1968618482351303, min : 0.16347636282444, max : 0.9486703276634216\n",
      "Label 0 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.6600569486618042, std : 0.1994917094707489, min : 0.17745453119277954, max : 0.9599853157997131\n",
      "Label 0 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.6770017147064209, std : 0.1993837207555771, min : 0.19438870251178741, max : 0.9611045122146606\n",
      "Label 0 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.7026804685592651, std : 0.20339761674404144, min : 0.2098582684993744, max : 0.9672924876213074\n",
      "Label 1 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.5225430727005005, std : 0.2988731265068054, min : -0.07923213392496109, max : 0.9178649187088013\n",
      "Label 1 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.6363747715950012, std : 0.19578397274017334, min : 0.16304539144039154, max : 0.9276597499847412\n",
      "Label 1 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.6780867576599121, std : 0.19929613173007965, min : 0.18186984956264496, max : 0.9340721368789673\n",
      "Label 1 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.6917425990104675, std : 0.20077604055404663, min : 0.1970122754573822, max : 0.9393244385719299\n",
      "Label 1 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.7177930474281311, std : 0.2048761397600174, min : 0.21089527010917664, max : 0.9482427835464478\n",
      "Label 1 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.05660247057676315, std : 0.1906750351190567, min : -0.20683380961418152, max : 0.8626918792724609\n",
      "Label 1 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.36151841282844543, std : 0.13424983620643616, min : 0.17437432706356049, max : 0.8874602317810059\n",
      "Label 1 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.4048369526863098, std : 0.14026981592178345, min : 0.21072161197662354, max : 0.9060805439949036\n",
      "Label 1 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.42373353242874146, std : 0.14511938393115997, min : 0.22795359790325165, max : 0.9133302569389343\n",
      "Label 1 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.4466513991355896, std : 0.14913535118103027, min : 0.22320178151130676, max : 0.9285348057746887\n",
      "Label 1 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.010706021450459957, std : 0.13415999710559845, min : -0.12075591087341309, max : 0.8142098188400269\n",
      "Label 1 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.2954194247722626, std : 0.11140655726194382, min : 0.15163394808769226, max : 0.8541951775550842\n",
      "Label 1 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.32559123635292053, std : 0.11812641471624374, min : 0.17536333203315735, max : 0.869929850101471\n",
      "Label 1 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.3399331569671631, std : 0.12245132029056549, min : 0.1705373078584671, max : 0.8906064033508301\n",
      "Label 1 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.3612827658653259, std : 0.12744739651679993, min : 0.18079736828804016, max : 0.9012503027915955\n",
      "Label 1 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.0012451540678739548, std : 0.1366245597600937, min : -0.21075432002544403, max : 0.8344056606292725\n",
      "Label 1 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.32787826657295227, std : 0.11366527527570724, min : 0.14870570600032806, max : 0.8786238431930542\n",
      "Label 1 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.3661702573299408, std : 0.12454875558614731, min : 0.15855860710144043, max : 0.8891308903694153\n",
      "Label 1 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.38741636276245117, std : 0.13127385079860687, min : 0.16836155951023102, max : 0.9039337635040283\n",
      "Label 1 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.41094207763671875, std : 0.13528984785079956, min : 0.18814074993133545, max : 0.9140376448631287\n",
      "Label 1 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.5602932572364807, std : 0.2684095799922943, min : -0.12387723475694656, max : 0.94101482629776\n",
      "Label 1 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.6780356764793396, std : 0.15230295062065125, min : 0.20905564725399017, max : 0.9448580145835876\n",
      "Label 1 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.7245907187461853, std : 0.1448879987001419, min : 0.24215638637542725, max : 0.9537810683250427\n",
      "Label 1 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.7424787282943726, std : 0.14066247642040253, min : 0.25267401337623596, max : 0.9556398391723633\n",
      "Label 1 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.7698893547058105, std : 0.14035795629024506, min : 0.26963183283805847, max : 0.9627378582954407\n",
      "Label 2 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.16440615057945251, std : 0.2787114083766937, min : -0.19989082217216492, max : 0.9130476117134094\n",
      "Label 2 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.4311179220676422, std : 0.1844501495361328, min : 0.17007513344287872, max : 0.9373192191123962\n",
      "Label 2 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.46085211634635925, std : 0.18433278799057007, min : 0.2097053974866867, max : 0.9459279179573059\n",
      "Label 2 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.4815312623977661, std : 0.1872490793466568, min : 0.221388041973114, max : 0.9536564946174622\n",
      "Label 2 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.5023761987686157, std : 0.18799708783626556, min : 0.22199730575084686, max : 0.9581859111785889\n",
      "Label 2 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.05660247057676315, std : 0.1906750351190567, min : -0.20683380961418152, max : 0.8626918792724609\n",
      "Label 2 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.36151841282844543, std : 0.13424983620643616, min : 0.17437432706356049, max : 0.8874602317810059\n",
      "Label 2 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.4048369228839874, std : 0.14026981592178345, min : 0.21072161197662354, max : 0.9060805439949036\n",
      "Label 2 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.42373353242874146, std : 0.14511938393115997, min : 0.22795359790325165, max : 0.9133302569389343\n",
      "Label 2 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.4466513395309448, std : 0.14913535118103027, min : 0.22320178151130676, max : 0.9285348057746887\n",
      "Label 2 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.6045173406600952, std : 0.23413264751434326, min : -0.11250270903110504, max : 0.9302945733070374\n",
      "Label 2 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.7092448472976685, std : 0.16001862287521362, min : 0.21973834931850433, max : 0.9532447457313538\n",
      "Label 2 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.7203956842422485, std : 0.16227002441883087, min : 0.2139260172843933, max : 0.956619918346405\n",
      "Label 2 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.7413339614868164, std : 0.15987293422222137, min : 0.2095843255519867, max : 0.9614754915237427\n",
      "Label 2 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.7544237375259399, std : 0.1580730825662613, min : 0.20931503176689148, max : 0.9640595316886902\n",
      "Label 2 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.5916039347648621, std : 0.2385210394859314, min : -0.1577303260564804, max : 0.9365274906158447\n",
      "Label 2 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.7083392143249512, std : 0.1557122766971588, min : 0.20331893861293793, max : 0.9530884027481079\n",
      "Label 2 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.7188128232955933, std : 0.15710411965847015, min : 0.18692757189273834, max : 0.9585409760475159\n",
      "Label 2 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.7396768927574158, std : 0.15477502346038818, min : 0.17867961525917053, max : 0.9634160995483398\n",
      "Label 2 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.7534337639808655, std : 0.15242931246757507, min : 0.16357745230197906, max : 0.9678512215614319\n",
      "Label 2 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.1366596519947052, std : 0.24914062023162842, min : -0.20052200555801392, max : 0.889251708984375\n",
      "Label 2 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.4306892156600952, std : 0.17116327583789825, min : 0.17512209713459015, max : 0.9302978515625\n",
      "Label 2 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.4586051404476166, std : 0.17067304253578186, min : 0.2133617252111435, max : 0.9344795346260071\n",
      "Label 2 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.47766032814979553, std : 0.1743686944246292, min : 0.21778757870197296, max : 0.9424742460250854\n",
      "Label 2 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.4971820116043091, std : 0.17574955523014069, min : 0.22306613624095917, max : 0.9446908831596375\n",
      "Label 3 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.1504773199558258, std : 0.290691614151001, min : -0.10740198940038681, max : 0.9219547510147095\n",
      "Label 3 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.39534991979599, std : 0.20856298506259918, min : 0.14367014169692993, max : 0.941573441028595\n",
      "Label 3 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.41513484716415405, std : 0.21012501418590546, min : 0.17024677991867065, max : 0.9454233646392822\n",
      "Label 3 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.43359121680259705, std : 0.21357014775276184, min : 0.1717073619365692, max : 0.9540903568267822\n",
      "Label 3 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.45226311683654785, std : 0.2135603129863739, min : 0.17556054890155792, max : 0.9583377838134766\n",
      "Label 3 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.010706021450459957, std : 0.13415999710559845, min : -0.12075591087341309, max : 0.8142098188400269\n",
      "Label 3 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.2954194247722626, std : 0.11140655726194382, min : 0.15163394808769226, max : 0.8541951775550842\n",
      "Label 3 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.32559123635292053, std : 0.11812641471624374, min : 0.17536333203315735, max : 0.869929850101471\n",
      "Label 3 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.3399331569671631, std : 0.12245132029056549, min : 0.1705373078584671, max : 0.8906064033508301\n",
      "Label 3 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.3612827956676483, std : 0.12744739651679993, min : 0.18079736828804016, max : 0.9012503027915955\n",
      "Label 3 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.6045173406600952, std : 0.23413264751434326, min : -0.11250270903110504, max : 0.9302945733070374\n",
      "Label 3 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.7092447876930237, std : 0.16001862287521362, min : 0.21973834931850433, max : 0.9532447457313538\n",
      "Label 3 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.7203956842422485, std : 0.16227002441883087, min : 0.2139260172843933, max : 0.956619918346405\n",
      "Label 3 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.7413340210914612, std : 0.15987293422222137, min : 0.2095843255519867, max : 0.9614754915237427\n",
      "Label 3 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.7544237375259399, std : 0.1580730825662613, min : 0.20931503176689148, max : 0.9640595316886902\n",
      "Label 3 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.6876903176307678, std : 0.2041340172290802, min : -0.06317465752363205, max : 0.9512727856636047\n",
      "Label 3 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.7697776556015015, std : 0.13848964869976044, min : 0.2200014442205429, max : 0.9672597646713257\n",
      "Label 3 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.7785600423812866, std : 0.14274781942367554, min : 0.17897345125675201, max : 0.971794068813324\n",
      "Label 3 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.7974633574485779, std : 0.13924895226955414, min : 0.1619008630514145, max : 0.9763551950454712\n",
      "Label 3 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.8079875111579895, std : 0.13807979226112366, min : 0.1506926715373993, max : 0.9787193536758423\n",
      "Label 3 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.10510541498661041, std : 0.23788860440254211, min : -0.12073713541030884, max : 0.9191293716430664\n",
      "Label 3 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.3782581090927124, std : 0.17510299384593964, min : 0.14864911139011383, max : 0.9384489059448242\n",
      "Label 3 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.3938644230365753, std : 0.1748606413602829, min : 0.17206372320652008, max : 0.9431729912757874\n",
      "Label 3 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.41034257411956787, std : 0.17851391434669495, min : 0.1711861789226532, max : 0.9506810903549194\n",
      "Label 3 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.4276755750179291, std : 0.18002194166183472, min : 0.18167726695537567, max : 0.9540650248527527\n",
      "Label 4 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.13553299009799957, std : 0.28613483905792236, min : -0.1939380019903183, max : 0.9266573786735535\n",
      "Label 4 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.41685405373573303, std : 0.19344574213027954, min : 0.14771002531051636, max : 0.9460487961769104\n",
      "Label 4 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.4417461156845093, std : 0.19479167461395264, min : 0.1580411195755005, max : 0.9508605003356934\n",
      "Label 4 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.46453097462654114, std : 0.1973867565393448, min : 0.17003680765628815, max : 0.9584901332855225\n",
      "Label 4 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.48534727096557617, std : 0.19680845737457275, min : 0.15541820228099823, max : 0.9631954431533813\n",
      "Label 4 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.001245154533535242, std : 0.1366245597600937, min : -0.21075432002544403, max : 0.8344056606292725\n",
      "Label 4 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.3278782367706299, std : 0.11366527527570724, min : 0.14870570600032806, max : 0.8786238431930542\n",
      "Label 4 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.3661702871322632, std : 0.12454875558614731, min : 0.15855860710144043, max : 0.8891308903694153\n",
      "Label 4 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.387416273355484, std : 0.13127385079860687, min : 0.16836155951023102, max : 0.9039337635040283\n",
      "Label 4 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.41094207763671875, std : 0.13528984785079956, min : 0.18814074993133545, max : 0.9140376448631287\n",
      "Label 4 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.5916039347648621, std : 0.2385210394859314, min : -0.1577303260564804, max : 0.9365274906158447\n",
      "Label 4 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.7083392143249512, std : 0.1557122766971588, min : 0.20331893861293793, max : 0.9530884027481079\n",
      "Label 4 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.718812882900238, std : 0.15710411965847015, min : 0.18692757189273834, max : 0.9585409760475159\n",
      "Label 4 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.7396768927574158, std : 0.15477502346038818, min : 0.17867961525917053, max : 0.9634160995483398\n",
      "Label 4 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.7534337639808655, std : 0.15242931246757507, min : 0.16357745230197906, max : 0.9678512215614319\n",
      "Label 4 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.6876903176307678, std : 0.2041340172290802, min : -0.06317465752363205, max : 0.9512727856636047\n",
      "Label 4 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.7697776556015015, std : 0.13848964869976044, min : 0.2200014442205429, max : 0.9672597646713257\n",
      "Label 4 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.7785600423812866, std : 0.14274781942367554, min : 0.17897345125675201, max : 0.971794068813324\n",
      "Label 4 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.7974633574485779, std : 0.13924895226955414, min : 0.1619008630514145, max : 0.9763551950454712\n",
      "Label 4 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.8079875111579895, std : 0.13807979226112366, min : 0.1506926715373993, max : 0.9787193536758423\n",
      "Label 4 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.10018706321716309, std : 0.24169422686100006, min : -0.20187917351722717, max : 0.9058313965797424\n",
      "Label 4 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.4132970869541168, std : 0.17341767251491547, min : 0.15139366686344147, max : 0.9404956698417664\n",
      "Label 4 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.43475615978240967, std : 0.17407368123531342, min : 0.15651212632656097, max : 0.9429734349250793\n",
      "Label 4 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.4562898874282837, std : 0.17734599113464355, min : 0.17437919974327087, max : 0.9492942094802856\n",
      "Label 4 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.4756259620189667, std : 0.1778101623058319, min : 0.18873079121112823, max : 0.9506818652153015\n",
      "Label 5 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.47098731994628906, std : 0.3205362558364868, min : -0.11561296880245209, max : 0.9378495216369629\n",
      "Label 5 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.6238322257995605, std : 0.1968618482351303, min : 0.16347636282444, max : 0.9486703276634216\n",
      "Label 5 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.6600569486618042, std : 0.1994917094707489, min : 0.17745453119277954, max : 0.9599853157997131\n",
      "Label 5 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.6770017147064209, std : 0.1993837207555771, min : 0.19438870251178741, max : 0.9611045122146606\n",
      "Label 5 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.7026804685592651, std : 0.20339761674404144, min : 0.2098582684993744, max : 0.9672924876213074\n",
      "Label 5 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.5602932572364807, std : 0.2684095799922943, min : -0.12387723475694656, max : 0.94101482629776\n",
      "Label 5 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.6780356168746948, std : 0.15230295062065125, min : 0.20905564725399017, max : 0.9448580145835876\n",
      "Label 5 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.7245907783508301, std : 0.1448879987001419, min : 0.24215638637542725, max : 0.9537810683250427\n",
      "Label 5 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.7424786686897278, std : 0.14066247642040253, min : 0.25267401337623596, max : 0.9556398391723633\n",
      "Label 5 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.7698894739151001, std : 0.14035795629024506, min : 0.26963183283805847, max : 0.9627378582954407\n",
      "Label 5 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.1366596668958664, std : 0.24914062023162842, min : -0.20052200555801392, max : 0.889251708984375\n",
      "Label 5 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.4306892454624176, std : 0.17116327583789825, min : 0.17512209713459015, max : 0.9302978515625\n",
      "Label 5 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.45860517024993896, std : 0.17067304253578186, min : 0.2133617252111435, max : 0.9344795346260071\n",
      "Label 5 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.47766032814979553, std : 0.1743686944246292, min : 0.21778757870197296, max : 0.9424742460250854\n",
      "Label 5 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.49718207120895386, std : 0.17574955523014069, min : 0.22306613624095917, max : 0.9446908831596375\n",
      "Label 5 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.10510540008544922, std : 0.23788860440254211, min : -0.12073713541030884, max : 0.9191293716430664\n",
      "Label 5 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.37825807929039, std : 0.17510299384593964, min : 0.14864911139011383, max : 0.9384489059448242\n",
      "Label 5 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.39386439323425293, std : 0.1748606413602829, min : 0.17206372320652008, max : 0.9431729912757874\n",
      "Label 5 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.41034257411956787, std : 0.17851391434669495, min : 0.1711861789226532, max : 0.9506810903549194\n",
      "Label 5 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.4276755750179291, std : 0.18002194166183472, min : 0.18167726695537567, max : 0.9540650248527527\n",
      "Label 5 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.10018706321716309, std : 0.24169422686100006, min : -0.20187917351722717, max : 0.9058313965797424\n",
      "Label 5 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.4132970869541168, std : 0.17341767251491547, min : 0.15139366686344147, max : 0.9404956698417664\n",
      "Label 5 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.43475615978240967, std : 0.17407368123531342, min : 0.15651212632656097, max : 0.9429734349250793\n",
      "Label 5 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.4562898874282837, std : 0.17734599113464355, min : 0.17437919974327087, max : 0.9492942094802856\n",
      "Label 5 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.4756259322166443, std : 0.1778101623058319, min : 0.18873079121112823, max : 0.9506818652153015\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "            print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "            print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "            print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "            print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    l_rep_0 = l_rep[0]\n",
    "    l_rep_1 = l_rep[1]\n",
    "    l_rep_2 = l_rep[2]\n",
    "    l_rep_3 = l_rep[3]\n",
    "    l_rep_4 = l_rep[4]\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "    reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "    reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "    reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "    reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "for main_label in range(6):\n",
    "    for next_label in range(6):\n",
    "        for layer_num in range(5):\n",
    "            lama_cosine = []\n",
    "            if main_label != next_label:\n",
    "                for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "                    for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "                        lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "                all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.49104276299476624, std : 0.30104321241378784, min : -0.06061321496963501, max : 0.9483209848403931\n",
      "Label 0 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.6206247806549072, std : 0.19752804934978485, min : 0.20239697396755219, max : 0.949982225894928\n",
      "Label 0 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.6621878147125244, std : 0.20493528246879578, min : 0.22527503967285156, max : 0.9605312347412109\n",
      "Label 0 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.6745672225952148, std : 0.209737166762352, min : 0.2252473682165146, max : 0.9623642563819885\n",
      "Label 0 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.6963954567909241, std : 0.21580903232097626, min : 0.25123119354248047, max : 0.9677630662918091\n",
      "Label 0 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.15634380280971527, std : 0.23554828763008118, min : -0.06872876733541489, max : 0.7742531299591064\n",
      "Label 0 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.40887266397476196, std : 0.16453783214092255, min : 0.17391207814216614, max : 0.8375195264816284\n",
      "Label 0 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.43802356719970703, std : 0.17054900527000427, min : 0.18605829775333405, max : 0.866995632648468\n",
      "Label 0 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.45322132110595703, std : 0.1771116852760315, min : 0.1823975294828415, max : 0.8825441002845764\n",
      "Label 0 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.4734562337398529, std : 0.1794327348470688, min : 0.19319450855255127, max : 0.8936076164245605\n",
      "Label 0 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.1358012557029724, std : 0.26918676495552063, min : -0.12449823319911957, max : 0.8533498644828796\n",
      "Label 0 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.3854272961616516, std : 0.18713515996932983, min : 0.17279647290706635, max : 0.8859785795211792\n",
      "Label 0 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.4089481234550476, std : 0.19280512630939484, min : 0.17979280650615692, max : 0.9017983675003052\n",
      "Label 0 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.42873644828796387, std : 0.197392538189888, min : 0.19081470370292664, max : 0.9154196381568909\n",
      "Label 0 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.4454805254936218, std : 0.19878466427326202, min : 0.19460779428482056, max : 0.9238801598548889\n",
      "Label 0 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.12897059321403503, std : 0.2516952157020569, min : -0.09904538094997406, max : 0.8477741479873657\n",
      "Label 0 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.40465855598449707, std : 0.17261117696762085, min : 0.16103927791118622, max : 0.884231686592102\n",
      "Label 0 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.43084970116615295, std : 0.1778867393732071, min : 0.17286215722560883, max : 0.8972817063331604\n",
      "Label 0 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.45161956548690796, std : 0.18264952301979065, min : 0.18619519472122192, max : 0.9119596481323242\n",
      "Label 0 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.4696166515350342, std : 0.1854972243309021, min : 0.20992913842201233, max : 0.9193711876869202\n",
      "Label 0 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.4104144275188446, std : 0.3359563946723938, min : -0.08138211071491241, max : 0.9165225028991699\n",
      "Label 0 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.5930052995681763, std : 0.2020476758480072, min : 0.20751477777957916, max : 0.9219884276390076\n",
      "Label 0 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.6293478608131409, std : 0.20469316840171814, min : 0.2326432764530182, max : 0.9359314441680908\n",
      "Label 0 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.6459349393844604, std : 0.20557136833667755, min : 0.23240163922309875, max : 0.9411964416503906\n",
      "Label 0 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.6704496145248413, std : 0.20936879515647888, min : 0.25871577858924866, max : 0.9505868554115295\n",
      "Label 1 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.49104276299476624, std : 0.30104321241378784, min : -0.06061321496963501, max : 0.9483209848403931\n",
      "Label 1 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.6206247210502625, std : 0.19752804934978485, min : 0.20239697396755219, max : 0.949982225894928\n",
      "Label 1 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.6621878147125244, std : 0.20493528246879578, min : 0.22527503967285156, max : 0.9605312347412109\n",
      "Label 1 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.6745671629905701, std : 0.209737166762352, min : 0.2252473682165146, max : 0.9623642563819885\n",
      "Label 1 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.6963953971862793, std : 0.21580903232097626, min : 0.25123119354248047, max : 0.9677630662918091\n",
      "Label 1 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.10253044962882996, std : 0.21131564676761627, min : -0.08481482416391373, max : 0.8504394292831421\n",
      "Label 1 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.37704071402549744, std : 0.15801841020584106, min : 0.15156471729278564, max : 0.8925286531448364\n",
      "Label 1 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.4094313383102417, std : 0.16161222755908966, min : 0.18699780106544495, max : 0.9016830325126648\n",
      "Label 1 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.42029014229774475, std : 0.16654880344867706, min : 0.18102648854255676, max : 0.9172935485839844\n",
      "Label 1 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.4413280487060547, std : 0.168716162443161, min : 0.18971562385559082, max : 0.9255659580230713\n",
      "Label 1 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.0588916651904583, std : 0.2155742198228836, min : -0.1300784796476364, max : 0.8623807430267334\n",
      "Label 1 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.3359367549419403, std : 0.15885189175605774, min : 0.16802451014518738, max : 0.8966096043586731\n",
      "Label 1 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.3601628541946411, std : 0.15986624360084534, min : 0.18225917220115662, max : 0.9055246710777283\n",
      "Label 1 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.375140905380249, std : 0.16406290233135223, min : 0.19040080904960632, max : 0.9196398258209229\n",
      "Label 1 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.3926440179347992, std : 0.1654970496892929, min : 0.1918448656797409, max : 0.9285220503807068\n",
      "Label 1 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.062234487384557724, std : 0.2109309732913971, min : -0.10180964320898056, max : 0.8369553089141846\n",
      "Label 1 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.36577481031417847, std : 0.15889914333820343, min : 0.15517333149909973, max : 0.8756765723228455\n",
      "Label 1 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.39300718903541565, std : 0.16021621227264404, min : 0.16901174187660217, max : 0.8896008133888245\n",
      "Label 1 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.40922898054122925, std : 0.16485653817653656, min : 0.1813449114561081, max : 0.9043276309967041\n",
      "Label 1 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.4279767572879791, std : 0.1681065559387207, min : 0.20567971467971802, max : 0.9128027558326721\n",
      "Label 1 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.4352719187736511, std : 0.3262033462524414, min : -0.08366178721189499, max : 0.9001944661140442\n",
      "Label 1 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.6165609359741211, std : 0.1850477010011673, min : 0.20491649210453033, max : 0.9032299518585205\n",
      "Label 1 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.6611412763595581, std : 0.18085870146751404, min : 0.24335011839866638, max : 0.9181498289108276\n",
      "Label 1 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.6790688633918762, std : 0.17789506912231445, min : 0.2668313980102539, max : 0.9231038689613342\n",
      "Label 1 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.706623375415802, std : 0.17727074027061462, min : 0.27554988861083984, max : 0.9351376891136169\n",
      "Label 2 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.15634381771087646, std : 0.23554828763008118, min : -0.06872876733541489, max : 0.7742531299591064\n",
      "Label 2 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.40887269377708435, std : 0.16453783214092255, min : 0.17391207814216614, max : 0.8375195264816284\n",
      "Label 2 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.43802356719970703, std : 0.17054900527000427, min : 0.18605829775333405, max : 0.866995632648468\n",
      "Label 2 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.45322132110595703, std : 0.1771116852760315, min : 0.1823975294828415, max : 0.8825441002845764\n",
      "Label 2 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.4734562337398529, std : 0.1794327348470688, min : 0.19319450855255127, max : 0.8936076164245605\n",
      "Label 2 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.10253044962882996, std : 0.21131564676761627, min : -0.08481482416391373, max : 0.8504394292831421\n",
      "Label 2 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.37704068422317505, std : 0.15801841020584106, min : 0.15156471729278564, max : 0.8925286531448364\n",
      "Label 2 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.4094313383102417, std : 0.16161222755908966, min : 0.18699780106544495, max : 0.9016830325126648\n",
      "Label 2 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.42029014229774475, std : 0.16654880344867706, min : 0.18102648854255676, max : 0.9172935485839844\n",
      "Label 2 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.4413280487060547, std : 0.168716162443161, min : 0.18971562385559082, max : 0.9255659580230713\n",
      "Label 2 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.6117189526557922, std : 0.23533733189105988, min : -0.008466653525829315, max : 0.9084749221801758\n",
      "Label 2 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.7106944918632507, std : 0.16378961503505707, min : 0.2474644035100937, max : 0.9300581812858582\n",
      "Label 2 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.7230424284934998, std : 0.16771762073040009, min : 0.22118842601776123, max : 0.9371703267097473\n",
      "Label 2 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.7426468133926392, std : 0.16658994555473328, min : 0.22459784150123596, max : 0.9467432498931885\n",
      "Label 2 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.7567704916000366, std : 0.16448955237865448, min : 0.20878808200359344, max : 0.951785683631897\n",
      "Label 2 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.6005682349205017, std : 0.2189742624759674, min : -0.009399625472724438, max : 0.901111900806427\n",
      "Label 2 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.7050049901008606, std : 0.14826169610023499, min : 0.2586275637149811, max : 0.9270445108413696\n",
      "Label 2 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.7168173789978027, std : 0.15119092166423798, min : 0.21952581405639648, max : 0.9320397973060608\n",
      "Label 2 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.7368567585945129, std : 0.149682879447937, min : 0.22213812172412872, max : 0.9431164860725403\n",
      "Label 2 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.7514374256134033, std : 0.1472984254360199, min : 0.22794181108474731, max : 0.9503753185272217\n",
      "Label 2 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.21914950013160706, std : 0.27948200702667236, min : -0.0711466446518898, max : 0.8557542562484741\n",
      "Label 2 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.4543095529079437, std : 0.19544249773025513, min : 0.17820242047309875, max : 0.9106044769287109\n",
      "Label 2 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.47630929946899414, std : 0.19344650208950043, min : 0.19479279220104218, max : 0.9262024164199829\n",
      "Label 2 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.4896862804889679, std : 0.19726568460464478, min : 0.18680833280086517, max : 0.9308004379272461\n",
      "Label 2 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.5091884136199951, std : 0.19617639482021332, min : 0.1925458461046219, max : 0.937005341053009\n",
      "Label 3 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.1358012706041336, std : 0.26918676495552063, min : -0.12449823319911957, max : 0.8533498644828796\n",
      "Label 3 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.3854272961616516, std : 0.18713515996932983, min : 0.17279647290706635, max : 0.8859785795211792\n",
      "Label 3 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.4089481830596924, std : 0.19280512630939484, min : 0.17979280650615692, max : 0.9017983675003052\n",
      "Label 3 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.42873644828796387, std : 0.197392538189888, min : 0.19081470370292664, max : 0.9154196381568909\n",
      "Label 3 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.4454805254936218, std : 0.19878466427326202, min : 0.19460779428482056, max : 0.9238801598548889\n",
      "Label 3 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.0588916651904583, std : 0.2155742198228836, min : -0.1300784796476364, max : 0.8623807430267334\n",
      "Label 3 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.3359367847442627, std : 0.15885189175605774, min : 0.16802451014518738, max : 0.8966096043586731\n",
      "Label 3 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.3601629137992859, std : 0.15986624360084534, min : 0.18225917220115662, max : 0.9055246710777283\n",
      "Label 3 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.37514087557792664, std : 0.16406290233135223, min : 0.19040080904960632, max : 0.9196398258209229\n",
      "Label 3 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.39264407753944397, std : 0.1654970496892929, min : 0.1918448656797409, max : 0.9285220503807068\n",
      "Label 3 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.611719012260437, std : 0.23533733189105988, min : -0.008466653525829315, max : 0.9084749221801758\n",
      "Label 3 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.7106944918632507, std : 0.16378961503505707, min : 0.2474644035100937, max : 0.9300581812858582\n",
      "Label 3 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.7230424284934998, std : 0.16771762073040009, min : 0.22118842601776123, max : 0.9371703267097473\n",
      "Label 3 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.7426466941833496, std : 0.16658994555473328, min : 0.22459784150123596, max : 0.9467432498931885\n",
      "Label 3 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.7567704319953918, std : 0.16448955237865448, min : 0.20878808200359344, max : 0.951785683631897\n",
      "Label 3 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.7155175805091858, std : 0.2096378654241562, min : 0.025728320702910423, max : 0.9171212911605835\n",
      "Label 3 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.7884035110473633, std : 0.1403961181640625, min : 0.2672543227672577, max : 0.947242796421051\n",
      "Label 3 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.797396183013916, std : 0.14434552192687988, min : 0.2509269118309021, max : 0.9484739303588867\n",
      "Label 3 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.8147109150886536, std : 0.13952724635601044, min : 0.275501549243927, max : 0.9556846022605896\n",
      "Label 3 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.8243623375892639, std : 0.1381683647632599, min : 0.29319456219673157, max : 0.95858234167099\n",
      "Label 3 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.21733160316944122, std : 0.3280966281890869, min : -0.12381869554519653, max : 0.8864870071411133\n",
      "Label 3 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.4458034038543701, std : 0.22903744876384735, min : 0.18295951187610626, max : 0.9227361679077148\n",
      "Label 3 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.4602057933807373, std : 0.2265104502439499, min : 0.18429552018642426, max : 0.9256579279899597\n",
      "Label 3 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.47845426201820374, std : 0.22747938334941864, min : 0.1897875815629959, max : 0.9337693452835083\n",
      "Label 3 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.49281033873558044, std : 0.2255319505929947, min : 0.18827003240585327, max : 0.9390102624893188\n",
      "Label 4 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.12897059321403503, std : 0.2516952157020569, min : -0.09904538094997406, max : 0.8477741479873657\n",
      "Label 4 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.40465855598449707, std : 0.17261117696762085, min : 0.16103927791118622, max : 0.884231686592102\n",
      "Label 4 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.43084967136383057, std : 0.1778867393732071, min : 0.17286215722560883, max : 0.8972817063331604\n",
      "Label 4 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.45161956548690796, std : 0.18264952301979065, min : 0.18619519472122192, max : 0.9119596481323242\n",
      "Label 4 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.46961668133735657, std : 0.1854972243309021, min : 0.20992913842201233, max : 0.9193711876869202\n",
      "Label 4 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.062234487384557724, std : 0.2109309732913971, min : -0.10180964320898056, max : 0.8369553089141846\n",
      "Label 4 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.36577481031417847, std : 0.15889914333820343, min : 0.15517333149909973, max : 0.8756765723228455\n",
      "Label 4 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.39300718903541565, std : 0.16021621227264404, min : 0.16901174187660217, max : 0.8896008133888245\n",
      "Label 4 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.40922901034355164, std : 0.16485653817653656, min : 0.1813449114561081, max : 0.9043276309967041\n",
      "Label 4 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.4279767572879791, std : 0.1681065559387207, min : 0.20567971467971802, max : 0.9128027558326721\n",
      "Label 4 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.6005682349205017, std : 0.2189742624759674, min : -0.009399625472724438, max : 0.901111900806427\n",
      "Label 4 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.7050050497055054, std : 0.14826169610023499, min : 0.2586275637149811, max : 0.9270445108413696\n",
      "Label 4 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.716817319393158, std : 0.15119092166423798, min : 0.21952581405639648, max : 0.9320397973060608\n",
      "Label 4 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.7368566393852234, std : 0.149682879447937, min : 0.22213812172412872, max : 0.9431164860725403\n",
      "Label 4 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.7514374256134033, std : 0.1472984254360199, min : 0.22794181108474731, max : 0.9503753185272217\n",
      "Label 4 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.715517520904541, std : 0.2096378654241562, min : 0.025728320702910423, max : 0.9171212911605835\n",
      "Label 4 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.7884035110473633, std : 0.1403961181640625, min : 0.2672543227672577, max : 0.947242796421051\n",
      "Label 4 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.7973961234092712, std : 0.14434552192687988, min : 0.2509269118309021, max : 0.9484739303588867\n",
      "Label 4 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.8147108554840088, std : 0.13952724635601044, min : 0.275501549243927, max : 0.9556846022605896\n",
      "Label 4 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.8243623971939087, std : 0.1381683647632599, min : 0.29319456219673157, max : 0.95858234167099\n",
      "Label 4 - Label 5 _ Layer : 0\n",
      "Lama mean : 0.2193300426006317, std : 0.32096537947654724, min : -0.09022942930459976, max : 0.8900092244148254\n",
      "Label 4 - Label 5 _ Layer : 1\n",
      "Lama mean : 0.4719197154045105, std : 0.2214241623878479, min : 0.17364564538002014, max : 0.9282689094543457\n",
      "Label 4 - Label 5 _ Layer : 2\n",
      "Lama mean : 0.4887608289718628, std : 0.21772630512714386, min : 0.17687717080116272, max : 0.9331209659576416\n",
      "Label 4 - Label 5 _ Layer : 3\n",
      "Lama mean : 0.5076916813850403, std : 0.21848121285438538, min : 0.1811559796333313, max : 0.9418851733207703\n",
      "Label 4 - Label 5 _ Layer : 4\n",
      "Lama mean : 0.5231521725654602, std : 0.2174660563468933, min : 0.20156794786453247, max : 0.9466685652732849\n",
      "Label 5 - Label 0 _ Layer : 0\n",
      "Lama mean : 0.4104144275188446, std : 0.3359563946723938, min : -0.08138211071491241, max : 0.9165225028991699\n",
      "Label 5 - Label 0 _ Layer : 1\n",
      "Lama mean : 0.5930052399635315, std : 0.2020476758480072, min : 0.20751477777957916, max : 0.9219884276390076\n",
      "Label 5 - Label 0 _ Layer : 2\n",
      "Lama mean : 0.6293479204177856, std : 0.20469316840171814, min : 0.2326432764530182, max : 0.9359314441680908\n",
      "Label 5 - Label 0 _ Layer : 3\n",
      "Lama mean : 0.6459349393844604, std : 0.20557136833667755, min : 0.23240163922309875, max : 0.9411964416503906\n",
      "Label 5 - Label 0 _ Layer : 4\n",
      "Lama mean : 0.6704495549201965, std : 0.20936879515647888, min : 0.25871577858924866, max : 0.9505868554115295\n",
      "Label 5 - Label 1 _ Layer : 0\n",
      "Lama mean : 0.4352719187736511, std : 0.3262033462524414, min : -0.08366178721189499, max : 0.9001944661140442\n",
      "Label 5 - Label 1 _ Layer : 1\n",
      "Lama mean : 0.6165609955787659, std : 0.1850477010011673, min : 0.20491649210453033, max : 0.9032299518585205\n",
      "Label 5 - Label 1 _ Layer : 2\n",
      "Lama mean : 0.6611413359642029, std : 0.18085870146751404, min : 0.24335011839866638, max : 0.9181498289108276\n",
      "Label 5 - Label 1 _ Layer : 3\n",
      "Lama mean : 0.679068922996521, std : 0.17789506912231445, min : 0.2668313980102539, max : 0.9231038689613342\n",
      "Label 5 - Label 1 _ Layer : 4\n",
      "Lama mean : 0.7066233158111572, std : 0.17727074027061462, min : 0.27554988861083984, max : 0.9351376891136169\n",
      "Label 5 - Label 2 _ Layer : 0\n",
      "Lama mean : 0.21914947032928467, std : 0.27948200702667236, min : -0.0711466446518898, max : 0.8557542562484741\n",
      "Label 5 - Label 2 _ Layer : 1\n",
      "Lama mean : 0.4543095827102661, std : 0.19544249773025513, min : 0.17820242047309875, max : 0.9106044769287109\n",
      "Label 5 - Label 2 _ Layer : 2\n",
      "Lama mean : 0.4763093590736389, std : 0.19344650208950043, min : 0.19479279220104218, max : 0.9262024164199829\n",
      "Label 5 - Label 2 _ Layer : 3\n",
      "Lama mean : 0.4896862804889679, std : 0.19726568460464478, min : 0.18680833280086517, max : 0.9308004379272461\n",
      "Label 5 - Label 2 _ Layer : 4\n",
      "Lama mean : 0.5091884136199951, std : 0.19617639482021332, min : 0.1925458461046219, max : 0.937005341053009\n",
      "Label 5 - Label 3 _ Layer : 0\n",
      "Lama mean : 0.21733158826828003, std : 0.3280966281890869, min : -0.12381869554519653, max : 0.8864870071411133\n",
      "Label 5 - Label 3 _ Layer : 1\n",
      "Lama mean : 0.4458034038543701, std : 0.22903744876384735, min : 0.18295951187610626, max : 0.9227361679077148\n",
      "Label 5 - Label 3 _ Layer : 2\n",
      "Lama mean : 0.4602058231830597, std : 0.2265104502439499, min : 0.18429552018642426, max : 0.9256579279899597\n",
      "Label 5 - Label 3 _ Layer : 3\n",
      "Lama mean : 0.47845426201820374, std : 0.22747938334941864, min : 0.1897875815629959, max : 0.9337693452835083\n",
      "Label 5 - Label 3 _ Layer : 4\n",
      "Lama mean : 0.49281027913093567, std : 0.2255319505929947, min : 0.18827003240585327, max : 0.9390102624893188\n",
      "Label 5 - Label 4 _ Layer : 0\n",
      "Lama mean : 0.2193300426006317, std : 0.32096537947654724, min : -0.09022942930459976, max : 0.8900092244148254\n",
      "Label 5 - Label 4 _ Layer : 1\n",
      "Lama mean : 0.4719196856021881, std : 0.2214241623878479, min : 0.17364564538002014, max : 0.9282689094543457\n",
      "Label 5 - Label 4 _ Layer : 2\n",
      "Lama mean : 0.48876088857650757, std : 0.21772630512714386, min : 0.17687717080116272, max : 0.9331209659576416\n",
      "Label 5 - Label 4 _ Layer : 3\n",
      "Lama mean : 0.5076916813850403, std : 0.21848121285438538, min : 0.1811559796333313, max : 0.9418851733207703\n",
      "Label 5 - Label 4 _ Layer : 4\n",
      "Lama mean : 0.5231521725654602, std : 0.2174660563468933, min : 0.20156794786453247, max : 0.9466685652732849\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "            print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "            print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "            print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "            print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.5121121406555176, std : 0.2874774932861328, min : -0.017242038622498512, max : 0.9495558738708496\n",
      "Bert mean : -0.14266011118888855, std : 0.13258406519889832, min : -0.2987886369228363, max : 0.9136795997619629\n",
      "Roberta mean : 0.05507628247141838, std : 0.19220171868801117, min : -0.08155005425214767, max : 0.8289508819580078\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.1943480521440506, std : 0.22652888298034668, min : -0.015423230826854706, max : 0.7862840890884399\n",
      "Bert mean : -0.22013546526432037, std : 0.0641045793890953, min : -0.284465491771698, max : 0.1181434914469719\n",
      "Roberta mean : -0.03148631006479263, std : 0.01759730465710163, min : -0.08271956443786621, max : 0.04119328409433365\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.17391426861286163, std : 0.2577849328517914, min : -0.05200731381773949, max : 0.8607800602912903\n",
      "Bert mean : -0.14841324090957642, std : 0.22305595874786377, min : -0.3096223771572113, max : 0.9735559821128845\n",
      "Roberta mean : 0.008408951573073864, std : 0.11687132716178894, min : -0.13296078145503998, max : 0.7164315581321716\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.171469047665596, std : 0.2397753745317459, min : -0.03663622587919235, max : 0.8556891083717346\n",
      "Bert mean : -0.17374226450920105, std : 0.041644103825092316, min : -0.28353458642959595, max : 0.07786160707473755\n",
      "Roberta mean : 0.05889064818620682, std : 0.03464774787425995, min : -0.06318297982215881, max : 0.18513426184654236\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.44060182571411133, std : 0.31493881344795227, min : -0.022837253287434578, max : 0.9185599088668823\n",
      "Bert mean : -0.17347352206707, std : 0.08544764667749405, min : -0.3197597861289978, max : 0.9465936422348022\n",
      "Roberta mean : 0.240835502743721, std : 0.1960969865322113, min : -0.1757146567106247, max : 0.7915641665458679\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.5121122002601624, std : 0.2874774932861328, min : -0.017242038622498512, max : 0.9495558738708496\n",
      "Bert mean : -0.14266009628772736, std : 0.13258406519889832, min : -0.2987886369228363, max : 0.9136795997619629\n",
      "Roberta mean : 0.05507627874612808, std : 0.19220171868801117, min : -0.08155005425214767, max : 0.8289508819580078\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.14493797719478607, std : 0.20426520705223083, min : -0.032380856573581696, max : 0.8583769202232361\n",
      "Bert mean : -0.05186343193054199, std : 0.20709696412086487, min : -0.4008021950721741, max : 0.9917566776275635\n",
      "Roberta mean : 0.053145840764045715, std : 0.19266051054000854, min : -0.10450652241706848, max : 0.967191755771637\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.10214070230722427, std : 0.20641542971134186, min : -0.048612967133522034, max : 0.8691487312316895\n",
      "Bert mean : -0.07531848549842834, std : 0.22725418210029602, min : -0.19512948393821716, max : 0.997750461101532\n",
      "Roberta mean : 0.071658194065094, std : 0.2503490149974823, min : -0.12399380654096603, max : 0.9962494969367981\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.11024295538663864, std : 0.20175567269325256, min : -0.03261736407876015, max : 0.8447762131690979\n",
      "Bert mean : -0.34318336844444275, std : 0.13537421822547913, min : -0.5188316702842712, max : 0.9617279171943665\n",
      "Roberta mean : 0.03931935504078865, std : 0.06881491839885712, min : -0.06753883510828018, max : 0.9563272595405579\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.4672698378562927, std : 0.3020721673965454, min : -0.022086750715970993, max : 0.9023157954216003\n",
      "Bert mean : -0.02648257464170456, std : 0.2990797758102417, min : -0.452262282371521, max : 0.9919337630271912\n",
      "Roberta mean : 0.1256842017173767, std : 0.20102496445178986, min : -0.10597074776887894, max : 0.9876200556755066\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.1943480521440506, std : 0.22652888298034668, min : -0.015423230826854706, max : 0.7862840890884399\n",
      "Bert mean : -0.22013548016548157, std : 0.0641045793890953, min : -0.284465491771698, max : 0.1181434914469719\n",
      "Roberta mean : -0.03148631006479263, std : 0.01759730465710163, min : -0.08271956443786621, max : 0.04119328409433365\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.14493796229362488, std : 0.20426520705223083, min : -0.032380856573581696, max : 0.8583769202232361\n",
      "Bert mean : -0.05186343193054199, std : 0.20709696412086487, min : -0.4008021950721741, max : 0.9917566776275635\n",
      "Roberta mean : 0.053145840764045715, std : 0.19266051054000854, min : -0.10450652241706848, max : 0.967191755771637\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.6303285360336304, std : 0.2229337990283966, min : 0.04164133220911026, max : 0.9125948548316956\n",
      "Bert mean : -0.07952677458524704, std : 0.09198979288339615, min : -0.1703888475894928, max : 0.33495303988456726\n",
      "Roberta mean : 0.013209665194153786, std : 0.0697527751326561, min : -0.10787702351808548, max : 0.40821245312690735\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.6202078461647034, std : 0.20573365688323975, min : 0.039365530014038086, max : 0.9057788848876953\n",
      "Bert mean : -0.021288929507136345, std : 0.3085426390171051, min : -0.18919792771339417, max : 0.9983481764793396\n",
      "Roberta mean : 0.08204485476016998, std : 0.2721855044364929, min : -0.055391713976860046, max : 0.9954660534858704\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : 0.25591567158699036, std : 0.26799359917640686, min : -0.02761932648718357, max : 0.8622138500213623\n",
      "Bert mean : 0.028415938839316368, std : 0.41320616006851196, min : -0.2942323088645935, max : 0.9946339726448059\n",
      "Roberta mean : 0.0293334499001503, std : 0.30441415309906006, min : -0.1615443378686905, max : 0.9827514886856079\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.17391426861286163, std : 0.2577849328517914, min : -0.05200731381773949, max : 0.8607800602912903\n",
      "Bert mean : -0.14841324090957642, std : 0.22305595874786377, min : -0.3096223771572113, max : 0.9735559821128845\n",
      "Roberta mean : 0.008408951573073864, std : 0.11687132716178894, min : -0.13296078145503998, max : 0.7164315581321716\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.10214070975780487, std : 0.20641542971134186, min : -0.048612967133522034, max : 0.8691487312316895\n",
      "Bert mean : -0.07531848549842834, std : 0.22725418210029602, min : -0.19512948393821716, max : 0.997750461101532\n",
      "Roberta mean : 0.0716581791639328, std : 0.2503490149974823, min : -0.12399380654096603, max : 0.9962494969367981\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.6303285360336304, std : 0.2229337990283966, min : 0.04164133220911026, max : 0.9125948548316956\n",
      "Bert mean : -0.07952676713466644, std : 0.09198979288339615, min : -0.1703888475894928, max : 0.33495303988456726\n",
      "Roberta mean : 0.013209667056798935, std : 0.0697527751326561, min : -0.10787702351808548, max : 0.40821245312690735\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.7288509011268616, std : 0.1993042677640915, min : 0.05995570868253708, max : 0.9234922528266907\n",
      "Bert mean : -0.09357620030641556, std : 0.06431104242801666, min : -0.36517447233200073, max : 0.21023023128509521\n",
      "Roberta mean : 0.10346400737762451, std : 0.2245309203863144, min : -0.09954153001308441, max : 0.981991708278656\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.2529654800891876, std : 0.3136674165725708, min : -0.042705778032541275, max : 0.8942092061042786\n",
      "Bert mean : -0.04590533301234245, std : 0.34482675790786743, min : -0.29041799902915955, max : 0.9927626848220825\n",
      "Roberta mean : 0.12237651646137238, std : 0.26982590556144714, min : -0.14889036118984222, max : 0.9821911454200745\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.171469047665596, std : 0.2397753745317459, min : -0.03663622587919235, max : 0.8556891083717346\n",
      "Bert mean : -0.17374230921268463, std : 0.041644103825092316, min : -0.28353458642959595, max : 0.07786160707473755\n",
      "Roberta mean : 0.05889064073562622, std : 0.03464774787425995, min : -0.06318297982215881, max : 0.18513426184654236\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.11024295538663864, std : 0.20175567269325256, min : -0.03261736407876015, max : 0.8447762131690979\n",
      "Bert mean : -0.34318339824676514, std : 0.13537421822547913, min : -0.5188316702842712, max : 0.9617279171943665\n",
      "Roberta mean : 0.03931935504078865, std : 0.06881491839885712, min : -0.06753883510828018, max : 0.9563272595405579\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.6202078461647034, std : 0.20573365688323975, min : 0.039365530014038086, max : 0.9057788848876953\n",
      "Bert mean : -0.021288929507136345, std : 0.3085426390171051, min : -0.18919792771339417, max : 0.9983481764793396\n",
      "Roberta mean : 0.08204485476016998, std : 0.2721855044364929, min : -0.055391713976860046, max : 0.9954660534858704\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.7288509607315063, std : 0.1993042677640915, min : 0.05995570868253708, max : 0.9234922528266907\n",
      "Bert mean : -0.09357620030641556, std : 0.06431104242801666, min : -0.36517447233200073, max : 0.21023023128509521\n",
      "Roberta mean : 0.10346400737762451, std : 0.2245309203863144, min : -0.09954153001308441, max : 0.981991708278656\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.2592938244342804, std : 0.30590856075286865, min : -0.033399444073438644, max : 0.8966891169548035\n",
      "Bert mean : -0.1386706829071045, std : 0.1856643706560135, min : -0.4289097785949707, max : 0.986427366733551\n",
      "Roberta mean : 0.04702417552471161, std : 0.21787522733211517, min : -0.11786077916622162, max : 0.9895791411399841\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.44060182571411133, std : 0.31493881344795227, min : -0.022837253287434578, max : 0.9185599088668823\n",
      "Bert mean : -0.1734735518693924, std : 0.08544764667749405, min : -0.3197597861289978, max : 0.9465936422348022\n",
      "Roberta mean : 0.240835502743721, std : 0.1960969865322113, min : -0.1757146567106247, max : 0.7915641665458679\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.4672698974609375, std : 0.3020721673965454, min : -0.022086750715970993, max : 0.9023157954216003\n",
      "Bert mean : -0.02648257464170456, std : 0.2990797758102417, min : -0.452262282371521, max : 0.9919337630271912\n",
      "Roberta mean : 0.1256842017173767, std : 0.20102496445178986, min : -0.10597074776887894, max : 0.9876200556755066\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : 0.25591567158699036, std : 0.26799359917640686, min : -0.02761932648718357, max : 0.8622138500213623\n",
      "Bert mean : 0.028415940701961517, std : 0.41320616006851196, min : -0.2942323088645935, max : 0.9946339726448059\n",
      "Roberta mean : 0.029333453625440598, std : 0.30441415309906006, min : -0.1615443378686905, max : 0.9827514886856079\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.2529654800891876, std : 0.3136674165725708, min : -0.042705778032541275, max : 0.8942092061042786\n",
      "Bert mean : -0.045905325561761856, std : 0.34482675790786743, min : -0.29041799902915955, max : 0.9927626848220825\n",
      "Roberta mean : 0.12237651646137238, std : 0.26982590556144714, min : -0.14889036118984222, max : 0.9821911454200745\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.2592938244342804, std : 0.30590856075286865, min : -0.033399444073438644, max : 0.8966891169548035\n",
      "Bert mean : -0.1386706829071045, std : 0.1856643706560135, min : -0.4289097785949707, max : 0.986427366733551\n",
      "Roberta mean : 0.047024186700582504, std : 0.21787522733211517, min : -0.11786077916622162, max : 0.9895791411399841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama layer별로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_dict = {i : {'lama' : {j : [] for j in range(6)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
