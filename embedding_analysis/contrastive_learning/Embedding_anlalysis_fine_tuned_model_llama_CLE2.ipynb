{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDataset import MyDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lama_embedding_path = '../llama_embedding/CL/tumor_CL_E2_finetuned_5layers/dataset_tensor/'\n",
    "bert_embedding_path = '../bert_embedding/tumor_finetuned/dataset_tensor/'\n",
    "roberta_embedding_path = '../roberta_embedding/tumor_finetuned/dataset_tensor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset('train', lama_embedding_path, bert_embedding_path, roberta_embedding_path)\n",
    "test_data =MyDataset('test', lama_embedding_path, bert_embedding_path, roberta_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "test_data = DataLoader(test_data, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[ 2.4686,  0.1128, -0.8303,  ..., -1.9006,  1.0106, -0.4239],\n",
      "         [ 2.8946, -0.5558, -0.2016,  ..., -1.9466,  0.9924, -1.9950],\n",
      "         [-0.6792,  0.6425, -0.0866,  ..., -0.4243, -0.0500, -0.0195],\n",
      "         [-0.9947,  0.5831, -0.0385,  ..., -0.4406,  0.1925, -0.1192],\n",
      "         [-0.5444,  0.5250, -0.0720,  ..., -0.1767, -0.2036, -0.0942]]]), tensor([[-0.8550,  0.6309, -0.6627,  ..., -0.5210,  0.2564, -0.3477]]), tensor([[ 1.7732,  0.9521, -0.7721,  ..., -0.5331, -0.3238, -0.3959]]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_data):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rep, b_rep, r_rep, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_rep.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4096]), torch.Size([1024]), torch.Size([1024]), 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rep.squeeze().shape, b_rep.squeeze().shape, r_rep.squeeze().shape, label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps 별 코사인 유사도 평균 구해보기\n",
    "def get_l1_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).sum()\n",
    "def get_l2_distance(x1, x2):\n",
    "    return ((x1 - x2)**2).sum()**.5\n",
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_cosine_similarity(reps_dict[0]['lama'][0].flatten() , reps_dict[0]['lama'][1].flatten()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 끼리의 코사인 유사도\n",
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.9076439738273621, std : 0.22898848354816437, min : 0.06392692029476166, max : 0.9995421171188354\n",
      "Bert mean : 0.9680062532424927, std : 0.06482237577438354, min : 0.5762061476707458, max : 0.9999645352363586\n",
      "Roberta mean : 0.7679231762886047, std : 0.3273807466030121, min : -0.14232468605041504, max : 0.9999403357505798\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.9596883058547974, std : 0.12654009461402893, min : 0.12441258132457733, max : 0.9996135830879211\n",
      "Bert mean : 0.967339038848877, std : 0.11414065212011337, min : 0.08446459472179413, max : 0.9999260902404785\n",
      "Roberta mean : 0.9764479398727417, std : 0.1192246600985527, min : 0.14114268124103546, max : 0.9999465346336365\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9716706871986389, std : 0.02594989351928234, min : 0.8022006750106812, max : 0.9987496137619019\n",
      "Bert mean : 0.9849059581756592, std : 0.014416240155696869, min : 0.9077816605567932, max : 1.0\n",
      "Roberta mean : 0.9430029988288879, std : 0.08131575584411621, min : 0.4610406756401062, max : 0.9998674392700195\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.9583266973495483, std : 0.10851956903934479, min : 0.21194614470005035, max : 0.9996213912963867\n",
      "Bert mean : 0.8764873147010803, std : 0.19998440146446228, min : -0.4412735402584076, max : 0.9999936819076538\n",
      "Roberta mean : 0.9174246191978455, std : 0.14946486055850983, min : 0.05138710141181946, max : 0.9999562501907349\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.8901370763778687, std : 0.25013598799705505, min : 0.00477683125063777, max : 0.9994548559188843\n",
      "Bert mean : 0.9256547689437866, std : 0.1994471251964569, min : -0.47379860281944275, max : 0.9999668598175049\n",
      "Roberta mean : 0.8538801670074463, std : 0.23610827326774597, min : -0.15768025815486908, max : 0.9999735355377197\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.9792730808258057, std : 0.01317859161645174, min : 0.9054394960403442, max : 0.9991573095321655\n",
      "Bert mean : 0.9804783463478088, std : 0.02478441223502159, min : 0.7493387460708618, max : 0.9998576641082764\n",
      "Roberta mean : 0.7132827639579773, std : 0.3693503737449646, min : -0.2799872159957886, max : 0.9978198409080505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.7235060334205627, std : 0.3557000160217285, min : 0.02549651637673378, max : 0.9993566870689392\n",
      "Bert mean : 0.8336034417152405, std : 0.3186386525630951, min : 0.08265118300914764, max : 0.9998219013214111\n",
      "Roberta mean : 0.6699555516242981, std : 0.3867779076099396, min : -0.21332025527954102, max : 0.9997442364692688\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.9228191375732422, std : 0.1395287960767746, min : 0.4617442786693573, max : 0.9992985725402832\n",
      "Bert mean : 0.7791711091995239, std : 0.4013894498348236, min : -0.16923663020133972, max : 0.9998753070831299\n",
      "Roberta mean : 0.9898001551628113, std : 0.011732799932360649, min : 0.9500171542167664, max : 0.9999515414237976\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9596724510192871, std : 0.05236944928765297, min : 0.7691056132316589, max : 0.989779531955719\n",
      "Bert mean : 0.9522998332977295, std : 0.059857457876205444, min : 0.6458935737609863, max : 0.9971312284469604\n",
      "Roberta mean : 0.6465266346931458, std : 0.39903706312179565, min : -0.11720796674489975, max : 0.9968264102935791\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.8567554950714111, std : 0.26419129967689514, min : 0.02644912339746952, max : 0.9994288682937622\n",
      "Bert mean : 0.8970180153846741, std : 0.0863928273320198, min : 0.6242274045944214, max : 0.999889612197876\n",
      "Roberta mean : 0.8786481618881226, std : 0.22249220311641693, min : 0.08519376814365387, max : 0.999923825263977\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.8094668984413147, std : 0.33617672324180603, min : 0.004766535945236683, max : 0.9851357340812683\n",
      "Bert mean : 0.6472193002700806, std : 0.46094951033592224, min : -0.22326429188251495, max : 0.9991183280944824\n",
      "Roberta mean : 0.7454431056976318, std : 0.34253740310668945, min : -0.050079647451639175, max : 0.9964597225189209\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.6661962270736694, std : 0.4460509121417999, min : 0.011831419542431831, max : 0.9982922673225403\n",
      "Bert mean : 0.5748860836029053, std : 0.49861255288124084, min : -0.22613559663295746, max : 0.9996704459190369\n",
      "Roberta mean : 0.5188899636268616, std : 0.45197492837905884, min : -0.2699298560619354, max : 0.9902639985084534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Layer 별 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)\n",
    "# # llama layer별\n",
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict[idx]['lama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 라벨들과의 코사인 유사도 비교\n",
    "## TrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.02409115433692932, std : 0.008159271441400051, min : -0.0031479548197239637, max : 0.12954245507717133\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390362352132797, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.054024361073970795, std : 0.01486006285995245, min : 0.019488442689180374, max : 0.15998908877372742\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.09816878288984299, std : 0.18058668076992035, min : 0.026359135285019875, max : 0.990395188331604\n",
      "Bert mean : 0.01212023664265871, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777684669941664, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.0709080696105957, std : 0.09367988258600235, min : -0.007364591117948294, max : 0.9580115675926208\n",
      "Bert mean : -0.24299663305282593, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745851278305, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.019550159573554993, std : 0.019631613045930862, min : -0.015792805701494217, max : 0.15248264372348785\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175329387188, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.02409115433692932, std : 0.008159271441400051, min : -0.0031479548197239637, max : 0.12954245507717133\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390361607074738, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.08189768344163895, std : 0.10444971174001694, min : 0.03623967617750168, max : 0.9657607078552246\n",
      "Bert mean : -0.22500866651535034, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239936351776123, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.03179103881120682, std : 0.013972407206892967, min : 0.013170937076210976, max : 0.891190767288208\n",
      "Bert mean : -0.08799591660499573, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.06770117580890656, std : 0.10008808970451355, min : -0.001735121477395296, max : 0.9504795670509338\n",
      "Bert mean : -0.2719807028770447, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572444558143616, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.023905767127871513, std : 0.007032372988760471, min : -0.0002896625082939863, max : 0.07055047899484634\n",
      "Bert mean : -0.23431454598903656, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608237445354462, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.054024361073970795, std : 0.01486006285995245, min : 0.019488442689180374, max : 0.15998908877372742\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.08189768344163895, std : 0.10444971174001694, min : 0.03623967617750168, max : 0.9657607078552246\n",
      "Bert mean : -0.22500869631767273, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239934489130974, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.0451730452477932, std : 0.09353309869766235, min : 0.006312339100986719, max : 0.9127663373947144\n",
      "Bert mean : -0.3086966276168823, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.8951668739318848, std : 0.1781766563653946, min : 0.010591350495815277, max : 0.9659716486930847\n",
      "Bert mean : 0.14542877674102783, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856135308742523, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : 0.012382745742797852, std : 0.01156130526214838, min : -0.002226716373115778, max : 0.11009686440229416\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.09816879034042358, std : 0.18058668076992035, min : 0.026359135285019875, max : 0.990395188331604\n",
      "Bert mean : 0.012120235711336136, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777686066925526, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.03179103881120682, std : 0.013972407206892967, min : 0.013170937076210976, max : 0.891190767288208\n",
      "Bert mean : -0.08799590915441513, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.0451730452477932, std : 0.09353309869766235, min : 0.006312339100986719, max : 0.9127663373947144\n",
      "Bert mean : -0.30869656801223755, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.05563327670097351, std : 0.12893342971801758, min : -0.0015645535895600915, max : 0.9596110582351685\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754584550857544, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.0025389608927071095, std : 0.007872678339481354, min : -0.010077707469463348, max : 0.07133185118436813\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.0388345941901207, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.0709080696105957, std : 0.09367988258600235, min : -0.007364591117948294, max : 0.9580115675926208\n",
      "Bert mean : -0.24299664795398712, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745702266693, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.06770116835832596, std : 0.10008808970451355, min : -0.001735121477395296, max : 0.9504795670509338\n",
      "Bert mean : -0.27198073267936707, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572443068027496, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.8951668739318848, std : 0.1781766563653946, min : 0.010591350495815277, max : 0.9659716486930847\n",
      "Bert mean : 0.14542879164218903, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856133818626404, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.05563327297568321, std : 0.12893342971801758, min : -0.0015645535895600915, max : 0.9596110582351685\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754583954811096, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.024535639211535454, std : 0.10933960229158401, min : -0.013909491710364819, max : 0.9896438717842102\n",
      "Bert mean : 0.020568503066897392, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.019550159573554993, std : 0.019631613045930862, min : -0.015792805701494217, max : 0.15248264372348785\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175143122673, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.023905768990516663, std : 0.007032372988760471, min : -0.0002896625082939863, max : 0.07055047899484634\n",
      "Bert mean : -0.23431456089019775, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608236700296402, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : 0.012382745742797852, std : 0.01156130526214838, min : -0.002226716373115778, max : 0.11009686440229416\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.0025389608927071095, std : 0.007872678339481354, min : -0.010077707469463348, max : 0.07133185118436813\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.038834597915410995, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.024535639211535454, std : 0.10933960229158401, min : -0.013909491710364819, max : 0.9896438717842102\n",
      "Bert mean : 0.02056850492954254, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lama layer 별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.039492830634117126, std : 0.06229321286082268, min : -0.0008880180539563298, max : 0.6483756899833679\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963636338710785, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.048924900591373444, std : 0.03375855088233948, min : -0.006950210779905319, max : 0.24981564283370972\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491055458784103, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.09095834940671921, std : 0.1469699591398239, min : -1.3598660189018119e-05, max : 0.7839920520782471\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.07578261196613312, std : 0.09962552785873413, min : -0.007185572758316994, max : 0.9631281495094299\n",
      "Bert mean : -0.0997287854552269, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851363122463226, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.20668815076351166, std : 0.3471216559410095, min : -0.005696347914636135, max : 0.9885799288749695\n",
      "Bert mean : 0.22141341865062714, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352897554636, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.039492834359407425, std : 0.06229321286082268, min : -0.0008880180539563298, max : 0.6483756899833679\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963634848594666, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.09144648909568787, std : 0.07308057695627213, min : 0.04137364402413368, max : 0.6180940866470337\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014228403568268, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.04593045637011528, std : 0.06098797917366028, min : 0.01406790129840374, max : 0.4960452616214752\n",
      "Bert mean : -0.09645825624465942, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.05983810871839523, std : 0.03975899890065193, min : 0.014707081951200962, max : 0.6265040040016174\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.1362455189228058, std : 0.2791677713394165, min : 0.004820690490305424, max : 0.9867432117462158\n",
      "Bert mean : 0.008446179330348969, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238015964627266, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.048924900591373444, std : 0.03375855088233948, min : -0.006950210779905319, max : 0.24981564283370972\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491056203842163, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.09144649654626846, std : 0.07308057695627213, min : 0.04137364402413368, max : 0.6180940866470337\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014227658510208, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.07341637462377548, std : 0.16896596550941467, min : 0.016329079866409302, max : 0.9278985857963562\n",
      "Bert mean : -0.3473791182041168, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312171295285225, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.8467259407043457, std : 0.24493753910064697, min : 0.004965039435774088, max : 0.9584474563598633\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : 0.023778945207595825, std : 0.029687929898500443, min : -0.0018263859674334526, max : 0.4161282479763031\n",
      "Bert mean : -0.04856457933783531, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.0909583568572998, std : 0.1469699591398239, min : -1.3598660189018119e-05, max : 0.7839920520782471\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.04593045637011528, std : 0.06098797917366028, min : 0.01406790129840374, max : 0.4960452616214752\n",
      "Bert mean : -0.09645824879407883, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.07341635972261429, std : 0.16896596550941467, min : 0.016329079866409302, max : 0.9278985857963562\n",
      "Bert mean : -0.3473791778087616, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312172133475542, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.107083760201931, std : 0.2299066036939621, min : 0.010346422903239727, max : 0.9693534970283508\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.053450748324394226, std : 0.19478023052215576, min : -0.018595289438962936, max : 0.9672185182571411\n",
      "Bert mean : -0.08600692451000214, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.07578261196613312, std : 0.09962552785873413, min : -0.007185572758316994, max : 0.9631281495094299\n",
      "Bert mean : -0.0997287929058075, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851364612579346, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.05983810871839523, std : 0.03975899890065193, min : 0.014707081951200962, max : 0.6265040040016174\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.8467259407043457, std : 0.24493753910064697, min : 0.004965039435774088, max : 0.9584474563598633\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.107083760201931, std : 0.2299066036939621, min : 0.010346422903239727, max : 0.9693534970283508\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.05540395528078079, std : 0.17637713253498077, min : -0.00803291704505682, max : 0.9585561156272888\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.20668818056583405, std : 0.3471216559410095, min : -0.005696347914636135, max : 0.9885799288749695\n",
      "Bert mean : 0.22141343355178833, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352890104055405, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.13624553382396698, std : 0.2791677713394165, min : 0.004820690490305424, max : 0.9867432117462158\n",
      "Bert mean : 0.008446180261671543, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238017827272415, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : 0.023778945207595825, std : 0.029687929898500443, min : -0.0018263859674334526, max : 0.4161282479763031\n",
      "Bert mean : -0.04856457561254501, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.05345075950026512, std : 0.19478023052215576, min : -0.018595289438962936, max : 0.9672185182571411\n",
      "Bert mean : -0.08600693196058273, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.055403951555490494, std : 0.17637713253498077, min : -0.00803291704505682, max : 0.9585561156272888\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama layer별로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m r_rep \u001b[38;5;241m=\u001b[39m r_rep\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mreps_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlama\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(l_rep)\n\u001b[1;32m     13\u001b[0m reps_dict[label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(b_rep)\n\u001b[1;32m     14\u001b[0m reps_dict[label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(r_rep)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(6)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'].append(l_rep)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
