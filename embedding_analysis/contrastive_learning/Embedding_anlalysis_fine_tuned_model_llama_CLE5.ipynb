{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDataset import MyDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lama_embedding_path = '../llama_embedding/CL/tumor_CL_E5_finetuned_5layers/dataset_tensor/'\n",
    "bert_embedding_path = '../bert_embedding/tumor_finetuned/dataset_tensor/'\n",
    "roberta_embedding_path = '../roberta_embedding/tumor_finetuned/dataset_tensor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset('train', lama_embedding_path, bert_embedding_path, roberta_embedding_path)\n",
    "test_data =MyDataset('test', lama_embedding_path, bert_embedding_path, roberta_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "test_data = DataLoader(test_data, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[ 0.5707,  1.8738,  1.7158,  ..., -0.5121, -0.5070, -3.2223],\n",
      "         [ 2.8981, -0.7300, -5.9436,  ...,  1.8892, 14.1718, -6.9318],\n",
      "         [ 1.7248, -2.8400, -8.1515,  ...,  1.4890, 11.0796, -8.2691],\n",
      "         [ 1.9318, -2.7744, -9.0985,  ...,  1.9688, 10.3446, -7.6217],\n",
      "         [ 2.0564, -2.6905, -9.5386,  ...,  2.1030, 10.2261, -7.8515]]]), tensor([[-0.8550,  0.6309, -0.6627,  ..., -0.5210,  0.2564, -0.3477]]), tensor([[ 1.7732,  0.9521, -0.7721,  ..., -0.5331, -0.3238, -0.3959]]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_data):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rep, b_rep, r_rep, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_rep.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4096]), torch.Size([1024]), torch.Size([1024]), 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rep.squeeze().shape, b_rep.squeeze().shape, r_rep.squeeze().shape, label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps 별 코사인 유사도 평균 구해보기\n",
    "def get_l1_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).sum()\n",
    "def get_l2_distance(x1, x2):\n",
    "    return ((x1 - x2)**2).sum()**.5\n",
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_cosine_similarity(reps_dict[0]['lama'][0].flatten() , reps_dict[0]['lama'][1].flatten()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 끼리의 코사인 유사도\n",
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.9995008111000061, std : 0.0002591870434116572, min : 0.9980225563049316, max : 0.9999945759773254\n",
      "Bert mean : 0.9680062532424927, std : 0.06482237577438354, min : 0.5762061476707458, max : 0.9999645352363586\n",
      "Roberta mean : 0.7679231762886047, std : 0.3273807466030121, min : -0.14232468605041504, max : 0.9999403357505798\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.999442458152771, std : 0.00029023661045357585, min : 0.9974765181541443, max : 0.9999966025352478\n",
      "Bert mean : 0.967339038848877, std : 0.11414065212011337, min : 0.08446459472179413, max : 0.9999260902404785\n",
      "Roberta mean : 0.9764479398727417, std : 0.1192246600985527, min : 0.14114268124103546, max : 0.9999465346336365\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9993741512298584, std : 0.000449829560238868, min : 0.9976463913917542, max : 0.9999779462814331\n",
      "Bert mean : 0.9849059581756592, std : 0.014416240155696869, min : 0.9077816605567932, max : 1.0\n",
      "Roberta mean : 0.9430029988288879, std : 0.08131575584411621, min : 0.4610406756401062, max : 0.9998674392700195\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.9995813965797424, std : 0.00021016695245634764, min : 0.9985610246658325, max : 0.999995768070221\n",
      "Bert mean : 0.8764873147010803, std : 0.19998440146446228, min : -0.4412735402584076, max : 0.9999936819076538\n",
      "Roberta mean : 0.9174246191978455, std : 0.14946486055850983, min : 0.05138710141181946, max : 0.9999562501907349\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.9996048808097839, std : 0.00026416481705382466, min : 0.9976479411125183, max : 0.9999995231628418\n",
      "Bert mean : 0.9256547689437866, std : 0.1994471251964569, min : -0.47379860281944275, max : 0.9999668598175049\n",
      "Roberta mean : 0.8538801670074463, std : 0.23610827326774597, min : -0.15768025815486908, max : 0.9999735355377197\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.9995560050010681, std : 0.00024492215015925467, min : 0.9982556700706482, max : 0.9999969601631165\n",
      "Bert mean : 0.9804783463478088, std : 0.02478441223502159, min : 0.7493387460708618, max : 0.9998576641082764\n",
      "Roberta mean : 0.7132827639579773, std : 0.3693503737449646, min : -0.2799872159957886, max : 0.9978198409080505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.9993864297866821, std : 0.0003063449403271079, min : 0.9984424710273743, max : 0.9999927282333374\n",
      "Bert mean : 0.8336034417152405, std : 0.3186386525630951, min : 0.08265118300914764, max : 0.9998219013214111\n",
      "Roberta mean : 0.6699555516242981, std : 0.3867779076099396, min : -0.21332025527954102, max : 0.9997442364692688\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.9994149804115295, std : 0.00030766334384679794, min : 0.9984554052352905, max : 0.999994695186615\n",
      "Bert mean : 0.7791711091995239, std : 0.4013894498348236, min : -0.16923663020133972, max : 0.9998753070831299\n",
      "Roberta mean : 0.9898001551628113, std : 0.011732799932360649, min : 0.9500171542167664, max : 0.9999515414237976\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9994351863861084, std : 0.0005457681254483759, min : 0.9978128671646118, max : 0.9999093413352966\n",
      "Bert mean : 0.9522998332977295, std : 0.059857457876205444, min : 0.6458935737609863, max : 0.9971312284469604\n",
      "Roberta mean : 0.6465266346931458, std : 0.39903706312179565, min : -0.11720796674489975, max : 0.9968264102935791\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.9996228218078613, std : 0.0001843401842052117, min : 0.998984694480896, max : 0.9999898076057434\n",
      "Bert mean : 0.8970180153846741, std : 0.0863928273320198, min : 0.6242274045944214, max : 0.999889612197876\n",
      "Roberta mean : 0.8786481618881226, std : 0.22249220311641693, min : 0.08519376814365387, max : 0.999923825263977\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.9996305704116821, std : 0.00019744987366721034, min : 0.9989349842071533, max : 0.9999063014984131\n",
      "Bert mean : 0.6472193002700806, std : 0.46094951033592224, min : -0.22326429188251495, max : 0.9991183280944824\n",
      "Roberta mean : 0.7454431056976318, std : 0.34253740310668945, min : -0.050079647451639175, max : 0.9964597225189209\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.9995167255401611, std : 0.0003053972322959453, min : 0.998537540435791, max : 0.9999960660934448\n",
      "Bert mean : 0.5748860836029053, std : 0.49861255288124084, min : -0.22613559663295746, max : 0.9996704459190369\n",
      "Roberta mean : 0.5188899636268616, std : 0.45197492837905884, min : -0.2699298560619354, max : 0.9902639985084534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Layer 별 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)\n",
    "# # llama layer별\n",
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict[idx]['lama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 라벨들과의 코사인 유사도 비교\n",
    "## TrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.9994385838508606, std : 0.0002674210409168154, min : 0.9973090291023254, max : 0.9999293684959412\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390362352132797, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.9993957281112671, std : 0.00033580372110009193, min : 0.9973435401916504, max : 0.9999249577522278\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.9995005130767822, std : 0.00025416750577278435, min : 0.9979906678199768, max : 0.999931812286377\n",
      "Bert mean : 0.01212023664265871, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777684669941664, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.99945068359375, std : 0.0003289066662546247, min : 0.9971153140068054, max : 0.9999415278434753\n",
      "Bert mean : -0.24299663305282593, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745851278305, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.9994911551475525, std : 0.0002763732336461544, min : 0.9979314804077148, max : 0.9999407529830933\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175329387188, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.9994385838508606, std : 0.0002674210409168154, min : 0.9973090291023254, max : 0.9999293684959412\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390361607074738, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.9993439316749573, std : 0.00034762296127155423, min : 0.9966157078742981, max : 0.9999240636825562\n",
      "Bert mean : -0.22500866651535034, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239936351776123, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.99944669008255, std : 0.00027823049458675086, min : 0.9972608089447021, max : 0.9999251365661621\n",
      "Bert mean : -0.08799591660499573, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.9993776082992554, std : 0.0003586740349419415, min : 0.9963864684104919, max : 0.9999255537986755\n",
      "Bert mean : -0.2719807028770447, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572444558143616, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.9994345903396606, std : 0.0003054449334740639, min : 0.997292697429657, max : 0.9999605417251587\n",
      "Bert mean : -0.23431454598903656, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608237445354462, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.9993956089019775, std : 0.0003358036919962615, min : 0.9973435401916504, max : 0.9999249577522278\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.9993440508842468, std : 0.00034762296127155423, min : 0.9966157078742981, max : 0.9999240636825562\n",
      "Bert mean : -0.22500869631767273, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239934489130974, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.9994450807571411, std : 0.0003453172685112804, min : 0.9979638457298279, max : 0.9999579191207886\n",
      "Bert mean : -0.3086966276168823, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.9994233846664429, std : 0.00042539165588095784, min : 0.9973556995391846, max : 0.9999335408210754\n",
      "Bert mean : 0.14542877674102783, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856135308742523, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : 0.9994300007820129, std : 0.0003689975419547409, min : 0.9979758858680725, max : 0.9999399781227112\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.9995005130767822, std : 0.00025416750577278435, min : 0.9979906678199768, max : 0.999931812286377\n",
      "Bert mean : 0.012120235711336136, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777686066925526, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.99944669008255, std : 0.00027823049458675086, min : 0.9972608089447021, max : 0.9999251365661621\n",
      "Bert mean : -0.08799590915441513, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.9994450807571411, std : 0.0003453172685112804, min : 0.9979638457298279, max : 0.9999579191207886\n",
      "Bert mean : -0.30869656801223755, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.9995619654655457, std : 0.0002477423695381731, min : 0.9977023601531982, max : 0.9999507069587708\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754584550857544, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.9995531439781189, std : 0.00022216528304852545, min : 0.9983874559402466, max : 0.9999434947967529\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.0388345941901207, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.99945068359375, std : 0.0003289066662546247, min : 0.9971153140068054, max : 0.9999415278434753\n",
      "Bert mean : -0.24299664795398712, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745702266693, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.9993777275085449, std : 0.0003586740349419415, min : 0.9963864684104919, max : 0.9999255537986755\n",
      "Bert mean : -0.27198073267936707, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572443068027496, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.9994233846664429, std : 0.00042539165588095784, min : 0.9973556995391846, max : 0.9999335408210754\n",
      "Bert mean : 0.14542879164218903, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856133818626404, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.9995619654655457, std : 0.0002477423695381731, min : 0.9977023601531982, max : 0.9999507069587708\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754583954811096, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.999559760093689, std : 0.0002693536225706339, min : 0.9975557923316956, max : 0.9999381899833679\n",
      "Bert mean : 0.020568503066897392, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.9994911551475525, std : 0.0002763732336461544, min : 0.9979314804077148, max : 0.9999407529830933\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175143122673, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.9994345903396606, std : 0.0003054449334740639, min : 0.997292697429657, max : 0.9999605417251587\n",
      "Bert mean : -0.23431456089019775, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608236700296402, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : 0.9994300007820129, std : 0.0003689975419547409, min : 0.9979758858680725, max : 0.9999399781227112\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.9995531439781189, std : 0.00022216528304852545, min : 0.9983874559402466, max : 0.9999434947967529\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.038834597915410995, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.9995597004890442, std : 0.0002693536225706339, min : 0.9975557923316956, max : 0.9999381899833679\n",
      "Bert mean : 0.02056850492954254, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lama layer 별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.9993765950202942, std : 0.00028103109798394144, min : 0.9983303546905518, max : 0.9998779892921448\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963636338710785, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.9994053244590759, std : 0.0004064904642291367, min : 0.9973613619804382, max : 0.999900758266449\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491055458784103, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.9994717240333557, std : 0.00026966611039824784, min : 0.9983727335929871, max : 0.999910831451416\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.9994506239891052, std : 0.0003173849545419216, min : 0.9981251358985901, max : 0.9999111294746399\n",
      "Bert mean : -0.0997287854552269, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851363122463226, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.9994179606437683, std : 0.0003320281393826008, min : 0.9983625411987305, max : 0.9999222755432129\n",
      "Bert mean : 0.22141341865062714, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352897554636, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.9993764758110046, std : 0.00028103109798394144, min : 0.9983303546905518, max : 0.9998779892921448\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963634848594666, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.9993712306022644, std : 0.0003454183752182871, min : 0.9975435733795166, max : 0.9998927116394043\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014228403568268, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.9994081854820251, std : 0.00024828899768181145, min : 0.9986282587051392, max : 0.9998750686645508\n",
      "Bert mean : -0.09645825624465942, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.9993758797645569, std : 0.0003028418868780136, min : 0.9983066320419312, max : 0.9998922944068909\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.9993528723716736, std : 0.0003118632303085178, min : 0.9983372688293457, max : 0.9999024271965027\n",
      "Bert mean : 0.008446179330348969, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238015964627266, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.9994052648544312, std : 0.0004064904642291367, min : 0.9973613619804382, max : 0.999900758266449\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491056203842163, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.9993711113929749, std : 0.0003454183752182871, min : 0.9975435733795166, max : 0.9998927116394043\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014227658510208, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : 0.9995241165161133, std : 0.00039872893830761313, min : 0.9978359937667847, max : 0.9999302625656128\n",
      "Bert mean : -0.3473791182041168, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312171295285225, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : 0.9995167851448059, std : 0.00044556453940458596, min : 0.997491180896759, max : 0.9998975992202759\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : 0.999465823173523, std : 0.00046458354336209595, min : 0.9976258873939514, max : 0.9999179840087891\n",
      "Bert mean : -0.04856457933783531, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.9994717836380005, std : 0.00026966611039824784, min : 0.9983727335929871, max : 0.999910831451416\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.9994080662727356, std : 0.00024828899768181145, min : 0.9986282587051392, max : 0.9998750686645508\n",
      "Bert mean : -0.09645824879407883, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : 0.9995240569114685, std : 0.00039872893830761313, min : 0.9978359937667847, max : 0.9999302625656128\n",
      "Bert mean : -0.3473791778087616, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312172133475542, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.9996210336685181, std : 0.00019417163275647908, min : 0.9987484216690063, max : 0.99992835521698\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.9995583295822144, std : 0.0002348854177398607, min : 0.9985525608062744, max : 0.9999027848243713\n",
      "Bert mean : -0.08600692451000214, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.99945068359375, std : 0.0003173849545419216, min : 0.9981251358985901, max : 0.9999111294746399\n",
      "Bert mean : -0.0997287929058075, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851364612579346, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.9993758797645569, std : 0.0003028418868780136, min : 0.9983066320419312, max : 0.9998922944068909\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : 0.9995169043540955, std : 0.00044556453940458596, min : 0.997491180896759, max : 0.9998975992202759\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.9996209144592285, std : 0.00019417160365264863, min : 0.9987484216690063, max : 0.99992835521698\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.9995713233947754, std : 0.0002619329316075891, min : 0.9982743859291077, max : 0.9999439120292664\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.9994179606437683, std : 0.0003320281393826008, min : 0.9983625411987305, max : 0.9999222755432129\n",
      "Bert mean : 0.22141343355178833, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352890104055405, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.999352753162384, std : 0.00031186325941234827, min : 0.9983372688293457, max : 0.9999024271965027\n",
      "Bert mean : 0.008446180261671543, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238017827272415, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : 0.999465823173523, std : 0.00046458354336209595, min : 0.9976258873939514, max : 0.9999179840087891\n",
      "Bert mean : -0.04856457561254501, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.9995584487915039, std : 0.00023488538863603026, min : 0.9985525608062744, max : 0.9999027848243713\n",
      "Bert mean : -0.08600693196058273, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.9995713829994202, std : 0.0002619329607114196, min : 0.9982743859291077, max : 0.9999439120292664\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama layer별로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m r_rep \u001b[38;5;241m=\u001b[39m r_rep\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mreps_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlama\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(l_rep)\n\u001b[1;32m     13\u001b[0m reps_dict[label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(b_rep)\n\u001b[1;32m     14\u001b[0m reps_dict[label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(r_rep)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : {j : [] for j in range(6)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
