{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyDataset import MyDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lama_embedding_path = '../llama_embedding/CL/tumor_CL_E3_finetuned_5layers/dataset_tensor/'\n",
    "bert_embedding_path = '../bert_embedding/tumor_finetuned/dataset_tensor/'\n",
    "roberta_embedding_path = '../roberta_embedding/tumor_finetuned/dataset_tensor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset('train', lama_embedding_path, bert_embedding_path, roberta_embedding_path)\n",
    "test_data =MyDataset('test', lama_embedding_path, bert_embedding_path, roberta_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "test_data = DataLoader(test_data, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[ 1.3761,  0.2127, -0.9604,  ..., -1.2733,  0.2727,  0.8831],\n",
      "         [ 1.7192, -0.6819, -0.1220,  ..., -1.2109,  0.6658, -0.7323],\n",
      "         [-0.5107,  0.4175, -0.2356,  ..., -0.0969,  0.0928,  0.3295],\n",
      "         [-0.7307,  0.2628, -0.3072,  ..., -0.3696,  0.3892,  0.3428],\n",
      "         [-0.4344,  0.2354, -0.2602,  ..., -0.1401,  0.1015,  0.3727]]]), tensor([[-0.8550,  0.6309, -0.6627,  ..., -0.5210,  0.2564, -0.3477]]), tensor([[ 1.7732,  0.9521, -0.7721,  ..., -0.5331, -0.3238, -0.3959]]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_data):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rep, b_rep, r_rep, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_rep.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4096]), torch.Size([1024]), torch.Size([1024]), 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rep.squeeze().shape, b_rep.squeeze().shape, r_rep.squeeze().shape, label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps 별 코사인 유사도 평균 구해보기\n",
    "def get_l1_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).sum()\n",
    "def get_l2_distance(x1, x2):\n",
    "    return ((x1 - x2)**2).sum()**.5\n",
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_cosine_similarity(reps_dict[0]['lama'][0].flatten() , reps_dict[0]['lama'][1].flatten()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 끼리의 코사인 유사도\n",
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.9617108702659607, std : 0.14054253697395325, min : 0.015174771659076214, max : 0.9996907114982605\n",
      "Bert mean : 0.9680062532424927, std : 0.06482237577438354, min : 0.5762061476707458, max : 0.9999645352363586\n",
      "Roberta mean : 0.7679231762886047, std : 0.3273807466030121, min : -0.14232468605041504, max : 0.9999403357505798\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.9832085967063904, std : 0.010509264655411243, min : 0.9182170033454895, max : 0.9996959567070007\n",
      "Bert mean : 0.967339038848877, std : 0.11414065212011337, min : 0.08446459472179413, max : 0.9999260902404785\n",
      "Roberta mean : 0.9764479398727417, std : 0.1192246600985527, min : 0.14114268124103546, max : 0.9999465346336365\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9851194620132446, std : 0.009300495497882366, min : 0.9220899939537048, max : 0.9993664026260376\n",
      "Bert mean : 0.9849059581756592, std : 0.014416240155696869, min : 0.9077816605567932, max : 1.0\n",
      "Roberta mean : 0.9430029988288879, std : 0.08131575584411621, min : 0.4610406756401062, max : 0.9998674392700195\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.9403934478759766, std : 0.18517710268497467, min : 0.008063254877924919, max : 0.9997779130935669\n",
      "Bert mean : 0.8764873147010803, std : 0.19998440146446228, min : -0.4412735402584076, max : 0.9999936819076538\n",
      "Roberta mean : 0.9174246191978455, std : 0.14946486055850983, min : 0.05138710141181946, max : 0.9999562501907349\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.9475976228713989, std : 0.17914046347141266, min : 0.11137229949235916, max : 0.9997807741165161\n",
      "Bert mean : 0.9256547689437866, std : 0.1994471251964569, min : -0.47379860281944275, max : 0.9999668598175049\n",
      "Roberta mean : 0.8538801670074463, std : 0.23610827326774597, min : -0.15768025815486908, max : 0.9999735355377197\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.9854423403739929, std : 0.02442088909447193, min : 0.8019742965698242, max : 0.9994410872459412\n",
      "Bert mean : 0.9804783463478088, std : 0.02478441223502159, min : 0.7493387460708618, max : 0.9998576641082764\n",
      "Roberta mean : 0.7132827639579773, std : 0.3693503737449646, min : -0.2799872159957886, max : 0.9978198409080505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i :{'lama' : [] ,'bert': [],'roberta': []} for i in range(6)}\n",
    "for idx in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "    lama_cosine = []\n",
    "    bert_cosine = []\n",
    "    roberta_cosine = []\n",
    "    for i in range(len(reps_dict[idx]['lama'])):\n",
    "        for j in range(i+1, len(reps_dict[idx]['lama'])):\n",
    "            lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i].flatten() , reps_dict[idx]['lama'][j].flatten()).item())\n",
    "            bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "            roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "    all_dict[idx]['lama'] = torch.tensor(lama_cosine)\n",
    "    all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "    all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 :\n",
      "Lama mean : 0.7692450881004333, std : 0.3749873638153076, min : 0.006966863293200731, max : 0.9994299411773682\n",
      "Bert mean : 0.8336034417152405, std : 0.3186386525630951, min : 0.08265118300914764, max : 0.9998219013214111\n",
      "Roberta mean : 0.6699555516242981, std : 0.3867779076099396, min : -0.21332025527954102, max : 0.9997442364692688\n",
      "\n",
      "Label 1 :\n",
      "Lama mean : 0.8680761456489563, std : 0.27354198694229126, min : 0.027804411947727203, max : 0.999306857585907\n",
      "Bert mean : 0.7791711091995239, std : 0.4013894498348236, min : -0.16923663020133972, max : 0.9998753070831299\n",
      "Roberta mean : 0.9898001551628113, std : 0.011732799932360649, min : 0.9500171542167664, max : 0.9999515414237976\n",
      "\n",
      "Label 2 :\n",
      "Lama mean : 0.9493721127510071, std : 0.05563623458147049, min : 0.7215757369995117, max : 0.9934617877006531\n",
      "Bert mean : 0.9522998332977295, std : 0.059857457876205444, min : 0.6458935737609863, max : 0.9971312284469604\n",
      "Roberta mean : 0.6465266346931458, std : 0.39903706312179565, min : -0.11720796674489975, max : 0.9968264102935791\n",
      "\n",
      "Label 3 :\n",
      "Lama mean : 0.910886824131012, std : 0.2582409679889679, min : 0.023600932210683823, max : 0.9996537566184998\n",
      "Bert mean : 0.8970180153846741, std : 0.0863928273320198, min : 0.6242274045944214, max : 0.999889612197876\n",
      "Roberta mean : 0.8786481618881226, std : 0.22249220311641693, min : 0.08519376814365387, max : 0.999923825263977\n",
      "\n",
      "Label 4 :\n",
      "Lama mean : 0.8983034491539001, std : 0.277132511138916, min : 0.018234234303236008, max : 0.9974738955497742\n",
      "Bert mean : 0.6472193002700806, std : 0.46094951033592224, min : -0.22326429188251495, max : 0.9991183280944824\n",
      "Roberta mean : 0.7454431056976318, std : 0.34253740310668945, min : -0.050079647451639175, max : 0.9964597225189209\n",
      "\n",
      "Label 5 :\n",
      "Lama mean : 0.6632119417190552, std : 0.4549517333507538, min : -0.011865469627082348, max : 0.9991556406021118\n",
      "Bert mean : 0.5748860836029053, std : 0.49861255288124084, min : -0.22613559663295746, max : 0.9996704459190369\n",
      "Roberta mean : 0.5188899636268616, std : 0.45197492837905884, min : -0.2699298560619354, max : 0.9902639985084534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Label {i} :')\n",
    "    print(f'Lama mean : {all_dict[i][\"lama\"].mean()}, std : {all_dict[i][\"lama\"].std()}, min : {all_dict[i][\"lama\"].min()}, max : {all_dict[i][\"lama\"].max()}')\n",
    "    print(f'Bert mean : {all_dict[i][\"bert\"].mean()}, std : {all_dict[i][\"bert\"].std()}, min : {all_dict[i][\"bert\"].min()}, max : {all_dict[i][\"bert\"].max()}')\n",
    "    print(f'Roberta mean : {all_dict[i][\"roberta\"].mean()}, std : {all_dict[i][\"roberta\"].std()}, min : {all_dict[i][\"roberta\"].min()}, max : {all_dict[i][\"roberta\"].max()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Layer 별 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)\n",
    "# # llama layer별\n",
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict[idx]['lama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# for idx in range(6):\n",
    "#     # bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "#     bert_cosine = []\n",
    "#     roberta_cosine = []\n",
    "#     for i in range(len(reps_dict[idx]['bert'])):\n",
    "#         for j in range(i+1, len(reps_dict[idx]['bert'])):\n",
    "#             bert_cosine.append(get_cosine_similarity(reps_dict[idx]['bert'][i].flatten() , reps_dict[idx]['bert'][j].flatten()).item())\n",
    "#             roberta_cosine.append(get_cosine_similarity(reps_dict[idx]['roberta'][i].flatten() , reps_dict[idx]['roberta'][j].flatten()).item())\n",
    "\n",
    "#     all_dict[idx]['bert'] = torch.tensor(bert_cosine)\n",
    "#     all_dict[idx]['roberta'] = torch.tensor(roberta_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(6):\n",
    "#     for i in range(5): # lama layer별\n",
    "#         lama_cosine = []\n",
    "#         for j in range(len(reps_dict[idx]['lama'][i])):\n",
    "#             for k in range(j+1, len(reps_dict[idx]['lama'][i])):\n",
    "#                 lama_cosine.append(get_cosine_similarity(reps_dict[idx]['lama'][i][j].flatten(),reps_dict[idx]['lama'][i][k].flatten()).item())\n",
    "        \n",
    "#         all_dict[idx]['lama'][i] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(f'Label {i} :')\n",
    "#     # lama layer 별\n",
    "#     print(f'lama layer 0  mean : {all_dict[i][\"lama\"][0].mean()} std : {all_dict[i][\"lama\"][0].std()} min : {all_dict[i][\"lama\"][0].min()} max : {all_dict[i][\"lama\"][0].max()}')\n",
    "#     print(f'lama layer 1  mean : {all_dict[i][\"lama\"][1].mean()} std : {all_dict[i][\"lama\"][1].std()} min : {all_dict[i][\"lama\"][1].min()} max : {all_dict[i][\"lama\"][1].max()}')\n",
    "#     print(f'lama layer 2  mean : {all_dict[i][\"lama\"][2].mean()} std : {all_dict[i][\"lama\"][2].std()} min : {all_dict[i][\"lama\"][2].min()} max : {all_dict[i][\"lama\"][2].max()}')\n",
    "#     print(f'lama layer 3  mean : {all_dict[i][\"lama\"][3].mean()} std : {all_dict[i][\"lama\"][3].std()} min : {all_dict[i][\"lama\"][3].min()} max : {all_dict[i][\"lama\"][3].max()}')\n",
    "#     print(f'lama layer 4  mean : {all_dict[i][\"lama\"][4].mean()} std : {all_dict[i][\"lama\"][4].std()} min : {all_dict[i][\"lama\"][4].min()} max : {all_dict[i][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 라벨들과의 코사인 유사도 비교\n",
    "## TrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.037323351949453354, std : 0.008496801368892193, min : 0.009072191081941128, max : 0.08510429412126541\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390362352132797, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : 0.0032276418060064316, std : 0.008141499012708664, min : -0.04681554064154625, max : 0.04380728676915169\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.031872574239969254, std : 0.10059945285320282, min : 0.0024723615497350693, max : 0.9944412112236023\n",
      "Bert mean : 0.01212023664265871, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777684669941664, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.06223475560545921, std : 0.02946130931377411, min : -0.01596590131521225, max : 0.35956987738609314\n",
      "Bert mean : -0.24299663305282593, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745851278305, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.010690048336982727, std : 0.03811904415488243, min : -0.025638097897171974, max : 0.4138660132884979\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175329387188, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.037323348224163055, std : 0.008496801368892193, min : 0.009072191081941128, max : 0.08510429412126541\n",
      "Bert mean : -0.018838658928871155, std : 0.06299740821123123, min : -0.28486230969429016, max : 0.41429656744003296\n",
      "Roberta mean : 0.09390361607074738, std : 0.2708650529384613, min : -0.15240997076034546, max : 0.9793902039527893\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : -0.011809089221060276, std : 0.015028316527605057, min : -0.05159218981862068, max : 0.12114269286394119\n",
      "Bert mean : -0.22500866651535034, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239936351776123, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.018840158358216286, std : 0.011589046567678452, min : 0.007649686653167009, max : 0.12523429095745087\n",
      "Bert mean : -0.08799591660499573, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.03807684779167175, std : 0.006799118127673864, min : -0.0062801712192595005, max : 0.07507852464914322\n",
      "Bert mean : -0.2719807028770447, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572444558143616, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.010978427715599537, std : 0.006905806716531515, min : -0.012371237389743328, max : 0.04989492520689964\n",
      "Bert mean : -0.23431454598903656, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608237445354462, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : 0.0032276418060064316, std : 0.008141499012708664, min : -0.04681554064154625, max : 0.04380728676915169\n",
      "Bert mean : -0.3624917268753052, std : 0.04028254747390747, min : -0.4767549932003021, max : -0.07394306361675262\n",
      "Roberta mean : -0.11761458963155746, std : 0.05398637428879738, min : -0.37233245372772217, max : 0.5991447567939758\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : -0.011809088289737701, std : 0.015028316527605057, min : -0.05159218981862068, max : 0.12114269286394119\n",
      "Bert mean : -0.22500869631767273, std : 0.03247443959116936, min : -0.3948671817779541, max : -0.09077092260122299\n",
      "Roberta mean : -0.021239934489130974, std : 0.09366828948259354, min : -0.25450876355171204, max : 0.6335989236831665\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : -0.03300337493419647, std : 0.017765972763299942, min : -0.1531553864479065, max : 0.08573558181524277\n",
      "Bert mean : -0.3086966276168823, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : -0.06515846401453018, std : 0.03716730698943138, min : -0.14616937935352325, max : 0.15900154411792755\n",
      "Bert mean : 0.14542877674102783, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856135308742523, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : -0.01679139770567417, std : 0.010612016543745995, min : -0.0633292943239212, max : 0.05723917484283447\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.03187257796525955, std : 0.10059945285320282, min : 0.0024723615497350693, max : 0.9944412112236023\n",
      "Bert mean : 0.012120235711336136, std : 0.07806583493947983, min : -0.27485862374305725, max : 0.40978315472602844\n",
      "Roberta mean : 0.006777686066925526, std : 0.201012521982193, min : -0.3271520137786865, max : 0.9829139113426208\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.018840156495571136, std : 0.011589046567678452, min : 0.007649686653167009, max : 0.12523429095745087\n",
      "Bert mean : -0.08799590915441513, std : 0.08840277791023254, min : -0.4192464351654053, max : 0.973304271697998\n",
      "Roberta mean : 0.05575276538729668, std : 0.13814081251621246, min : -0.04205579683184624, max : 0.9858633875846863\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : -0.03300337493419647, std : 0.017765972763299942, min : -0.1531553864479065, max : 0.08573558181524277\n",
      "Bert mean : -0.30869656801223755, std : 0.08488523215055466, min : -0.5305621027946472, max : 0.3753809630870819\n",
      "Roberta mean : -0.1547616869211197, std : 0.0766592025756836, min : -0.27888932824134827, max : 0.6092892289161682\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.031423717737197876, std : 0.14861156046390533, min : -0.02998904325067997, max : 0.9778938293457031\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754584550857544, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : -0.013560492545366287, std : 0.006831990089267492, min : -0.030611885711550713, max : 0.054573558270931244\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.0388345941901207, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.06223475560545921, std : 0.02946130931377411, min : -0.01596590131521225, max : 0.35956987738609314\n",
      "Bert mean : -0.24299664795398712, std : 0.08786565065383911, min : -0.4041677713394165, max : 0.2545238435268402\n",
      "Roberta mean : -0.2362745702266693, std : 0.18965429067611694, min : -0.5853338837623596, max : 0.9626269936561584\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.03807684779167175, std : 0.006799118127673864, min : -0.0062801712192595005, max : 0.07507852464914322\n",
      "Bert mean : -0.27198073267936707, std : 0.12753696739673615, min : -0.7699481844902039, max : 0.8982016444206238\n",
      "Roberta mean : 0.14572443068027496, std : 0.1879795491695404, min : -0.21899022161960602, max : 0.9835910201072693\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : -0.06515847146511078, std : 0.03716730698943138, min : -0.14616937935352325, max : 0.15900154411792755\n",
      "Bert mean : 0.14542879164218903, std : 0.09525129944086075, min : -0.312148779630661, max : 0.6478833556175232\n",
      "Roberta mean : 0.13856133818626404, std : 0.16926130652427673, min : -0.2909478545188904, max : 0.9911037087440491\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.031423717737197876, std : 0.14861156046390533, min : -0.02998904325067997, max : 0.9778938293457031\n",
      "Bert mean : -0.13584299385547638, std : 0.1295749396085739, min : -0.7394278049468994, max : 0.5099546909332275\n",
      "Roberta mean : 0.2754583954811096, std : 0.18291328847408295, min : -0.2663937509059906, max : 0.9731398224830627\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.027492482215166092, std : 0.13708986341953278, min : -0.006977029610425234, max : 0.9844419360160828\n",
      "Bert mean : 0.020568503066897392, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.010690046474337578, std : 0.03811904415488243, min : -0.025638097897171974, max : 0.4138660132884979\n",
      "Bert mean : 0.1278710514307022, std : 0.07636721432209015, min : -0.007680320646613836, max : 0.8919113874435425\n",
      "Roberta mean : -0.03071175143122673, std : 0.15300799906253815, min : -0.4110931158065796, max : 0.9816251993179321\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.010978425852954388, std : 0.006905806716531515, min : -0.012371237389743328, max : 0.04989492520689964\n",
      "Bert mean : -0.23431456089019775, std : 0.03871849551796913, min : -0.3775964081287384, max : 0.12458926439285278\n",
      "Roberta mean : -0.09608236700296402, std : 0.2717704474925995, min : -0.2987373173236847, max : 0.991300106048584\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : -0.01679139770567417, std : 0.010612016543745995, min : -0.0633292943239212, max : 0.05723917484283447\n",
      "Bert mean : -0.019465411081910133, std : 0.0568927526473999, min : -0.30030906200408936, max : 0.3677624464035034\n",
      "Roberta mean : -0.16422729194164276, std : 0.1085774376988411, min : -0.37281328439712524, max : 0.6679128408432007\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : -0.013560492545366287, std : 0.006831990089267492, min : -0.030611885711550713, max : 0.054573558270931244\n",
      "Bert mean : -0.16696394979953766, std : 0.05927324667572975, min : -0.4768078327178955, max : 0.13811197876930237\n",
      "Roberta mean : -0.038834597915410995, std : 0.12950637936592102, min : -0.2883627116680145, max : 0.9667000770568848\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.027492478489875793, std : 0.13708986341953278, min : -0.006977029610425234, max : 0.9844419360160828\n",
      "Bert mean : 0.02056850492954254, std : 0.08309351652860641, min : -0.3269423246383667, max : 0.6470546126365662\n",
      "Roberta mean : -0.07529955357313156, std : 0.1879197061061859, min : -0.37152040004730225, max : 0.9764658212661743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lama layer 별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in train_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reps_dict = {i : {'lama' : {j : [] for j in range(5)},\n",
    "#                   'bert' : [],\n",
    "#                 'roberta' : [] } for i in range(6)}\n",
    "# cnt = 0\n",
    "# for data in test_data:\n",
    "#     cnt += 1\n",
    "#     l_rep, b_rep, r_rep, label = data\n",
    "#     l_rep = l_rep.squeeze()\n",
    "#     l_rep_0 = l_rep[0]\n",
    "#     l_rep_1 = l_rep[1]\n",
    "#     l_rep_2 = l_rep[2]\n",
    "#     l_rep_3 = l_rep[3]\n",
    "#     l_rep_4 = l_rep[4]\n",
    "#     b_rep = b_rep.squeeze()\n",
    "#     r_rep = r_rep.squeeze()\n",
    "#     label = label.item()\n",
    "#     reps_dict[label]['lama'][0].append(l_rep_0)\n",
    "#     reps_dict[label]['lama'][1].append(l_rep_1)\n",
    "#     reps_dict[label]['lama'][2].append(l_rep_2)\n",
    "#     reps_dict[label]['lama'][3].append(l_rep_3)\n",
    "#     reps_dict[label]['lama'][4].append(l_rep_4)\n",
    "#     reps_dict[label]['bert'].append(b_rep)\n",
    "#     reps_dict[label]['roberta'].append(r_rep)\n",
    "\n",
    "# cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dict = {i : {j :{'lama' : {k : [] for k in range(5)}}  for j in range(6) if i != j } for i in range(6)}\n",
    "# for main_label in range(6):\n",
    "#     for next_label in range(6):\n",
    "#         for layer_num in range(5):\n",
    "#             lama_cosine = []\n",
    "#             if main_label != next_label:\n",
    "#                 for l in range(len(reps_dict[main_label]['lama'][layer_num])):\n",
    "#                     for k in range(len(reps_dict[next_label]['lama'][layer_num])):\n",
    "#                         lama_cosine.append(get_cosine_similarity(reps_dict[main_label]['lama'][layer_num][l], reps_dict[next_label]['lama'][layer_num][k]))\n",
    "            \n",
    "#                 all_dict[main_label][next_label]['lama'][layer_num] = torch.tensor(lama_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "# for i in range(6):\n",
    "#     for j in range(6):\n",
    "#         if i != j:\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {0}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][0].mean()}, std : {all_dict[i][j][\"lama\"][0].std()}, min : {all_dict[i][j][\"lama\"][0].min()}, max : {all_dict[i][j][\"lama\"][0].max()}')\n",
    "            \n",
    "#             print(f'Label {i} - Label {j} _ Layer : {1}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][1].mean()}, std : {all_dict[i][j][\"lama\"][1].std()}, min : {all_dict[i][j][\"lama\"][1].min()}, max : {all_dict[i][j][\"lama\"][1].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {2}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][2].mean()}, std : {all_dict[i][j][\"lama\"][2].std()}, min : {all_dict[i][j][\"lama\"][2].min()}, max : {all_dict[i][j][\"lama\"][2].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {3}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][3].mean()}, std : {all_dict[i][j][\"lama\"][3].std()}, min : {all_dict[i][j][\"lama\"][3].min()}, max : {all_dict[i][j][\"lama\"][3].max()}')\n",
    "\n",
    "#             print(f'Label {i} - Label {j} _ Layer : {4}')\n",
    "#             print(f'Lama mean : {all_dict[i][j][\"lama\"][4].mean()}, std : {all_dict[i][j][\"lama\"][4].std()}, min : {all_dict[i][j][\"lama\"][4].min()}, max : {all_dict[i][j][\"lama\"][4].max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : [],\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {i : {j :{'lama' : [] ,'bert': [],'roberta': []}  for j in range(6) if i != j } for i in range(6)}\n",
    "for i in range(6):\n",
    "    # lama, bert, roberta 각각의 평균 코사인 유사도 구하기\n",
    "\n",
    "    for j in range(6):\n",
    "        lama_cosine = []\n",
    "        bert_cosine = []\n",
    "        roberta_cosine = []\n",
    "        if i != j :\n",
    "            for l in range(len(reps_dict[i]['lama'])):\n",
    "                for m in range(len(reps_dict[j]['lama'])):\n",
    "                    lama_cosine.append(get_cosine_similarity(reps_dict[i]['lama'][l].flatten() , reps_dict[j]['lama'][m].flatten()).item())\n",
    "                    bert_cosine.append(get_cosine_similarity(reps_dict[i]['bert'][l].flatten() , reps_dict[j]['bert'][m].flatten()).item())\n",
    "                    roberta_cosine.append(get_cosine_similarity(reps_dict[i]['roberta'][l].flatten() , reps_dict[j]['roberta'][m].flatten()).item())\n",
    "            \n",
    "            all_dict[i][j]['lama'] = torch.tensor(lama_cosine)\n",
    "            all_dict[i][j]['bert'] = torch.tensor(bert_cosine)\n",
    "            all_dict[i][j]['roberta'] = torch.tensor(roberta_cosine)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 - Label 1 :\n",
      "Lama mean : 0.045304007828235626, std : 0.08481265604496002, min : -0.02327115461230278, max : 0.9857052564620972\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963636338710785, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 0 - Label 2 :\n",
      "Lama mean : -0.0009570687543600798, std : 0.01725848950445652, min : -0.05253167450428009, max : 0.03516114503145218\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491055458784103, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 0 - Label 3 :\n",
      "Lama mean : 0.017102360725402832, std : 0.02292558178305626, min : -0.020675532519817352, max : 0.14399981498718262\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 0 - Label 4 :\n",
      "Lama mean : 0.06649848073720932, std : 0.07302244752645493, min : 0.001803247956559062, max : 0.9626232981681824\n",
      "Bert mean : -0.0997287854552269, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851363122463226, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 0 - Label 5 :\n",
      "Lama mean : 0.16119080781936646, std : 0.32978716492652893, min : -0.015633337199687958, max : 0.992753803730011\n",
      "Bert mean : 0.22141341865062714, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352897554636, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 1 - Label 0 :\n",
      "Lama mean : 0.04530400410294533, std : 0.08481265604496002, min : -0.02327115461230278, max : 0.9857052564620972\n",
      "Bert mean : -0.022854795679450035, std : 0.1350095123052597, min : -0.31750237941741943, max : 0.9513490796089172\n",
      "Roberta mean : 0.15963634848594666, std : 0.3471395969390869, min : -0.23151761293411255, max : 0.9594566822052002\n",
      "\n",
      "Label 1 - Label 2 :\n",
      "Lama mean : 0.0017077241791412234, std : 0.05107094347476959, min : -0.05353783443570137, max : 0.30913180112838745\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014228403568268, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 1 - Label 3 :\n",
      "Lama mean : 0.04349739849567413, std : 0.14523008465766907, min : -0.02387377992272377, max : 0.8541058301925659\n",
      "Bert mean : -0.09645825624465942, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 1 - Label 4 :\n",
      "Lama mean : 0.03996223956346512, std : 0.050862591713666916, min : -0.010284904390573502, max : 0.9539262652397156\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 1 - Label 5 :\n",
      "Lama mean : 0.15201836824417114, std : 0.32330742478370667, min : -0.012307453900575638, max : 0.9843407869338989\n",
      "Bert mean : 0.008446179330348969, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238015964627266, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 2 - Label 0 :\n",
      "Lama mean : -0.0009570687543600798, std : 0.01725848950445652, min : -0.05253167450428009, max : 0.03516114503145218\n",
      "Bert mean : -0.31181126832962036, std : 0.11207077652215958, min : -0.4233130216598511, max : 0.20009838044643402\n",
      "Roberta mean : -0.09491056203842163, std : 0.13751399517059326, min : -0.4239324927330017, max : 0.9356622695922852\n",
      "\n",
      "Label 2 - Label 1 :\n",
      "Lama mean : 0.0017077235970646143, std : 0.05107094347476959, min : -0.05353783443570137, max : 0.30913180112838745\n",
      "Bert mean : -0.20090918242931366, std : 0.08017340302467346, min : -0.34529367089271545, max : 0.18873149156570435\n",
      "Roberta mean : 0.07014227658510208, std : 0.22841495275497437, min : -0.12296874076128006, max : 0.9614444971084595\n",
      "\n",
      "Label 2 - Label 3 :\n",
      "Lama mean : -0.016957884654402733, std : 0.0610000379383564, min : -0.06195007637143135, max : 0.4534146189689636\n",
      "Bert mean : -0.3473791182041168, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312171295285225, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 2 - Label 4 :\n",
      "Lama mean : -0.010103518143296242, std : 0.13604100048542023, min : -0.13718219101428986, max : 0.4915931522846222\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 2 - Label 5 :\n",
      "Lama mean : -0.013051770627498627, std : 0.018698854371905327, min : -0.05134469270706177, max : 0.20499855279922485\n",
      "Bert mean : -0.04856457933783531, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 3 - Label 0 :\n",
      "Lama mean : 0.017102358862757683, std : 0.02292558178305626, min : -0.020675532519817352, max : 0.14399981498718262\n",
      "Bert mean : 0.03316356986761093, std : 0.11331013590097427, min : -0.25164732336997986, max : 0.3738418221473694\n",
      "Roberta mean : -0.03451103717088699, std : 0.11352895200252533, min : -0.27556857466697693, max : 0.9362984895706177\n",
      "\n",
      "Label 3 - Label 1 :\n",
      "Lama mean : 0.04349739849567413, std : 0.14523008465766907, min : -0.02387377992272377, max : 0.8541058301925659\n",
      "Bert mean : -0.09645824879407883, std : 0.08421290665864944, min : -0.3080822229385376, max : 0.22445005178451538\n",
      "Roberta mean : 0.06684635579586029, std : 0.18887443840503693, min : -0.04020862281322479, max : 0.9788969159126282\n",
      "\n",
      "Label 3 - Label 2 :\n",
      "Lama mean : -0.016957884654402733, std : 0.0610000379383564, min : -0.06195007637143135, max : 0.4534146189689636\n",
      "Bert mean : -0.3473791778087616, std : 0.09243215620517731, min : -0.5907150506973267, max : -0.006314937490969896\n",
      "Roberta mean : -0.01312172133475542, std : 0.26787322759628296, min : -0.25325828790664673, max : 0.9363290071487427\n",
      "\n",
      "Label 3 - Label 4 :\n",
      "Lama mean : 0.013956038281321526, std : 0.05101507157087326, min : -0.020963722839951515, max : 0.2937835156917572\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 3 - Label 5 :\n",
      "Lama mean : 0.04238574951887131, std : 0.20743316411972046, min : -0.0215994231402874, max : 0.9867609143257141\n",
      "Bert mean : -0.08600692451000214, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 4 - Label 0 :\n",
      "Lama mean : 0.06649848073720932, std : 0.07302244752645493, min : 0.001803247956559062, max : 0.9626232981681824\n",
      "Bert mean : -0.0997287929058075, std : 0.3285520076751709, min : -0.3672631084918976, max : 0.9919329285621643\n",
      "Roberta mean : -0.17851364612579346, std : 0.24300803244113922, min : -0.5049283504486084, max : 0.9862451553344727\n",
      "\n",
      "Label 4 - Label 1 :\n",
      "Lama mean : 0.03996224328875542, std : 0.050862591713666916, min : -0.010284904390573502, max : 0.9539262652397156\n",
      "Bert mean : -0.2185652256011963, std : 0.0840262621641159, min : -0.333223432302475, max : 0.34588995575904846\n",
      "Roberta mean : 0.1985037475824356, std : 0.2422560453414917, min : -0.0488472506403923, max : 0.9567964673042297\n",
      "\n",
      "Label 4 - Label 2 :\n",
      "Lama mean : -0.010103517211973667, std : 0.13604100048542023, min : -0.13718219101428986, max : 0.4915931522846222\n",
      "Bert mean : 0.06368101388216019, std : 0.20084938406944275, min : -0.5298604965209961, max : 0.4530509412288666\n",
      "Roberta mean : 0.252959668636322, std : 0.26998600363731384, min : -0.23154032230377197, max : 0.981590747833252\n",
      "\n",
      "Label 4 - Label 3 :\n",
      "Lama mean : 0.013956038281321526, std : 0.05101507157087326, min : -0.020963722839951515, max : 0.2937835156917572\n",
      "Bert mean : -0.04929725453257561, std : 0.258964478969574, min : -0.5007271766662598, max : 0.997059166431427\n",
      "Roberta mean : 0.2819465398788452, std : 0.21042105555534363, min : -0.06995376944541931, max : 0.9646940231323242\n",
      "\n",
      "Label 4 - Label 5 :\n",
      "Lama mean : 0.05068622902035713, std : 0.1777331382036209, min : -0.0016350975492969155, max : 0.9705699682235718\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n",
      "Label 5 - Label 0 :\n",
      "Lama mean : 0.16119080781936646, std : 0.32978716492652893, min : -0.015633337199687958, max : 0.992753803730011\n",
      "Bert mean : 0.22141343355178833, std : 0.3151133954524994, min : -0.2321678102016449, max : 0.9950602650642395\n",
      "Roberta mean : 0.039352890104055405, std : 0.2891603112220764, min : -0.2664935886859894, max : 0.9926162958145142\n",
      "\n",
      "Label 5 - Label 1 :\n",
      "Lama mean : 0.15201835334300995, std : 0.32330742478370667, min : -0.012307453900575638, max : 0.9843407869338989\n",
      "Bert mean : 0.008446180261671543, std : 0.41783544421195984, min : -0.2999997138977051, max : 0.9937301874160767\n",
      "Roberta mean : 0.021238017827272415, std : 0.38459140062332153, min : -0.2784186601638794, max : 0.9926742911338806\n",
      "\n",
      "Label 5 - Label 2 :\n",
      "Lama mean : -0.013051769696176052, std : 0.018698854371905327, min : -0.05134469270706177, max : 0.20499855279922485\n",
      "Bert mean : -0.04856457561254501, std : 0.13895487785339355, min : -0.5418837666511536, max : 0.4134030342102051\n",
      "Roberta mean : -0.14919257164001465, std : 0.1446860283613205, min : -0.34119468927383423, max : 0.9573752880096436\n",
      "\n",
      "Label 5 - Label 3 :\n",
      "Lama mean : 0.04238574579358101, std : 0.20743316411972046, min : -0.0215994231402874, max : 0.9867609143257141\n",
      "Bert mean : -0.08600693196058273, std : 0.23804083466529846, min : -0.4076484143733978, max : 0.9791983962059021\n",
      "Roberta mean : 0.015365083701908588, std : 0.22845496237277985, min : -0.2486766278743744, max : 0.9874584078788757\n",
      "\n",
      "Label 5 - Label 4 :\n",
      "Lama mean : 0.050686221569776535, std : 0.1777331382036209, min : -0.0016350975492969155, max : 0.9705699682235718\n",
      "Bert mean : -0.027991795912384987, std : 0.15719610452651978, min : -0.35787591338157654, max : 0.9758130311965942\n",
      "Roberta mean : -0.07541196048259735, std : 0.18687768280506134, min : -0.3792670965194702, max : 0.9633582830429077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i, j 레이블 간의 평균 코사인 유사도 구하기\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i != j:\n",
    "            print(f'Label {i} - Label {j} :')\n",
    "            print(f'Lama mean : {all_dict[i][j][\"lama\"].mean()}, std : {all_dict[i][j][\"lama\"].std()}, min : {all_dict[i][j][\"lama\"].min()}, max : {all_dict[i][j][\"lama\"].max()}')\n",
    "            print(f'Bert mean : {all_dict[i][j][\"bert\"].mean()}, std : {all_dict[i][j][\"bert\"].std()}, min : {all_dict[i][j][\"bert\"].min()}, max : {all_dict[i][j][\"bert\"].max()}')\n",
    "            print(f'Roberta mean : {all_dict[i][j][\"roberta\"].mean()}, std : {all_dict[i][j][\"roberta\"].std()}, min : {all_dict[i][j][\"roberta\"].min()}, max : {all_dict[i][j][\"roberta\"].max()}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama layer별로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m r_rep \u001b[38;5;241m=\u001b[39m r_rep\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mreps_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlama\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(l_rep)\n\u001b[1;32m     13\u001b[0m reps_dict[label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(b_rep)\n\u001b[1;32m     14\u001b[0m reps_dict[label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(r_rep)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "reps_dict = {i : {'lama' : {j : [] for j in range(6)},\n",
    "                  'bert' : [],\n",
    "                'roberta' : [] } for i in range(6)}\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    cnt += 1\n",
    "    l_rep, b_rep, r_rep, label = data\n",
    "    l_rep = l_rep.squeeze()\n",
    "    b_rep = b_rep.squeeze()\n",
    "    r_rep = r_rep.squeeze()\n",
    "    label = label.item()\n",
    "    reps_dict[label]['lama'].append(l_rep)\n",
    "    reps_dict[label]['bert'].append(b_rep)\n",
    "    reps_dict[label]['roberta'].append(r_rep)\n",
    "cnt == len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
